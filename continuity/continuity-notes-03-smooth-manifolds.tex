\documentclass[oneside,english]{amsbook} 
\usepackage[T1]{fontenc} 
\usepackage[latin9]{inputenc} 
\usepackage{amsmath} 
\usepackage{amstext} 
\usepackage{amsthm} 
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{hyperref}
\usepackage[all,cmtip]{xy}

\makeatletter 

\numberwithin{section}{chapter} 
\theoremstyle{plain} 
\newtheorem{thm}{\protect\theoremname}   
\theoremstyle{definition}   
\newtheorem{defn}[thm]{\protect\definitionname}

\makeatother

\usepackage{babel}   
\providecommand{\definitionname}{Definition} 
\providecommand{\theoremname}{Theorem}
\providecommand{\Cech}{\v{C}ech }
\providecommand{\Poinacre}{Poincar\'e }
\providecommand{\Kunneth}{K{\"u}nneth }

\begin{document}
	
	\title{Smooth Manifolds -- Notes}
	
	\maketitle
	
	\tableofcontents
	
	\chapter*{About this Document}
	
	This is part of \href{https://github.com/FineArtMaths/continua}{the continua project}. The project is broken down into `modules', each covering a specific topic. Each module ultimately becomes one of more courses. The approach is:
	
	\begin{itemize}
		\item{Identify a topic that should be its own module (this is recorded in the `overview' documents in each main folder)}
		\item{Create a `notes' document assembling the technical material for each module in a fairly condensed form, but with some indication of how the pedagogy might go (that's what you're looking at now)}
		\item{Create one or more coursebooks that contain the technical material along with motivation, philosophical reflections, pictures, examples, intuitive explanations and so on.}
	\end{itemize}
	
	When the coursebooks are complete, the notes document is no longer needed and will probably be deleted. So the fact that you're looking at this means this is a work in progress -- it is incomplete, disorganised and probably full of errors.
	
	\part{Introduction to Smooth Manifolds}
	
	\chapter*{General Approach}
	
	The idea is to develop as much as we can in flat space first,, with concrete examples and calculations. This will include introducing the idea of a limit, which I'd like to approach in the SDG manner without being dogmatic.
	
	I have notes from a previous course elsewhere that cover most of this material but without the unifying view. Given where this course falls in the sequence, I think we can be more ambitious here and aim for something conceptually harmonious. 
	
	In that spirit, I'd like to introduce manifolds as locally ringed spaces and do things with that setup (including developing a little bit of sheaf cohomology). But I still want the reader to come away with the same understanding as their peers who studied this stuff elsewhere so it's a balancing act. I'm not sure how to carry that off yet so these notes are likely to move around a lot over time as the right approach comes into focus. 

	\chapter{Introduction}
		\section{Intuitive Overview}
			Examples of where we want to get to, without any formalism.
		\section{Algebra Review}
			\subsection{Dual Space and Inner Product}
			\subsection{Covariance and Contravariance}
			Recall scalar fields and curves
			Include algebra of scalar fields. Point out there is no corresponding algebra of curves. 
			\subsection{The Exterior Product}
			\subsection{Fibre Bundles}
			Scalar Fields, vector fields and covector fields as sections of bundles
			\subsection{Ideals and Quotients of Rings}

	
	\chapter{The Microlocal Structure of $\mathbb{R}$}
	
		In Introduction to General Topology we gave an information characterization of the continuous line $\mathbb{R}$ and the open sets of its usual topology. In Topology, Logic and Stone Duality this object is constructed in a rigorous way that agrees with the intuition that it is a true continuum within which one can identify `regions' as open sets, which are capable of unlimited refinement.
		
		Topology alone is capable of characterising the structure of continuous phenomena but it is insufficient for describing them precisely. For this we need to dig deeper into the microscopic structure that lies between the level of open sets (which all can be thought of as having some finite size) and the imaginary points they contain (which have no size). 
		
		\section{The Algebra of Germs at a Point}
		
			\begin{defn}
				A \textbf{function} on an open set $U\subseteq \mathbb{R}$ is a map $U\to \mathbb{R}$.
			\end{defn}
			
			In this section we will consider only continuous functions, i.e. maps that are compatible with the usual topology of $\mathbb{R}$.
		
			\begin{thm}
				The functions on the open sets of $\mathbb{R}$ form a sheaf.
			\end{thm}

			\begin{thm}
				Let $U$ and $V$ be open sets in $\mathbb{R}$ such that $U\subset V$. Let $f_V$ be a function on $V$. Then there is a function $f_U$ on $U$ such that $\text{res}^V_U(f_V) = f_U$.  
			\end{thm}
			
			\begin{thm}
				Let $U$ and $V$ be open sets in $\mathbb{R}$ such that $U \cap V\ne\emptyset $. Let $f_V$ be a function on $V$ and $f_U$ be a function on $U$. Then there is a function $g = f_{U\cap V}$ on $U$ such that $\text{res}^V_g(g_V) = \text{res}^U_g(f_U)$.
			\end{thm}
			
			\begin{defn}
				Let $x$ be a point of $\mathbb{R}$ and $f$ a function defined on any open neighbourhood of $f$. The \textbf{germ} of $f$ at $x$, written $[f]_x$, is the set of all functions $g$ such that there is an open neighbourhood $U$ of $x$ on which $f_U = g_U$.  
			\end{defn}
			
			The point of this definition is to capture the idea that some functions are identical if you stay close enough to $x$ even if they become different further away. A function's germ is the set of functions that `look identical to it' if we stay close enough to $x$. Note that $U$ may be different for different functions; the only requirement is that it must be a neighbourhood of $x$.
			
			\begin{defn}
				Let $x$ be a point of $\mathbb{R}$. The set of all germs of (continuous) functions at $x$ is called the \textbf{stalk} at $x$, written $\mathscr{C}^0_x$.
			\end{defn}
			
			The 0 in this notation will be explained shortly. The stalk arises from a process of `zooming in' on $x$ by looking at ever-smaller neighbourhoods. Two functions that do not agree on a large open set might still be equal on a small one. Of course, two functions whose values at $x$ are different will never share the same germ. 
			
		\section{Differentials at a Point}
		
			\begin{thm}
				The functions in each germ at $x$ form an algebra with the usual operations, i.e. where $f, g$ are functions and $k$ is a real number we have
				\begin{itemize}
					\item{$[f]_x + [g]_x = [f + g]_x$}
					\item{$[f]_x [g]_x = [fg]_x$}
					\item{$k[f]_x = [kf]_x$}
				\end{itemize}
				Multiplication in this algebra inherits commutativity from multiplication in $\mathbb{R}$.
			\end{thm}
			
			We are interested in devloping a language to express change, which does not really take much interest in where we start. When a sprinter runs the 100 metres race, we are interested in how fast they went, how quickly they accelerated, whether they fell at the end and so on. Information about where they began the race is not part of the information about the process of change the race represented. We would therefore like to focus on the germs of a function that all start in the same place so we can compare the way they change as we move away, not unlike the sprinters in the race.
			
			\begin{thm}
				The set of germs whose functions are all equal to 0 at $x$ is an ideal of the algebra $\mathscr{C}^0_x$.
			\end{thm}
			
			\begin{proof}
				Write the set of germs that are zero at $x$ as $I_0$. Suppose $[f]_x$ and $[g]_x$ are in $I_0$, i.e. for any $f\in [f]_x$ and $g\in [g]_x$ we have $f(x) = g(x) = 0$. Then $[f + g]_x$ must also be in $I_0$ by the definition of addition. Also, since $f(x)=0$ for any other function $h$ we must have $h(x)f(x) = 0$ and therefore $[hf]_x\in I_0$. 
			\end{proof}
			
			\begin{thm}
				Let $[f]_x$ be a germ at $x$ whose value $f(x)$ is not necessarily zero. Then $[f - f(x)]_x \in I_0$.
			\end{thm}
							
			This fact allows us to treat the germs in $I_0$ as representatives of all germs at $x$. If you are interested in a function with a non-zero value there, simply transform it into a function with a zero value -- the characteristics we will be interested in do not change, only its `position'. Suppose you watched a 100m sprint on television believing it was taking place in Birmingham but later discovered it actually happened in Glasgow. Although your opinion would change about the \emph{locations} of the runners, you would probably not feel anything had changed about the \emph{race}.
			
			We will now perform the most technical and also the most intuition-based step in this section. Imagine zooming in on the point where a tangent line touches a circle -- the closer we get to that point, the more both lines appear straight. We can imagine zooming into an `infinitesimal' region around the point, too small to be any open set but larger than the zero-sized point itself. This imaginary pocket of space is just large enough to hold some information about \emph{which way} things are changing at the point -- this is a kind of linear information that we can imagine as a vector pointing away from the point saying `$f$ is going that-a-way'. But there's not enough room in this infinitesimal region to represent any higher-order information such as how $f$ is accelerating. There is room in this region for a straight line but not for a curve.
			
			The germs in $I_0$ include the germ of the constant function $f(a) = 0$ for every $a\in \mathbb{R}$. While our infinitesimal region can contain other germs that are different from this, their deviations from zero must be extremely small -- so small that if we square any function in one of those germs those tiny differences vanish. 
			
			This leads to the following definitions: 
			
			\begin{defn}
				The \textbf{higher order elements} of $I_0$ are the elements of $I_0^2$, that is, the smallest ideal containing all the products of elements of $I_0$. Specifically, an element of $I_0^2$ is a sum of finitely many products of pairs of germs in $I_0$. 
			\end{defn}
			
			What do the higher order elements look like? In fact, they are everywhere. For example, take $x = 0$. Clearly $[x^3]_0\in I_0^2$ because $[x^3]_0 = [x^2]_0[x]_0$, which is a finite product of elements of $I_0$. An intuitive way to say this is that if we zoom in close enough -- infinitely close -- the graphs of $f(x) = 0$ and $g(x) = x^3$ should look identical. 
						
			On the other hand, $[x]_0\notin I_0^2$ because the functions that multiply together would have to be defined on an open neighbourhood of $0$, which means they would include negative numbers; roots of negative numbers are not defined, which means the functions we need aren't available where we need them! 
			
			Let's now consider the germ $[x + x^3]_0 = [x]_0 + [x^3]_0$ Our claim is that since $[x^3]_0$ is a higher-order element of $I_0$, it can be disregarded and we ought to be able to express an equivalence relation `$\cong$' so that $[x + x^3]_0 \cong [x]_0$. The following definition shows how to define this equivalence precisely:

			\begin{defn}
				The \textbf{differentials} at $x$ are the elements of $I_0/I_0^2$, i.e. the ideal containing equivalence classes of germs that differ only by a second-order element. The equivalence class to which $[f]_x$ belongs is written $df_x$; it contains all $[g]_x$ such that $[f]_x - [g]_x \in I_0^2$.
			\end{defn}
			
			A differential's job is to give the least possible amount of information about how a function is changing as we move an infinitesimal distance away from $x$.
			
			As an example, let $f(a) = a^2$ and choose the point $x = 1$. We look at the germ $[f - f(1)]_1$, represented by the function $f(x) =  x^2 - 1$. We can represent an infinitesimal distance $x$ as $\epsilon$. We will show that $[f - f(1)]_1$ is equivalent to  $[g - g(1)]_1$ where $g(x) = 2x$. To do this we calculate the difference between the two functions as we move a variable distance $\epsilon$ away from $x = 1$:

			\begin{align}[rcl]
				(x + \epsilon)^2 - 1 - (2(x + \epsilon) - 2) &=& x^2 - 2x + - 2\epsilon + 2\epsilon x + \epsilon^2 +1 \\
                                           &=& - 2\epsilon + 2\epsilon + \epsilon^2 \\
                                           &=& \epsilon^2
			\end{align}

			But $[\epsilon^2]_0 \in I_0^2$, since this is nothing but the germ $[x^2]_0$. It follows that $d[x^2]_1 = d[2x]_1$. That is, in a microlocal region around 1, $x^2$ and $2x$ change in the same way. Each differential at a point can be thought of in this way: it's a collection of all the functions that are indistinguishable on a microlocal region around that point. 
			
			In each equivalence class of germs there is one which is linear: in this case it's $[2x]_1$. In a sense this linear germ is the simplest possible representative of the differential because any others will be equal to it except for some higher-order element that vanishes when we zoom in enough. We will often abuse notation and write $dx^2(1) = 2x$ -- all we have done here is removed the square brackets that remind us we're dealing with germs and write $dx^2$ as if it were a function being evaluated at $x = 1$. This is not unreasonable since we could vary the value we evaluate the differential at and get different results. 

			\begin{defn}
				A function defined on an open neighbourhood of $a$ is \textbf{differentiable} at $a$ if its germ is equivalent to a differential in $I_0/I_0^2$. If $[kx]_a$ is the representative germ of that differential we call $k$ the \textbf{rate of change} of the function at $a$. 
			\end{defn}
			
			This observation leads us to adopt the following Principle: 

			\begin{defn}
				The \textbf{Principle of Microaffineness} states that every function defined on an open neighbourhood of a point is differentiable at that point. 
			\end{defn}
			
			Without it, there would be functions that do not have differentials. Such functions are frequently considered and used in traditional courses on analysis, but often they pose unnecessary problems because they are in some sense `fictional' functions that never arise in real applications. But this is not universally true; in some situations the Principle of Microaffineness does not hold. We could perhaps state it more as an aspiration: every function \emph{ought} to be differentiable at every point where it's defined. On this course the Principle of Microaffineness holds and all functions are differentiable everywhere they're defined.

			\begin{defn}
				Let $f$ be a function defined on an open set $U\subseteq \mathbb{R}$ and suppose it is differentiable at every point in $U$. The \textbf{derivative} of $f$ is the function $df:U\to\mathbb{R}$ that assigns to each point the rate of change $k$ such that $[f]_a\equiv [kx]_a$.
			\end{defn}
			
			\begin{defn}
				The process of finding the derivative of a function is called \textbf{differentiation}.
			\end{defn}

			Notice that the derivative of a function is another function with the same domain and codomain, so we can perform the operation of `taking derivatives' multiple times. 
			
			Let's show this through another example. We will start with the function $f(x) = x^3 - x^2$. Again we subtract the value of $f$ at the point of interest but this time we allow that point to be any value of $x$, then we set higher powers of $\epsilon$ to zero because they vanish in our extreme close-up view:
			\begin{align}
				f(x + \epsilon) - f(x) &=& (x + \epsilon)^3 - (x + \epsilon)^2 - x^3 + x^2 \\
	                                                  &=& x^3 - 3 x^2 \epsilon + 3 \epsilon^2 x  - \epsilon^3 - x^2 - 2x\epsilon -\epsilon^2 - x^3 + x^2 \\
	                                                   &=& 3 x^2 \epsilon + 3 \epsilon^2 x  - \epsilon^3 - 2x\epsilon -\epsilon^2 \\
	                                                   &\cong& 3 x^2 \epsilon + 2x\epsilon 
			\end{align}
			So $d(x^3-x^2) = 3x^2 + 2x$. For any valid value of $x$, the slope of the graph of $x^3-x^2$ can be calculated by plugging $x$ into $3x^2 + 2x$. 
						
		\section{The Covariance of Differentials}
			
			Recall that $\mathbb{R}$ is a vector space, although admittedly its one dimension is a bit restricted and it can't contain very much geometry. The following remark is fairly obvious:
			\begin{thm}
				The differentials of all functions at a point $p$ in $\mathbb{R}$ form a one-dimensional vector space over $\mathbb{R}$ with basis, for example, $\{[x]_p\}$.
			\end{thm}
			There is only one-dimensional vector space over $\mathbb{R}$ but when we have two of them we can ask what happens to one when we change the coordinates of the other. In $\mathbb{R}$ we can think of this as changing our units of measurement for some physical phenomenon, then looking at what happened to the coefficients of the differentials.
			
			To be concrete, suppose we are using metres and we have the function $f(x) = x^2$ representing the temperature at points along $\mathbb{R}$, measured in Celsius. We are interested in the rate of change of this function at $x = 3$, i.e. 3 metres away from the origin where the temperature is $f(3) = 9^\circ$ Celsius. We calculate the derivative as in the previous section and arrive at $df(x) = 2x$ so what $x = 3$ we have the rate of change $df(3) = 6$, in units of Celsius per metre.
			
			Now we change our units of measurement to centimetres. The same spot is now $x = 300$ and the temperature function is no longer $f(x) = x^2$ but $g(x) = x^2/10000$. It turns out that the derivative $dg(x) = 2x/10000$, which is a rate of $dg(300) = 0.06$ Celsius per centimetre at the point of interest. Of course this is exactly the same as the 6 Celsius per metre we arrived at earlier, which is just as well because we're trying to describe a physical phenomenon that doesn't care which units of measurement we use!
			
			Notice that when we converted from metres to centimetres, the unit vector shrank (by a factor of 100) and the differential also shrank (by the same factor squared, which isn't too surprising). Since they varied in the same direction, we conclude they are covariant and so
			
			\begin{thm}
				Differentials in $\mathbb{R}$ are elements of $\mathbb{R}^\star$. A function defines a differential at each point in its domain, which is a covector field that describes how the function is changing throughout its domain.
			\end{thm}
						
		\section{Some Rules for Differentials}
			
			We can prove these in more or less the usual way
			
			\begin{thm}
				The \textbf{scalar multiple rule} states that $d[kf]_x = kd[f]_x$
			\end{thm}
			
			\begin{thm}
				The \textbf{sum rule} states that $d[f + g]_x = d[f]_x + d[g]_x$
			\end{thm}
			
			\begin{thm}
				The \textbf{product rule} states that $d[fg]_x = fd[g]_x + gd[f]_x$
			\end{thm}
			
			\begin{thm}
				The \textbf{power rule} states that $d[x^n]_x = d[nx^{n-1}]_x$
			\end{thm}

	\chapter{Calculus in $\mathbb{R}^n$}
	
		\section{The Directional Derivative}
		
		TODO: Define the tangent bundle and sections of it. Show in $\mathbb{R}^2$ how the derivative of a scalar field can be combined with a tangent vector. 
		
		\section{The Alternating Algebra of Differentials}
		
			We saw that a function on an open set $U\subseteq \to \mathbb{R}$ gives rise, through a process called `differentiation', to a covector field $U\subseteq \to \mathbb{R}^\star$ that assignes to each point the differential at that point. When we expand our interests to $\mathbb{R}^n$ there is room for a lot more structure.
			
			We will always assume that $\mathbb{R}^n$ is equipped with an orthonormal basis $\{v_1, v_2, ... v_n\}$. We therefore have access to the dot product but we will use it judiciously and take note when we do because in the spaces we study later this will not b so straightforward.
			
			We begin by reviewing some linear algebra and defining some alternative jargon that's used in the present context.
			
			\begin{defn}
				The \textbf{$k^{\text{th}}$ external product} of $\mathbb{R}^{n}$ is the vector space
					\[
						\Omega^k(\mathbb{R}^{n}) = \bigwedge^k \mathbb{R}^{n\star}
					\] 
				Its induced basis consists of all linearly indepdenent $k$-covectors.
			\end{defn}
			
			For example, suppose we are looking at $\Omega^2(\mathbb{R}^{4})$. The following twofold wedges of basis covectors are linearly independent:
				\begin{align}[c]
					v_1\wedge v_2, v_1\wedge v_3, v_1\wedge v_4, \\
					v_2\wedge v_3, v_2\wedge v_4, v_3\wedge v_4
				\end{align}
			Thus, $\Omega^2(\mathbb{R}^{4})$ is a six-dimensional vector space.
			
			\begin{thm}
				if $k > n$, $\Omega^k(\mathbb{R}^{n})$ is the trivial vector space containing only the zero element.
			\end{thm}
			
			\begin{defn}
				The \textbf{external algebra} in $\mathbb{R}^n$, written $\Omega(\mathbb{R}^n)$, is the direct sum of all non-trivial external products of $\mathbb{R}^n$,
				\[
					\Omega(\mathbb{R}^n) = \bigoplus_{0<k\le n}\Omega^k(\mathbb{R}^n)
				\]
			\end{defn}
			
		\section{The Cotangent Bundle}
		
			\begin{defn}
				Let $B$ and $E$ be topological spaces and $p:E\to B$ be a surjective homomorphism. The set $(B, E, p)$ taken together is called a \textbf{bundle} over $B$ and $p$ is called its \textbf{projection}. At a point $x\in B$ the preimage $p^{-1}(\{x\})$ is called the \textbf{fibre} over $x$.
			\end{defn}			
			
			\begin{defn}
				Let $(B, E, p)$ be a bundle over $B$. It is called a \textbf{vector bundle} if the following criteria are met:
				\begin{itemize}
					\item The fibre over each point in $B$ is isomorphic as a vector space to $\mathbb{R}^n$ for some fixed $n$; we write $F$ for the fibre.
					\item There is an open cover of $B$ such that for each open set it contains, $p^{-1}(U)\cong U\times F$.
				\end{itemize}
			\end{defn}
			
			The intuition here is that a vector bundle attaches a copy of the vector space $F$ to every point of the base space $B$.
			
			\begin{defn}
				A map $s:B\to E$ is called a \textbf{section} of the vector bundle $(B, E, p)$ if $s\circ p$ is the identity map.
			\end{defn}
			
			\begin{defn}
				The vector bundle $(\mathbb{R}^n, \mathbb{R}^n\times \Omega^k(\mathbb{R}^n), p)$ , where $p^{-1}(x)\cong \Omega^k(\mathbb{R}^n)$, is called the \textbf{contangent bundle} over $\mathbb{R}^n$ and is written $T^\star\mathbb{R}^n$. The fibre at the point $p$ is written $T_p^\star\mathbb{R}^n$.
			\end{defn}
			
			Later we will meet the tangent bundle to a manifold and this terminology and notation should seem slightly less peculiar.
			
		\section{Differential Forms}
		
			\begin{defn}
				A \textbf{differential 0-form} on $\mathbb{R}^n$ is a section of the vector bundle $(\mathbb{R}^n, \mathbb{R}^n\times \mathbb{R}, p)$, i.e. a scalar field.
			\end{defn}

			\begin{defn}
				A \textbf{differential $k$-form} (where $k > 0$) on $\mathbb{R}^n$ is a section of the cotangent bundle $(\mathbb{R}^n, \mathbb{R}^n\times \Omega^k(\mathbb{R}^n), p)$.
			\end{defn}
			
			We've already seen that a differential 1-form arises as the derivative of a 0-form -- at each point, the covector indicates the gradient of the scalar field. The same pattern continues: a differential 2-form is the derivative of a 1-form and so on. The problem this presents, though, is that differential $k$-forms can be difficult to picture in arbitrary dimensions. If we choose three dimensions as a comfortable place for forming intuitive pictures, we get the wrong pictures because in each dimension the $k$-forms behave a bit differently. This is because $k$-forms only exist up to $k=n$ and their dimensions vary according to both $k$ and $n$, a fact you've previously encountered in the context of linear algebra:
			
			\begin{thm}
				$\Omega^k(\mathbb{R}^n)$ is a vector space is equal to the binomial coefficient 
				\[
					\binom{n}{k} = \frac{n!}{k!(n-k)!}
				\]
			\end{thm}
			
			Thus, one of the obstacles to forming \emph{visual} intuitions about differential forms is that their meaning and even dimension changes depending on the base space. In a sense, the calculus we will develop embodies the right intuition but, admittedly, that is not very satisfactory in the beginning. 

		\section{The Exterior Derivative}

			The differentiation process we have discussed so far turns 0-forms into 1-forms. We want to generalise this to turn $k$-forms into $(k+1)$-forms in a consistent way. Specifically, we want it to be a morphism of sheaves so that it takes the sheaf $()$
			
			\begin{defn}
				The \textbf{exterior derivative} on $\mathbb{R}^n$ is a map $d:\Omega(\mathbb{R}^n)\to \Omega(\mathbb{R}^n)$ defined as follows:
				\begin{itemize}
					\item When $f$ is a 0-form, $df$ is its differential.
					\item If $f, g$ are $k$-forms, $d(f + g) = df + dg$
					\item For any $k$-form $f$ and scalar $a$, $d(af) = adf$
					\item For any $k$-form $f$, ddf = 0
					\item For any $k$-form $f$ and $j$-form $g$, $d(f\wedge g) = df\wedge g + (-1)^kf\wedge dg$
				\end{itemize}
			\end{defn}
			
			TODO: Discuss the last two rules and why they're needed
			
			TODO: Set out explicitly how to calculate $df$ with worked examples. This is already done in the Intermediate Calculus notes -- do it briefly here in the abstract, then we'll see concrete applications in the vector calc chapter.
			
	
		\section{Integration of Differential Forms}
		
			

		\section{A Glimpse of De Rham Cohomology}
		
			\begin{defn}
				A differential $k$-form $f$ is \textbf{exact} if there is a $g$ such that $dg = f$. 
			\end{defn}
			
			The exact forms are the ones that can be integrated, since if $d:g\mapsto f$ then $g$ is the primitive of $f$. Since integration can be tricky, it would be good to have a quick and easy test that determines whether a differentiable form is exact before we proceed to integrating it. We might start by noticing the following:

			\begin{thm}
				If $f$ is an exact form, so that $dg = f$, then $df = 0$.
			\end{thm}
			
			This leads us to give a name to `differential forms whose derivative is zero':
			
			\begin{defn}
				A differential $k$-form $f$ is \textbf{closed} if $df = 0$. 
			\end{defn}
	
			It looks as if `closed' and `exact' might be two names for the same idea, which would be very convenient. But are there closed forms that are not exact? Consider the following chain complex of maps between exterior powers (each map is just $d$ with its domain restricted to the appropriate exterior power)):		
			\[ 
			\xymatrix@C+0.3em{
				0 \ar[r] & \Omega^0(\mathbb{R}^n))\ar[r] & \Omega^1(\mathbb{R}^n))\ar[r] & ... \ar[r] & \Omega^0(\mathbb{R}^n))\ar[r]  & 0
			}
			\]
			At each stage the kernel of $d$ is the closed forms in its domain while its image is the exact forms in its codomain. Thus the exact differential forms are cocycles and the closed forms are the coboundaries.
			
			Since every exact form is closed, this is indeed a chain complex. However, is it an exact sequence? To decide this we could compute cohomology at each point; this is called the De Rham cohomology. 
			
			In $\mathbb{R}^n$ these groups are all zero but in the spaces we meet later this will not be the case; de Rham cohomology will measure an important way in which exterior differentiation `misbehaves' in spaces whose topology differs from $\mathbb{R}^n$.

			TODO: Is it useful to point out that $d$ acts like a `boundary operator' on forms at this point? This would already be familiar to the reader...
			
	\chapter{Vector Calculus in $\mathbb{R}^3$}

		A brief chapter where we show that normal vector calculus (as far as we can express it without a metric!) just `drops out of' the work we've already done. Should be very concrete and rather easy, with plenty of opportunities for reinforcement. No new theory so the chapter can be skipped.

	\chapter{Differential Calculus on Manifolds}
	
		\section{Locally Ringed Spaces}

			\begin{defn} 
				A \textbf{sheaf of rings} $\mathscr{O}(X)$ on a topological space $X$ assigns a ring to each open set of $X$ such that the sheaf axioms hold, that is (here $U$, $V$ and $W$ are open sets in $X$):
				\begin{itemize}
					\item For every pair $U\subseteq V$ there is a ring homomorphism $res_U^V: \mathscr{O}(V)\to \mathscr{O}(U)$
					\item For each open set $U$, $res_U^U$ is the identity map
					\item $res_U^V \circ res_V^W = res_U^W$ (a sort of `triangle inequality')
				\end{itemize}
			\end{defn}
			
			\begin{defn} 
				A \textbf{ringed space} is a topological space $X$ equipped with a sheaf of rings $\mathscr{O}(X)$, which is called its \textbf{structure sheaf}
			\end{defn}
			
			\begin{defn} 
				A \textbf{local ring} is a ring that has a unique maximal ideal.
			\end{defn}
			
			\begin{defn} 
				A \textbf{locally ringed space} is a ringed space such that the stalk of the structure sheaf at every point is a local ring.
			\end{defn}

		\section{Smooth Manifolds}

		\section{Diffeomorphisms}

		\section{De Rham Cohomology Revisited}
		
			Guillemin and Haine have a nice section on this that shows how to use Mayer-Vietoris and the Long Exact Sequence to calculate de Rham cohomology. Some of the calculations are left as an exercise but we should work them out in full here (or another example).


	\chapter{Integral Calculus on Manifolds}

		\section{Pushforwards and Pullbacks}
		\section{Integration of Forms}
		\section{Stokes's Theorem}


	\chapter{Vector Flows and Integral Curves}
	
		\section{Differential Equations}

		\section{Foliations}

		\section{The Lie Derivative}
	
		\section{The Frobenius Theorem}



\end{document}
