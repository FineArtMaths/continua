\documentclass[oneside,english]{amsbook} 
\usepackage[T1]{fontenc} 
\usepackage[latin9]{inputenc} 
\usepackage{amsmath} 
\usepackage{amstext} 
\usepackage{amsthm} 
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{hyperref}
\usepackage[all]{xy}

\makeatletter 

\numberwithin{section}{chapter} 
\theoremstyle{plain} 
\newtheorem{thm}{\protect\theoremname}   
\theoremstyle{definition}   
\newtheorem{defn}[thm]{\protect\definitionname}

\makeatother

\usepackage{babel}   
\providecommand{\definitionname}{Definition} 
\providecommand{\theoremname}{Theorem}
\providecommand{\Cech}{\v{C}ech }
\providecommand{\Poinacre}{Poincar\'e }
\providecommand{\Kunneth}{K{\"u}nneth }

\newcommand{\catname}[1]{{\normalfont\textbf{#1}}}
\newcommand{\RMod}{\catname{RMod\ }}
\newcommand{\SMod}{\catname{SMod\ }}
\newcommand{\im}{\text{\upshape{im}}}
\newcommand{\kVect}{\mathbf{k}\mathbf{Vect}}


\begin{document}
	
	\title{Introduction to Homological Algebra}
	
	\maketitle
	
	\tableofcontents
	
	\chapter*{Welcome!}
	
		In a slogan, homological algebra is a toolkit for studying objects that don't behave the way we think they ought to. Homology can detect -- and describe, to some extent -- the ways in which our object of interest `falls short' of our idealised expectations. 
		
		The homological viewpoint first arose in topology, where for instance on a 2D surface we expect a closed loop to contain a region of space; if it doesn't, there must be a hole there. Homology can detect holes and tell us what kinds they are by saying how the situation falls short of the way things ought to be. And if we think we have a cylinder, homology can tell us when, in fact, it's twisted like a M\"obius strip instead. 
		
		This course is about the structure of these tools and how they can be generalized to situations beyond topology. We primarily study sequences of objects that ought to have certain desirable properties and develop tools to detect when those properties fail.

		The first half of this course develops homological algebra as a tool for studying modules, which are vector spaces whose scalars don't necessarily form a field. Remember that a field must obey very strict rules about both addition and multiplication -- those rules exclude some very useful number systems, including the integers. 
		
		Vector spaces are very tame and well-behaved; the world of modules is much larger, richer and weirder. That means that when we find ourselves working with a module, the techniques of homology will be useful for telling us what sort of strange beast we're dealing with. In particular it will be a guide to how the module falls short of the impeccable behaviour we observe in vector spaces.

		The second half of the course generalizes this study to a wide range of mathematical objects -- those that form `abelian categories' -- and look at some important example applications. We finish with some more advanced tools from the toolkit.
		
		This course assumes you're familiar with the content of Introduction to Linear Algebra. If you find the material excessively abstract it may help to study Introduction to Algebraic Topology, where we introduce a specific homology theory in a very concrete setting. You could study that alongside this course if that suits you. Some very basic ideas about algebraic groups are introduced quickly on this course but more gently, and with a lot more context, on the dedicated Group Theory course. 
	
	\part{Linear Algebra over Commutative Rings}
	
	\chapter{Introduction to Commutative Rings}

		\section{General Introduction to Part 1}
		
			\subsection{The Theme of this Course}
			
				In \textbf{Introduction to Linear Algebra} we studied finite-dimensional vector spaces over a field. These in fact live within a more general class of algebraic structures called `modules'. The first half of this course introduces modules and aims to study them in the spirit of the linear algebra you already know.
				
				The problem is that the general theory of modules is a lot less neat and tidy. Vector spaces have certain properties that aren't immediately obvious but that make everything about them work remarkably smoothly. These properties aren't shared by all modules. Our approach will be to study modules as if they were `defective' vector spaces and develop tools to notice, measure and understand their defects.
				
				In reality, there is nothing `wrong' with modules; the defect is in our expectation that every module `ought' to behave like the vector spaces we're familiar with. This is a human perspective that helps us develop the theory and, in the process, grow beyond our naive expectations. 
				
				To prepare the ground, this section looks at some aspects of finite-dimensional vector spaces that were taken somewhat for granted in the previous course. These are shared by all vector spaces but not by all modules, blocking us from transferring all the theory we already know to this more general setting. Some of what follows is a bit imprecise and everything in this section will be restated later in more formal terms -- here the idea is just to prime our intuition.
				
			\subsection{Free Objects}
	
				You already know that every vector space has a basis, which is a set of elements of the space that have the following properties:
				\begin{itemize}
					\item They are \textbf{linearly independent}, meaning you can't make any one of them from a linear combination of the others. Recall that a linear combination of vectors is a sum of scalar multiples of them.
					\item They \textbf{span} the vector space in the sense that any other vector can be made from a linear combination of basis vectors.
				\end{itemize}
				It is a non-obvious fact that all the bases of a given vector space have the same number of elements, which is called the `dimension' of the space. We limited ourselves to finite-dimensional vector spaces on the introduction course but the same idea can be extended (with modifications) to infinite-dimensional spaces as well.
				
				As a philosophical aside, it's worth noting that there are vector spaces for which no basis can ever be given, but mathematicians allow themselves assumptions that imply such a basis `exists' nevertheless. An example would be if we consider the space that has the real numbers as its vectors and the rational numbers as its scalars. Whether we can claim this space has a basis is philosophically controversial; fortunately we will not need to concern ourselves with such issues on this course.
	
				In the wider context we will say that all vector spaces are `free modules', which simply means they can all be assigned a basis. Many modules we meet on this course will not be free, which means we can't work `in coordinates' or `in a basis' and often have to make do with more abstract definitions. For instance, the tensor product for modules can't be constructed from basis elements the way we did for vector spaces.
				
		\subsection{Torsion}
				Suppose that $v$ is an element of a vector space and $k$ a scalar. It may seem obvious that if $kv$ is the zero vector, it must be that either $k = 0$ or that $v$ was already the zero vector. In the world of modules, though, it can happen that $kv = 0$ even though $k\ne 0$ and $v\ne 0$! Then $v$ is a so-called `torsion element' of the module.
				
				To see why this can never happen in a vector space, think about how we would express a torsion element as a linear combination of basis elements. Then a scalar multiple of that linear combination would be the zero vector -- but that means the basis wasn't linearly independent, and so wasn't a basis after all! Thus if we want to have torsion elements in our modules, we have to give up the convenience of our modules always being free objects. 
				
				What's more, it turns out that even a module without any torsion elements can still fail to be a free module for other reasons, i.e. we may still be unable to specify a basis. We'll see examples when we have a bit more machinery to work with.
				
			\subsection{Split Extensions}
			
				Suppose that $V$ and $W$ are vector spaces. We know how to form their direct sum, $V\oplus W$. In fact, if we start with any vector space we can usually decompose into a direct sum of simpler or `smaller' vector spaces, often in multiple ways. For example, $\mathbb{R}^5 = \mathbb{R}^2\oplus\mathbb{R}^3$, or alternatively $\mathbb{R}^5 = \mathbb{R}^1\oplus\mathbb{R}^4$. 
				
				These different ways to `split' the vector space could help us understand its structure, although they don't seem very powerful in this context because they seem completely obvious. In the world of modules, however, we can't take this procedure.
				
				We can look at the situation a bit differently by considering, say, $\mathbb{R}^2$ and asking how we could `extend' it using $\mathbb{R}^3$ and the direct sum. With vector spaces this only works in one way, and leads to $\mathbb{R}^5$.  What's more, using the quotient we can get back to where we started, since $\mathbb{R}^5/\mathbb{R}^3 = \mathbb{R}^2$.
				
				This ability to put vector spaces together and break them apart using the direct sum is extremely convenient. It works so well because all extensions of one vector space by another are direct sums or, to put it another way, they're all `split extensions'. For modules this is not true, so we need some machinery that can help us understand and enumerate all the possible extensions and in particular to know under what conditions they split like a direct sum. 
	
			\subsection{The Role of Homology}
				
				It's possible to interpret the phrase `homological algebra' in two ways that are both correct:
				
				\begin{itemize}
					\item The study of \textbf{algebra using homology}: here we apply homological techniques we already know to objects besides the topological spaces it was originally invented for. In Part 1, modules play the role of topological spaces and we hope that their homology will tell us useful things about them.
					\item The study of \textbf{homology using algebra}: here the abstract structures that arise from homology are treated as objects of study that have intrinsic interest, which can be justified by how many different fields of study they crop up in.
				\end{itemize}
				
				As in the context of topology our slogan will be that homology measures the gap between `ought' and `is'. To summarise what we just sketched out, recall that:				
				
				\begin{itemize}
					\item Vector spaces are free and never exhibit torsion
					\item The direct sum and quotient of vectors spaces form a nice arithmetic that works very smoothly
					\item Extensions of vector spaces work in a straightforward way
					\item The tensor product is very well-behaved
				\end{itemize}
				
				Perhaps we think these `ought' to be true of modules, too, but they are not. Homology is the tool that will help us deal with this. Perhaps it will even help us develop a more refined perspective, so that we no longer think of these things as shortcomings of the theory of modules.

		\section{From Fields to Commutative Rings}
			
			We'll start with a definition you may or many not have seen before -- it will be useful to us later on and will make our upcoming definitions shorter:
			
			\begin{defn}
				A set $S$ is a \textbf{group} if it's equipped with a binary operation, call it $\star$, such that the following are true (where $a$, $b$ and $c$ are elements of $S$):
				\begin{itemize}
					\item $a\star b$ is defined for every value of $S$ and its value is another element of $S$ (`closure')
					\item There is an element in $S$, call it $e$, such that $a\star e = a$ and $e\star a = a$ (`identity element')
					\item For every element $a$, there is another element $b$ such that $a\star b = e$ and $b\star a = e$ (`existence of inverses')
					\item We can simplify $a\star (b\star c) = (a\star b)\star c$ (`associativity')
				\end{itemize}
				If in addition we always have that $a\star b = b\star a$ we say that $\star$ is \textbf{commutative} and call $(S, \star)$ an \textbf{abelian group}.
			\end{defn}
			
			There's a lot in this definition but here we'll just use it as a sort of building block for the main things we're interested in -- if you haven't seen it before, press on to the examples that come in a moment and it will hopefully make more sense.
			
			The scalars that belong to vector spaces form a field, which we can now define as follows:
			
			\begin{defn}
				A set $S$ is a \textbf{field} if it's equipped with binary operations $+$ and $\times$ that obey the following rules:
				\begin{itemize}
					\item $(S, +)$ is an abelian group
					\item $(S\setminus\{0\}, \times)$ is an abelian group (here $0$ denotes the additive identity)
					\item $a(b + c) = ab + ac = (b + c)a$
				\end{itemize}
			\end{defn}
			
			We're used to thinking of the scalars in a vector space as numbers, but there are some sets of numbers that don't qualify as a field. For example, consider the integers, written $\mathbb{Z}$, that consist of all the positive and negative whole numbers, along with zero. They behave well under addition but their multiplication operation is a bit of a problem: while it's associative, it has closure (you can multiply any two integers and get another integer) and it has an identity element (the number 1), it doesn't have any multiplicative inverses. For example, there's no whole number you can multiply 5 by to get 1. (You \emph{can} multiply 5 by 0.2 to get 1, but 0.2 isn't an integer; the rational numbers, which include things like 0.2, do indeed form a field.)
			
			You may wonder why the choice of number system matters but often real-world problems can only be properly represented using integers or similar systems. Furthermore, in computing and related applications we can easily run into problems by assuming we have access to the rational or the real numbers. Computers can't represent calculations using these with infinite precision, which can lead to errors and messy compromises. Furthermore, commutative rings aren't just the integers and those with more exotic structure can allow for problems to be represented (and solved) in particularly simple and expressive ways. A key example from computing is modular arithmetic, which usually produces number systems that aren't fields.
			
			Examples like this led to the definition of a `field lite' that has most of the properties of a field but drops a couple, making it more permissive:
		
			\begin{defn}
				A set $S$ is a \textbf{ring} if it obeys the same rules as a field except its multiplication operation is allowed either of the following exceptions:
				\begin{itemize}
					\item Multiplication need not be commutative
					\item Multiplicative inverses need not exist
					\end{itemize}
			\end{defn}
			
			Note that fields are also rings, they just don't happen to make use of the exceptions. There is a special class of rings that we might think of as being `halfway to being fields', since they only make use of one of the two exceptions:
			
			\begin{defn}
				A ring in which multiplication is commutative is called a \textbf{commutative ring}.
			\end{defn}
			
			The study of commutative rings (and modules over them) is known as `commutative algebra' and has become one of the central branches of mathematics. Most of homological algebra works the same whether our rings are commutative or not but including non-commutative rings in our theory at this stage adds distracting complications.
			
			Therefore in the rest of Part 1, we will only be concerned with extending the theory of vector spaces to scalars from commutative rings, although we will note some places where care is needed in the setting of rings in general.
			
		\subsection{Examples (TODO)}
			
			List of quick examples we will see many times -- integers, modular arithmetic, polynomials (incl infinite variables).
			
						
		\section{Ideals of Rings}
		
			\subsection{Ideals are Kernels of Homomorphisms}
			
				In the theory of vector spaces, groups or just about any other algebraic objects, a special role is played by subsets that `inherit' the algebraic structure. In the case of rings there are two to consider.
				
				The first is the subring. This is easy enough to deal with: a subring is a subset of a ring that is, itself, a ring with the same multiplication and addition operations. For example, consider the ring of polynomials over the integers $\mathbb{Z}[x]$. Clearly $\mathbb{Z}$ is a subring of $\mathbb{Z}[x]$.
				
				From your previous studies of algebra you might expect the kernels of ring homomorphisms to be subrings but this is not true. Consider the map $f:\mathbb{Z}\to \mathbb{Z}$ that sends each integer to $0$ if it's even and $1$ if it's odd. The image of $f$ is indeed a subring, $\mathbb{Z}/2\mathbb{Z}$. However, its kernel is $2\mathbb{Z}$, the set of all even numbers and this is not a ring, since it doesn't contain the multiplicative identity 1, which isn't an even number. So in the world of rings, kernels of homomorphisms will be something different. This something is known as an `ideal'.
				
				We'll get to an official definition in a moment; first let's consider $2\mathbb{Z}$, the set of all even integers, a bit more closely as a subset of $\mathbb{Z}$. This set is closed under integer addition and also under multiplication by \emph{any} integer (not just the even ones). We have all the additive inverses we need, since if $n$ is even so is $-n$ and we have the additive identity $0$, which is an even number too. However, we do not have the multiplicative identity. 
				
				So an ideal is, in a sense, more than a subring since it must be closed under multiplication by \emph{any} element of the ring (not just the ones in the ideal). But it's also less, since it doesn't have to contain the multiplicative identity. 
				
				The curious name comes from attempts to think about numbers in an `idealised' way; in a sense $2\mathbb{Z}$ is `the number 2, along with all of its possible multiples'; it turns out you can sometimes use ideals as stand-ins for numbers to help prove theorems about those numbers. But that's not how we're going to use the notion of an ideal.
				
				Now to the formal definitions. We would like to say that an ideal of a ring $R$ is a subset of $R$ that is a group under addition of its own elements and multiplication by any element of $R$. However, we have to be careful if the ring might not be commutative! So we end up with the following definitions:
			 
				\begin{defn}
					A \textbf{left ideal} of a ring $R$ is a subset of $R$ that is a group under addition, as well as multiplication on the left by elements of $R$.
				\end{defn}
				
				\begin{defn}
					A \textbf{right ideal} of a ring $R$ is a subset of $R$ that is a group under addition, as well as multiplication on the right by elements of $R$.
				\end{defn}
				
				\begin{defn}
					A \textbf{two-sided ideal} of a ring $R$ is a subset of $R$ that is at the same time a left ideal and a right ideal.
				\end{defn}
				
				\begin{thm}
					If $R$ is commutative, the ideas of `left ideal', `right ideal' and `two-sided ideal' are all equivalent.
				\end{thm}
				
				We will often use the term `ideal' on its own to mean whichever of these concepts is appropriate in context. Even when $R$ is non-commutative the distinction between left and right ideals doesn't always matter; in Part 1 of this course all of our examples will involve commutative rings so it will never be important but it's good to be aware of it for future study. 
				
				The following theorem reminds us why ideals will be central to everything that follows:
				
				\begin{thm}
					Let $R$ and $S$ be any rings and $f:R\to S$ be a ring homomorphism. Then the kernel of $f$ is an ideal of $R$.
				\end{thm}
				
				As you might recall from previous studies, this means that ideals -- not subrings -- are precisely the things it makes sense to quotient out of rings. We will say a great deal more about kernels and quotients as we go through the coming chapters.
				
				\begin{defn}
					Let $S$ be any subset of a ring $R$ (not necessarily a subring or ideal). The \textbf{ideal generated by} $S$, written $\langle S\rangle$, is the set of all sums of elements of $S$, each multiplied by elements of $R$, i.e.
					\[
						\langle S\rangle = \{ \sum_{i=1}^n r_is_i : i \ge 0, r_i\in R, s_i\in S \}
					\]
					We call $S$ a \textbf{generating set} of the ideal.
				\end{defn}
				
				When $S$ has few elements we write them out explicitly. For example, the set of all multiples of 2 and 3 is an ideal of $\mathbb{Z}$, commonly written $\langle 2, 3\rangle$. Note that the values of the $r_i$ can be zero; for instance, $\langle 2, 3\rangle$ contains the number 15, which is $0\times 2 + 5\times 3$.
				
				\begin{defn}
					An ideal is said to be \textbf{finitely generated} if it can be written as $\langle S\rangle$ for a finite set $S$.
				\end{defn}
			
			\subsection{Some Important Kinds of Ideals}
			
				\begin{defn}
					A \textbf{principal ideal} is one that can be generated by a single element of the ring. 
				\end{defn}
				
				Note that an ideal could be specified using two generators but still be principal if it's possible to specify it using only one. For instance, in $\mathbb{Z}$ the ideal $\langle 4, 6\rangle$ contains the element $6-4 = 2$ so it contains all multiples of 2, not just multiples of 4 and 6. That means $\langle 4, 6\rangle = \langle 2\rangle$, so it's a principal ideal even though we met it in a representation that used two generators. This is a bit like a spanning set of a vector space, which might have more vectors in it than a basis would.
				
				Technically a ring is an ideal of itself but we're not usually interested in that case. The following jargon can be useful in connection with that.
				
				\begin{defn}
					An ideal of a ring is said to be a \textbf{proper ideal} if it's not the whole ring.
				\end{defn}
				
				\begin{defn}
					An ideal of a ring is said to be a \textbf{maximal ideal} if it's not a subset of any other ideal of the ring (apart from the whole ring).
				\end{defn}
				
				In $\mathbb{Z}$, not all principal ideals are maximal. For example
				\[
				\langle 6\rangle = \{0, 6, -6, 12, -12, \ldots\}
				\]  is a subset of both \[
				\langle 3\rangle = \{0, 3, -3, 6, -6, 9, -9, 12, -12, \ldots\}
				\]
				and 
				\[
				\langle 2\rangle = \{0, 2, -2, 4, -4, 6, -6, 8, -8, 10, -10, 12, -12, \ldots\}
				\] 
				so $\langle 6\rangle$ isn't maximal in $\mathbb{Z}$.
				
				On the other hand if $p$ is a prime number, $\langle p\rangle$
				is always maximal. To see why, first notice that we can't do the trick in the previous paragraph because a prime doesn't have any factors. Still, suppose that $p = 5$ -- what about an ideal like $\langle 5, 11\rangle$? This certainly contains all the elements of $\langle 5\rangle$ and more, so why is $\langle 5\rangle$ maximal? the answer is that the ideal generated by any pair of integers is actually all of $\mathbb{Z}$. In this case, $\langle 5, 11\rangle$ must include $11 - 2\times 5 = 1$, so it includes every integer.
			
				Now, this trick relies on a theorem called B\'ezout's identity, which says that if $a$ and $b$ are integers that don't have a common divisor, we can make any integer by adding multiples of them together. B\'ezout's identity is true of the integers and of quite a few other rings but not all of them, so watch out! (That could be the slogan for the whole course.)				
				
			\subsection{Finiteness Conditions on Ideals (*)}
			
				This section describes a pair of conditions that impose a limit on the ideals a ring can have -- in particular, we will define a `sequence of ideals' in a moment and these conditions ensure that such a sequence will never `run off to infinity'. If a ring is finite in one of these ways we can often say a little more about it and its modules. We won't do a lot with these but they will come up occasionally later so if you skip this section now you may want to refer back to it when they do.

				\begin{defn}
					A sequence of ideals $J_0\subset J_1\subset J_2\subset \ldots$ such that each ideal is a strict superset of the one to its left is called an \textbf{ascending sequence of ideals}.
				\end{defn}
				
				\begin{defn}
					A ring is called \textbf{Noetherian} if there is no ascending sequence of ideals that is infinitely long.
				\end{defn}
				
				The integers form a Noetherian ring, since any ideal we start with eventually reaches its `upper limit' in an ideal generated by a single prime number. For example, if we start with $\langle 27\rangle$ we will have $\langle 27\rangle\subset\langle 9\rangle\subset\langle 3\rangle$ and there's nowhere to go from there.
				
				This motivates the following definition, which is often useful but looks a bit odd at first glance:
				
				\begin{defn}
					An ideal $I$ of a ring $R$ is called a \textbf{prime ideal} if:
					\begin{itemize}
						\item $I$ is a proper ideal of $R$
						\item There are no two elements $x, y$ of $R$ that are not in $I$ such that their product $xy$ is in $I$
					\end{itemize}
				\end{defn}
				
				To go back to our exaple of the integers, $I = \langle 9\rangle$ is not a prime ideal because it contains 9, which is $3\times 3$, and 3 is not in $I$. But $J = \langle 7\rangle$ is prime because there are no elements of $J$ that are a product of two integers from outside $J$. Note, though, that prime ideals need not be principal: $I = \langle 2, 3\rangle$ is prime because even though it contains non-prime numbers like 6 it also contains all their divisors.
				
				As an example of a non-Noetherian ring, consider the polynomials $\mathbb{R}[x_1, x_2, x_3, \ldots]$ with an infinite number of variables. An example of an infinite descending chain of ideals is given by $\langle x_1\rangle\subset\langle x_1, x_2\rangle\subset\langle x_1, x_2, x_3\rangle\subset\ldots$.
				
				Here is a convenient alternative definition of Noetherian rings:
				
				\begin{thm}
					A ring is Noetherian if and only if all of its ideals are finitely generated.
				\end{thm}
								
				\begin{defn}
					A sequence of ideals $J_0\supset J_1\supset J_2\supset \ldots$ such that each ideal is a strict subset of the one to its left is called a \textbf{descending sequence of ideals}.
				\end{defn}
				
				\begin{defn}
					A ring is called \textbf{Artinian} if there is no descending sequence of ideals that is infinitely long.
				\end{defn}
				
				If a ring is Artinian it is automatically Noetherian, but not \emph{vice versa}. This breaks the apparent symmetry of the definitions and is not completely straightforward to prove but it isn't necessarily surprising when you think that asking for `an infinite chain of things that get smaller and smaller' is quite different from one where the things get bigger and bigger.

				In fact the integers themselves, which are Noetherian, are not Artinian since, for example, $\langle 2\rangle\supset \langle 4\rangle\supset \langle 8\rangle\supset \ldots$ is an infinite descending sequence of ideals. 
				
				We can think of Noetherian-ness as a `bare minimum of good behaviour' for ideals in a ring -- non-Noetherian rings as quite `degenerate' compared with the integers. Artinian-ness, on the other hand, is `exemplary behaviour' that even the integers don't exhibit. We'll often expect a ring to be Noetherian and be sad if it isn't, whereas we'll be pleasantly surprised if it exceeds our expectations by being Artinian as well.
				
		\section{Some Common Kinds of Ring}
		
			\begin{defn}
				A \textbf{domain} is a ring that has no non-zero zero divisors, i.e. elements that are not zero but multiply together to give zero.
			\end{defn}
			
			As a non-example of a domain, consider $\mathbb{Z}/12\mathbb{Z}$, where $3\times 4 = 0$.
			
			\begin{defn}
				An \textbf{integral domain} is a domain where multiplication is commutative.
			\end{defn}
			
			An example of a domain that is not an integral domain would be the $2\times 2$ matrices over $\mathbb{Z}$ with the usual matrix multiplication. Since Part 1 of this course only concerns commutative rings, however, we won't meet any domains that aren't also integral domains.
						
			\begin{defn}
				A \textbf{principal ideal domain} (PID) is an integral domain in which every ideal is principal, i.e. can be generated by a single element.
			\end{defn}
			
			The canonical example of a PID is $\mathbb{Z}$; in a PID many of the features of integer arithmetic hold (for example, a version of `prime factorization'). These features won't be our main focus on this course but PIDs are of great importance because their modules are often the most vector-space-like; many theorems are only guaranteed to be true for modules over PIDs.
			
			An example of an integral domain that isn't a PID is given by the polynomial ring $\mathbb{Z}[x]$ -- for example, the ideal $\langle 2, x\rangle$ is not principal.
				
			\section{The Field of Fractions (*)}
			
				Sometimes we are working with a ring but need to turn it into a field. This transforms it rather radically but we can still find a copy of the original ring inside the transformed verson, and this is often useful. 
				
				One example of this might be familiar from everyday life. Many things can only be represented by integer values, which of course live in a ring. It may be convenient to represent them with rational or real values instead, in part because they form a field. An example would be population migration statistics -- numbers of people are naturally represented by integers, with immigrants counted by positive numbers and emigrants by negative ones. But for statistical calculations it's usually more convenient to treat these as if they were decimals. Technically we pass from the integers to its field of fractions. The fractions of the form $\frac{n}{1}$ are considered equivalent to the original integers so we can `round' the calculated values at the end to get back to integers again.
				
				Turning a ring into a field effectively means giving it all the multiplicative inverses it's currently missing. This is only possible in this case because $\mathbb{Z}$ is an integral domain, which means that if $ab = ac$ then we must have $b = c$. This makes it possible to \emph{imagine} that multiplicative inverses could exist for integers even though they don't. Put another way, if $R$ is a ring that isn't an integral domain, there's an obstruction to carrying out this procedure and no field of fractions can be defined.			
				
				The field of fractions of an integral domain is defined in an exactly analogous way to the way the rational numbers are constructed from the integers:
				\begin{itemize}
					\item A rational number is a `fraction' made of two integers, called the `numerator' and `denominator'.
					\item The only constraint on thsi is that zero can't be the denominator.
					\item Two fractions $\frac{a}{b}$ and $\frac{x}{y}$ are considered equivalent if $ay = bx$. Technically a rational number is actually all the equivalent fractions gathered together.
					\item Addition and multiplication of fractions is defined as in school arithmetic. The additive identity is $\frac{0}{1}$ and the multiplicative identity is $\frac{1}{1}$
				\end{itemize}
				
				\begin{defn}
					Let $R$ be any integral domain. Its \textbf{field of fractions} is the set $\{\frac{a}{b} : a, b \in R\}$ under the equivalence relation $\frac{a}{b}\cong\frac{x}{y}$ if and only if $ay = bx$ with addition and multiplication defined as for the rational numbers.
				\end{defn}
				
				The field of fractions of an integral domain $R$ is denoted $\text{Frac}(R)$.
				
				\begin{thm}
					If $R$ is an integral domain, $\text{Frac}(R)$ is a field.
				\end{thm}
			
			\section{Graded Rings (TODO)}

	\chapter{Introduction to Modules}

			\section{Modules Defined}	
				Ordered lists of integers like this
				\[
					(4, 17, -9, 22, -34 )
				\]
				are of great importance in many applications involving algorithms, data and computer programming in general. These \emph{look} like vectors, but since the integers aren't a field we can't bring our linear algebra knowledge to bear on such objects. It's true that we could pretend these are real numbers (`floating point numbers' in computing parlance) but that can lead to additional compexity and even numerical errors.
				
				This motivates us to try substituting rings for fields in the definition of a vector space, which reveals our main object of study. Giving a definition that works in general for all rings involves a complication, so first we give a special version only for the commutative rings we are focusing on:
				
				\begin{defn}
					An \textbf{$R$-module} is a vector space over a commutative ring ($R$) instead of a field. More formally, an $R$-module is an abelian group $(S, +)$ along with a commutative ring $R$ and an operation of `scalar multiplication' that maps $R\times S\to S$ according to the following rules (here $a$ and $b$ are elements of $S$, $x$ and $y$ are elements of $R$):
					\begin{itemize}
						\item $x(a + b) = xa + xb$ 
						\item $(x + y)a = xa + ya$
						\item $(xy)a = x(ya)$
						\item $1_Ra = a$, where $1_R$ is the multiplicative identity in the ring $R$.
					\end{itemize}
				\end{defn}
				
				Unfortunately it's not common to call the elements of a module `vectors' despite the strong analogy with vector spaces. 
				
				For a non-commutative ring there is an ambiguity about this definition since our practice of writing scalar multiplication on the left of the `vector' is just a notational convention. Suppose we want to multiply the module element $x$ by two ring elements -- first $a$, then $b$. We could write this as $b(ax) = (ba)x$, using multiplication on the right, $(xa)b = x(ab)$. However, if the ring is not commutative it could be that $ab\ne ba$ and so these could be two completely different things! 
				
				This leads us to make the following definitions -- specifying whether a module is a left or right module removes the ambiguity that arises from non-commutative rings:
				
				\begin{defn}
					A \textbf{left $R$-module} over a ring $R$ is defined in exactly the same way as an $R$-module over a commutative ring with the additional stipulation that scalar multiplication is applied on the left.
				\end{defn}
				
				\begin{defn}
					A \textbf{right $R$-module} over a ring $R$ is defined in exactly the same way as an $R$-module over a commutative ring with the additional stipulation that scalar multiplication is applied on the right.
				\end{defn}
				
				Even when working with general rings, it's common to just use left $R$-moules for convenience. However, it's worth bearing in mind that a set can simultaneously have the structure of a left $R$-module and a right $T$-module for some other ring $T$ -- this won't come up very often but it has rather far-reaching consequences.
				
				It turns out that the dual of a right $R$-module is a left $R$-module and \emph{vice versa}. When $R$ is non-commutative, the dual of each of its modules is not of `the same kind' as the original. This creates wrinkles in the theory that we don't have to worry about when $R$ is commutative, as in the case of vector spaces. We'll make a more systematic study of this phenomenon in Part 2 of this course, when we recast the idea of duality in terms of so-called `opposite categories'.

			\section{Examples of Modules}

				We said that every ring is an abelian group equipped with the extra structure of scalar multiplication. In Introduction to Group Theory you may have learned that we have a full classification of abelian groups -- that is, we have a description that covers all of them. We will review this fact quickly here since it will furnish us with a large collection of modules. 
				
				We begin by noticing that we are already very familiar with one abelian group:
				
				\begin{thm}
					The integers, written $\mathbb{Z}$, form an abelian group under addition.
				\end{thm}
				
				Now, when talking about groups, rings and modules it's easy to get mixed up between them. The set $\mathbb{Z}$ is an abelian group under addition; it can also be equipped with a multiplication operation, turning it into a ring. Or, as we'll see shortly, it can be equipped with a scalar multiplication operation, turning it into a module. We'll try to be clear about which structure we care about every time there could ba ambiguity. For the moment, we're just interested in $\mathbb{Z}$ as an abelian group.
				
				In the group of integers $\mathbb{Z}$ we can identify subsets of the form $n\mathbb{Z}$ that contain all the integers multiplied by a set integer $n$. For example, $2\mathbb{Z}$ contains all the even integers, $3\mathbb{Z}$ contains all the integer multiples of 3 and so on. Here we will take it on trust that the quotient $\mathbb{Z}/n\mathbb{Z}$ always makes sense and is an abelian group (this is something you will learn more about in Introduction to Group Theory if you haven't seen it before). It consists of the numbers $\{0, 1, 2, \ldots, n-1\}$ with addition modulo $n$.
				
				We can easily turn any abelian group into a $\mathbb{Z}$-module in the following way:
				\begin{defn}
					By $\mathbb{Z}_n$ we mean the $\mathbb{Z}$-module formed by the abelian group $\mathbb{Z}/n\mathbb{Z}$ with scalar multiplication defined as multiplication modulo $n$, following the rules for a module.
				\end{defn}

			\section{Maps of Modules}
				
				Continuing our analogy with the theory of vector spaces, we want to identify the maps between $R$-modules that `respect their structure' -- the equivalent of homomorphisms of vector spaces. Fortunately we can simply lift the definition from the world of vector spaces and it works just the same:
				
				\begin{defn}
					An \textbf{$R$-module homomorphism} from $A$ to $B$ (where $A$ and $B$ are of course $R$-modules) is a map $f:A\to B$ that satisfies the following, for all $r$ in $R$, $a$ in $A$ and $b$ in $B$:
					\begin{itemize}
						\item $rf(a) = f(ra)$
						\item $f(a + b) = f(a) + f(b)$
					\end{itemize}
				\end{defn}
				
				We've now defined a collection of objects -- the $R$-modules for a given ring $R$ -- and the structure-preserving maps between them. This is enough information to create a `category'. We will call this category \RMod. For two objects $A$ and $B$, the set of all maps $A\to B$ will be written $\hom(A, B)$ as you've seen before in relation to vector spaces. In the first part of this course we develop some rather refined techniques for probing the inner structure and behaviour of $R$-modules; in the second part we generalise those techniques to study a wide range of other mathematical objects that live in categories that are sufficiently similar to \RMod. This generalizability is what makes homological algebra so powerful outside the world of abstract algebra.
				
				In Linear Algebra we studied vector space homomorphisms $V\to W$ as elements of the space $W\otimes V^\star$ but we won't be able to do this from the start for two reasons. The first is that $R$ might not be commutative, in which case $W\otimes V^\star$ might not even make sense. The second is that we can only identify $W\otimes V^\star$ with $\hom(V, W)$ if $V$ and $W$ meet certain criteria. We'll discuss those in a later session.				
		
			\section{Quotients and Products (TODO)}
		
			\section{A Note on Duality (*)}
		
				In your previous study of linear algebra the idea of the dual to a vector space played a crucial part. Indeed, the perfect duality we find in the theory of vector spaces turns out to be a big part of why it's so neat and tidy.
				
				As a reminder, we observed that every finite-dimensional vector space $V$ over a given field has a dual, $V^\star$, that is isomorphic to $V$. Homomorphisms $V\to W$ can be characterised as elements of the tensor product $W\otimes V^\star$. But crucially, $W\otimes V^\star\cong V\otimes W^\star$, so each of these can also be thought of as a homomorphism between the dual spaces going in the opposite direction, i.e. $W^\star\to V^\star$. Concretely this corresponds to `applying the matrix to a (horizontal) covector on the right, instead of applying it to a (vertical) vector on the left'.
				
				Abstractly, we were working in the category $\kVect$ of finite-dimensional vector spaces over the field $k$. This category has an opposite, which is obtained by keeping the objects the same but reversing all the morphisms; if we think of the objects as being the (isomorphic) dual spaces then the morphisms do indeed reverse direction all by themselves.
				
				What this meant for the theory is that everything in $\kVect$ has a mirror-image in the opposite category; almost any statement we make there has not only a dual statement but also a dual proof that we get `for free' by translating our original statement and proof into their mirror-image counterparts.
				
				The situation is subtly different in the category of modules over a particular ring. The dual of such a category is almost never equivalent to it -- worse, it usually isn't a category of modules at all! This means that we lose one of the most reassuring structural symmetries that we had in the theory of vector spaces. We will learn much later on this course that all so-called `abelian categories' are self-dual but that module categories are abelian categories plus additional information that interferes with the duality. 
				
				Although we will have to wait a while to understand this properly, for now it's worth noting that a pair of dual statements in module theory, even when both are true, may have to be understood (and proved) in very different ways. The power of duality in the world of vector spaces is greatly reduced when it comes to modules and we can't expect to find dual versions of things everywhere, nor for them to have exact mirror-image features when we do.

			\section{Free Modules}
				
				Suppose you are managing some bank accounts with money in three currencies: pounds (£), dollars (\$) and yen (\yen). Each account has a balance represented by a number that can be positive or negative. We may was well use a real number to represent the balance, which enables us to represent our whole financial portfolio with a vector in $\mathbb{R}^3$ -- for example, we could write
				\begin{align}
					£ &=& (1, 0, 0) \\
					\$ &=& (0, 1, 0) \\
					\yen &=& (0, 0, 1)
				\end{align}
				so that having £5000, \$1000 and \yen -14000 is the vector $(5000, 1000, -14000)$. 
				
				Notice that we do not need to say what it means to `add a dollar to a yen' -- we simply interpret `\$1 + \yen 1' as `a dollar and a yen'. Indeed, this is one of the things the direct sum does for us; it allows us to form linear combinations of things that have no natural algebraic relationship. The fact that every vector space can be expressed at a direct sum of basis elements in this way is very useful.
				
				As you may have come to expect by now, not every module can do this. On the upside, though, vector spaces are not the only ones that can. We can form the direct sum of multiple copies of a ring just as we can with a field, which leads to the following definition:				
				\begin{defn}
					Let $M$ be and $R$-module. Then $M$ is a \textbf{free module} is there if a set $S$ such that
					\[
						M \cong \bigoplus_{x\in S} R
					\]
				\end{defn}
	
				Free modules perform a universal role in module theory that is important but far from obvious, so as well as stating it we'll offer a proof. To do the proof we'll need the following intermediate result (such results are often called `lemmas'):
				
				\begin{thm}
					Let $M$ and $N$ be $R$-modules and consider a homomorphism $f:M\to N$. Then the following are all true:
					\begin{itemize}
						\item $\ker(f)$ is a submodule of $M$
						\item $\im(f)$ is a submodule of $N$
						\item $\im(f)\cong M/\ker(f)$
					\end{itemize}
				\end{thm}
				
				The first two should be unsurprising from your previous experience with homomorphisms of other algebraic objects: they are more or less what we mean when we say such maps are `structure-preserving'. We will use the last fact in the above list to prove our key result:
				
				\begin{thm}
					Every module can be expressed as a quotient of a free module.
				\end{thm}
				
				\begin{proof}
					Let $M$ be the module in question and $R$ its ring of scalars. Create a free module
					\[
						F = \bigoplus_{m\in M\setminus\{0\}} R
					\]
					Here we are iterating through the nonzero elements of $M$ as basis vectors and creating a (probably enormous) free module whose elements `look like' vectors, i.e. we can think of them as lists of scalars, with each position corresponding to an element of $M$. In particular, $F$ has a natural basis, which is the elements that have a 1 assigned to one element of $M$ and 0 assigned to all the others.
					
					(Aside: This direct sum will often involve an infinite number of basis elements; in that case the direct sum only includes `vectors' that have a finite number of non-zero entries.)
					
					Now consider the homomorphism $f:F\to M$ defined by assigning each basis element to the element of $M$ that its 1 is assigned to and extending linearly. 
					
					Note that $f$ is surjective; in particular, every element of $M$ is mapped to by the basis element representing it, along with numerous other elements of $F$. That means that $\im(f) = M$. It follows from the `lemma' stated above that $M\cong F/\ker(f)$. 
				\end{proof}
				
	\chapter{Exact Sequences and Resolutions}

		\section{Sequences, Exact Sequences and Complexes}
		
			\subsection{Key Definitions}
			
				You may already have seen the ideas in this section quite extensively in the context of topology. If not, this might initially seem rather arbitrary. To avoid getting bogged down, we'll breeze through the definitions here without too much elaboration as we'll see many fresh applications of them in everything that follows.
			
				Let $\{M_0, M_1, M_2, \ldots\}$ be an ordered set of modules over the same ring $R$. Consider the hom-sets $\hom_R(M_i, M_{i+1})$, which contain the functions from each module to the next one in the list. Choosing any one function from each hom-set allows us to construct a \textbf{sequence} of modules
				
				\[ 
				\xymatrix@C+0.3em{
					M_0 \ar[r]_{f_0} & M_1\ar[r]_{f_1} & M_1\ar[r]_{f_2} & \ldots 
				}
				\]
				
				This is a very general situation -- these homomorphisms could be absoltely anything and there's very little we can say about a sequence like this without more information. It turns out that one very useful restriction is the following, which states that if you go down any two consecutive arrows you always arrive at 0, no matter what module element you started with:
				
				\begin{defn}
					A \textbf{chain complex} is a sequence of modules in which $f_{i+1}\circ f_i$ is the zero homomorphism for every $i$.
				\end{defn}
				
				The following theorem gives an alternative, equivalent characterisation of a chain complex that's usually easier to check and that we'll find more expressive:
				
				\begin{thm}
					In a chain complex, $\im(f_i)\subseteq\ker(f_{i+1})$ for all $i$.
				\end{thm}
				
				Note that we only require the image of each map to be contained in the kernel of the next one; that's what's required to ensure that $f_{i+1}\circ f_i$ is the zero homomorphism. However, something special happens when the kernal and image are equal. We will have a lot more to say about why this is special in the rest of this chapter.
				
				\begin{defn}
					A sequence is said to be \textbf{exact at $M_{i+1}$} if  $\im(f_i)=\ker(f_{i+1})$ for all $i$. If it is exact at every module it contains, we call it an \textbf{exact sequence}.
				\end{defn}
				
				It's no exaggeration to say that homological algebra is the study of the ways in which chain complexes fail to be exact sequences. From your previous study you have may already have seen that this inexactness can reveal a surprising amount about structures the chain complexes arise from. If not, this surprising usefulness will hopefully reveal itself in what follows.
			
			\subsection{Split Exact Sequences}
			
				\begin{defn}
					Consider the following short exact sequence:
					\[ 
						\xymatrix@C+0.3em{
							0 \ar[r] & A\ar[r] & B\ar[r] & C\ar[r] & 0
						}
					\]
					The sequence is said to \textbf{split} if $B\cong A\oplus C$; it is then called a \textbf{split exact sequence}.					
				\end{defn}
				
				\begin{thm}
					Let $A$ and $B$ be $R$-modules and $f:A\to B$ be any $R$-module homomorphism. Then $f$ has a left inverse if and only if the following sequence splits:
					\[ 
						\xymatrix@C+0.3em{
							0 \ar[r] & A\ar[r]^{f} & B\ar[r] & \text{coker}(f)\ar[r] & 0
						}
					\]
					and a right inverse if and only if the following sequence splits:
					\[ 
						\xymatrix@C+0.3em{
							0 \ar[r] & \ker(f)\ar[r] & A\ar[r]^{f} & B\ar[r] & 0
						}
					\]
				\end{thm}
				
				This fact alone should be enough to convince us that short exact sequences are important -- whether a homomorphism has left and right inverses is a pretty fundamental property. In the next session we will see that there are things we can do to an entire category of $R$-modules all at once, and in fact that some of these are already long familiar to you from your previous studies in linear algebra. If one of these doesn't preserve the exactness of sequences it may be too `violent' to preserve all the structure we'd otherwise expect.
		
		\section{Resolutions of Modules}
			
			We have said that life would be very easy if only all modules were free (like vector spaces are), but they are not -- which is a good thing, really, because if they were then many interesting modules would not exist. Any module, though, can be described using a setup called a `resolution' that's made entirely of free modules. 
			
			Every module admits a free resolution. We will expand this to projective, injective and flat in the next chapter.
			
			The length of all minimal free resolutions is the same and gives us a version of dimension for modules in general (the projective dimension).
			
		\section{Betti Numbers}
		
			We should start by pointing out that modules are a bit inscrutable and it can be hard to tell whether or not two modules are the same. One way is to find invariants -- if two modules differ in an invariant, they can't possibly be isomorphic.
		
			"We refer to the kernel of the differential map di?1 in the minimal free resolution of M over R as the ith syzygy module of M over R."
		
			Given a minimal free resolution of a module, roughly speaking all but one term "is a syzygy module that represents the relations between the generators of the previous term" and the dimensions of these modules are the Betti numbers. You don't need to compute homology for this. Maybe useful: \url{https://eloisagrifo.github.io/CASeminar/Adam2.pdf}
		
		\section{Maps of Complexes}
		
			Define these maps in the obvious way. The issue of whether exactness is preserved is the key thing to point out.

		\section{Graded Modules}
			\subsection{Definitions}
				
				\url{https://www.proquest.com/openview/21abf19e3503ef5e13c82a7d545c91f5/1?cbl=18750&diss=y&pq-origsite=gscholar} -- we have a PDF downloaded. useful for the next section as well
				
				\begin{defn}
					graded ring
				\end{defn}
		
				\begin{defn}
					graded module
				\end{defn}
				
				\begin{defn}
					graded module homomorphism
				\end{defn}

				
			\subsection{The Hilbert Function}
	
				Not sure whether to do this here. See \url{https://www.math.purdue.edu/~murayama/hilbertpresentation.pdf}. 

	\chapter{Hom and Tensor}

		\section{The Tensor Product Revisited}
		
			\subsection{Multilinear and Linear Maps}
		
				In Introduction to Linear Algebra we defined the tensor product very explicitly in terms of basis vectors of the two spaces. This was possible because we were working in the category of finite-dimensional vector spaces over $\mathbb{R}$. With modules, however, there is often no basis and we must define tensor products more abstractly.
				
				The definition given in this chapter is certainly less concrete than the one we gave for vector spaces, but it's equivalent to it; think of it as a more sophisticated perspective on the same idea that allows it to be applied to all modules, not just vector spaces.

			\subsection{Solving the Universal Problem}
		

			\subsection{Explicit Calculations of Small Tensor Products}
			
				Include as many `tricks' as we can -- e.g. any pure tensor that involves the additive identity in either module will collapse down to $0\otimes 0$.

		\section{Hom and Tensor as Functors}
		
			Exactness is the key property here, and we need to motivate it very clearly. I've seen a suggestion that the exactness of the tensor product essentially tells you when you can change rings without messing up your exact sequences; the exactness of hom relates to duality. But in general hopefully we've seen enough exact sequences by now to appreciate their importance and usefulness.
		
		\section{Projective and Injective Modules}
						
			\subsection{Projective Modules}
			
				TODO: For motivation, start with the fact that hom-sets are sometimes also tensor products in \RMod just as they (always) are for vector spaces -- wikipedia has a nice clear section on this: \url{https://en.wikipedia.org/wiki/Tensor_product_of_modules#As_linear_maps}. Essentially $V^\star\otimes W\cong \hom(V, W)$ only if $V$ and $W$ are finitely generated and projective. The fact that it's not enough for them to be finitely generated gives us some idea that projectivity will be an important property.

				$\hom(P,\_)$ is exact iff $P$ is projective; equivalently, any short exact sequence with $P$ in the last position splits.
				
				In a PID, `projective' and `free' are equivalent. So you need something that fails to be a PID to see the difference.
				
				Example: $\mathbb{Z}/2\mathbb{Z}$ as a module over $\mathbb{Z}/6\mathbb{Z}$. It is projective but couldn't be a free module as it only has two elements, so it can't be a direct sum of copies of the six-element $\mathbb{Z}/6\mathbb{Z}$.
				
				Talk about projective resolutions
			
			\subsection{Injective modules}			
			
				$\hom(\_, I)$ is exact iff $I$ is injective; equivalently, any short exact sequence with $I$ in the first position splits.
				
				Example: $\mathbb{Q}$ as a $\mathbb{Z}$-module.
				
				Need examples of modules that are projective but not injective, \emph{vice versa}, both and neither.
				
				Talk about injective resolutions
				
				Injectivity seems `dual' to projectivity -- explain how they're related and why they're different

		\section{Homological Dimension (*)}

				projective (dually, injective) resolutions allow us to define a version of dimension for modules. 

		\section{The Tensor-Hom Adjunction Revisited}
			
			
		\section{Change of Rings}

			\subsection{Introduction}
			
				Suppose $R$ and $S$ are rings and there is a ring homomorphism $f:R\to S$. If we know a lot about $R$-modules, we might be able to use that to learn about $S$-modules by `translating' what we know using the homomorphism $f$. Conceptually we can imagine ourselves changing the underlying ring of an $R$-module to $S$ to make it into an $S$-module or \emph{vice versa}.
				
				We can form a wider point of view by considering $f$ to be a machine that turns $R$-modules into $S$-modules and \emph{vice versa}. In this section we'll concentrate on the technique applied to individual modules but in the next we'll see how this translates quite easily to the wider perspective, which turns out to be extremely powerful.
				
			\subsection{Restriction of Scalars}
			
				In this section we consider the problem of converting an $R$-module into an $S$-module, where we somehow `embed' a copy of $R$ into $S$. We will show how this works by means of examples. 
				
				The most natural situation occurs when the image of $f$ is isomorphic to $R$. For our first example, suppose we have a $mathbb{Q}$-module $Q$ that we would like to restrict to just a $\mathbb{Z}$-module. This kind of thing occurs often when we have information in floating point format and want to round it off into integers to make data manipulation easier. We find a $f:\mathbb{Z} \to \mathbb{Q}$ such that $f(n) = n/1$. 
				
				We can now define left-multiplication of an element $m$ in $Q$ by an integer $n$ as simply $f(n)m$. Since $f(n)$ is an element of $\mathbb{Q}$, we retain exactly the same module structure except now we can only use integers as our scalars. This turns $Q$ into a $\mathbb{Z}$-module.
				
				For a less intuitive example, consider a $\mathbb{Z}/12\mathbb{Z}$-module $Z_{12}$ that we would like to turn into a $\mathbb{Z}$-module -- a requirement that might come up when dealing with data that involves times or dates. We will choose the homomorphism $f:\mathbb{Z}\to \mathbb{Z}/12\mathbb{Z}$ defined by $f(n) = n \mod 12$.
				
				Again we can define left-multiplication of an element $m$ of $Z_{12}$ by any element $n$ of $\mathbb{Z}$ by $f(n)m$. Thus turns $Z_{12}$ into a $\mathbb{Z}$-module.
				
				There are two things to note about the second example. The first is that when we `restricted' our scalars, we actually ended up with a larger set of scalars that we started with -- we went from a finite ring to an infinite one, in fact. The reason we use the term `restriction' in this context will become clearer in the next section.
				
				The second is that there is no isomorphic copy of $\mathbb{Z}$ inside $\mathbb{Z}/12\mathbb{Z}$, but that doesn't matter. As long as we have a homomorphism from the ring we want to the ring we started with, we can carry out the construction.
				
				Care is obviously needed when restricting scalars: if the homomorphism we chose does violence to its domain as it embeds it in the codomain we may not get the structure we expected.
				
				We do not only need to restrict scalars on one $R$-module; if we have a homomorphism $S\to R$ then we can use the same idea ofn \emph{any} $R$-module. This leads to the idea of a functor that maps the whole of \RMod to \SMod. We will leave out some details here but we will introduce the notation
				\[
					F^\star:\RMod \to \SMod
				\]
				for the functor derived from a ring homomorphism $f:S\to R$. This functor has both a left adjoint and a right adjoint; we will look at those two operations concretely and then in each case see how their adjunction to restriction of scalars resembles the adjunction of tensor to Hom.
				
			\subsection{Extension of Scalars}
			
				As usual, let $R$ and $S$ be rings and $f:R\to S$ a ring homomorphism. This time we start with an $R$-module $M$ and carry out the following operation:
				\[
					S\otimes_R M					
				\]
				Now this only works if $S$ can be considered an $R$-odule, since otherwise the tensor product is nonsense. But this is what $f$ does for us -- we can left-multiply an element $s\in S$ with a scalar $R\in R$ by $f(r)s$, where multiplication now is just the multiplication defined inside $S$ (which it has, because it's a ring!). Here we exploit the ring multiplication in $S$ and the existence of the homomorphism $f$ to build an $R$-module structure onto $S$. 
				
				With that done, the tensor product makes sense. This turns $M$ into an $S$-module because now multiplication of an element $s\otimes_R m$ by a scalar $x\in S$ is just
				\[
					x(s\otimes_R m) = xs\otimes_R m
				\]
				which is an element of the tensor product. 
				
				Note that for the result to be an $S$-module we need to ensure that, for any element of $r$,
				\[
					f(r)s\otimes_R m = s\otimes_R rm
				\]
				but this is guaranteed by the tensor product's definition, since we always have $xa\otimes b = a\otimes xb$.
				
				Again we can extend this to the whole category \RMod and define a functor
				\[
					F_!:\RMod\to \SMod
				\]
				The elements of the $S$-module $F_!(M)$ are the pairs in $S\times M$ with the tensor product's `rules' quotiented out, i.e.
				\[
					(f(r)s, m) \equiv (s, rm)
				\]
				
				As an example, consider the problem of extending an $\mathbb{R}$-module to the complex numbers, i.e. turning an $\mathbb{R}$-module into a $\mathbb{C}$-module. If we write a complex number as $a + bi$ the simplest ring homomorphism is $f:\mathbb{R}\to \mathbb{C}$ defined by $f(x) = x + 0i$. The resulting $\mathbb{C}$-module is called the `complexification' of the original.
				
				You might wonder why the (simpler) restriction of scalars construction wouldn't also solve this problem, but the issue is that then we would need a homomorphism $\mathbb{C}\to \mathbb{R}$ and any we choose will `collapse complex numbers down to mere real numbers' -- the result only \emph{seems} to complexify the original module. 
				
				For example, the most natural function would be $f(a + bi) = a$, which projects each complex number onto its real part, discarding the complex part. While we can use this to restrict scalars and multiply elements of our $\mathbb{R}$-module by complex numbers, we're just fooling ourselves: the homomorphism throws away all the interesting features of the complex scalars.
				
				Extension of scalars, by contrast, really does turn our $\mathbb{C}$-module into a $\mathbb{C}$-module, keeping the same structure of its abelian group of elements but fully expanding the scalars to the complex numbers.
				
				The functor $F_!$ is left adjoint to the functor $f^\star$. TODO: Add explanation and proof
				
			\subsection{Co-Extension of Scalars (*)}

				This section is included for neatness and symmetry although we won't need it for anything that follows. The idea is that just as extension of scalars is left-adjoint to restriction, there is a dual right-adjoint known as co-extension. It will probably not astonish you to learn that in place of the tensor product, co-extension uses Hom.
				
				We will start with an $R$-module and, once again, the desire to `enlarge' it to an $S$-module using a ring homomorphism $f:R\to S$. Note that this goes in the same direction as for extension of scalars (so we will be going `from small to big') and both are the opposite direction to restriction (which goes `from big to small').
				
				We can now think of $\hom_R(S, M)$, which is naturally an $R$-module, as an $S$-module simply by defining, for each $h$, $(sh)(x) =h(sx)$. The left hand side is the homomorphism we get from scalar-multiplying by a ring element $s\in S$; the right hand side simply scales each element of the domain by $s$ before applying the homomorphism, which is obviously another element of $\hom_R(S, M)$.
				
				Therefore we can think of $\hom_R(S, M)$ as an $S$-module and define a new function, the co-extension of scalars:
				\[
					F_\star: \RMod\to \SMod
				\]
				 that takes each $R$-module $M$ to the $S$-module $\hom_R(S, M)$.
				 
				 TODO: Show that this is adjoint to restriction, and show off the symmetry of the whole machine. Ideally show an example, although I don't know of a really meaningful one.  Perhaps useful: \url{https://math.stackexchange.com/questions/1846772/extension-restriction-and-coextension-of-scalars-adjunctions-in-the-case-of-no}
				 
				 As en example, consider the rings $\mathbb{Z}/2\mathbb{Z}$ and $\mathbb{Z}/4\mathbb{Z}$, along with the only ring homomorphism $f: \mathbb{Z}/2\mathbb{Z}\to \mathbb{Z}/4\mathbb{Z}$ given by $f(n) = 2n$. 
				 
				 For simplicity we'll choose $\mathbb{Z}$ as a module over $\mathbb{Z}/2\mathbb{Z}$, with scalar multiplication defined as you should expect -- multiplication by 0 gives 0, multiplication by 1 is the identity. Our goal is to change rings to $\mathbb{Z}/4\mathbb{Z}$ and we will do this using extension of scalars first, then co-extension.
				 
				 Using the formula for extending scalars gives the module
				 \[
				 	\mathbb{Z}/4\mathbb{Z}\otimes_{\mathbb{Z}/2\mathbb{Z}}\mathbb{Z}
				 \]
				 Since this is the tensor product of two very small modules we can write out all of its elements; these turn out to be just the pure tensors that don't involve a zero: $1\otimes 1$, $2\otimes 1$, $3\otimes 1$ along with the zero element $0\otimes 0$. The additive group is isomorphic to the Klein 4-group.
				  
				 On the other hand, co-extending gives the module
				 \[
				 	\hom_{\mathbb{Z}/2\mathbb{Z}}(\mathbb{Z}/4\mathbb{Z}, \mathbb{Z})
				 \]
				 This time there are just two module elements, corresponding to the maps $f(x) = 0$ and $f(x) = x \mod 2$; as we can see, extension and co-extension yielded different $\mathbb{Z}/4\mathbb{Z}$-modules.

		\chapter{Torsion}
		
				
		TODO: All flat modules are torsion-free but not \emph{vice versa} unless the scalars form a PID. If they do the two notions are equivalent. We should include an example of a torsion-free module that's not flat.

		\section{Torsion Elements}
			
			\subsection{Basic Definitions}

				Let's consider the $\mathbb{Z}$-module $\mathbb{Z}_2\oplus\mathbb{Z}_3$. This consists of the following elements:
				\begin{itemize}
					\item $(0, 0)$, the identity element
					\item$(1, 0)$
					\item$(0, 1)$
					\item$(1, 1)$
					\item$(0, 2)$
					\item$(1, 2)$
				\end{itemize}
				Remember that scalar multiplication is done modulo each factor, so for example
				\begin{align}[rcl]
					7(1, 2) &=& (7\times 1 \text{mod 2}, 7\times 2 \text{mod 3})
					 &=& (7 \text{mod 2}, 14 \text{mod 3})
					 &=& (1, 2)
				\end{align}
				This means that sometimes a non-zero scalar can multiply a non-zero vector and produce the zero vector, as in the following example:
				\begin{align}[rcl]
					6(1, 2) &=& (6\times 1 \text{mod 2}, 6\times 2 \text{mod 3})
					 &=& (6 \text{mod 2}, 12 \text{mod 3})
					&=& (0, 0)
				\end{align}
				This leads us to the following definition:
				\begin{defn}
					Suppose an $R$-module $M$ has an element $v$ and a non-zero scalar $k$ such that $kv$ is the zero element of $M$. Then $v$ is called a \textbf{torsion element} of the module.
				\end{defn}
				In $\mathbb{Z}_2\oplus\mathbb{Z}_3$ every element is a torsion element -- a module with this property is called a `torsion module' and a module with no torsion elements is said to be `torsion-free'.

		\section{The Tor Functor}

			\subsection{Definition}

			\subsection{The First Tor Module}

		\section{Example Calculations with Tor}

			\subsection{Flat modules}			
			
				Not sure whether to do this here but maybe it's the right place?

				$F \otimes\_$ is exact iff $F$ is flat.
				
				The Govorov-Lazard Theorem: a module is flat iff it can be expressed as the direct limit of finitely generated free modules 
				
				Talk about flat resolutions
				


	\chapter{Extensions}
	
		\section{Extensions}
			\subsection{Extensions of Modules}
			
				This has some nice pedagogic explanations of the basics: \url{https://www.mas.ncl.ac.uk/library/display_pdf.php?id=270}
				
				Specifically call out the danger of confusion with extension of scalars!
			
			\subsection{Split Extensions}
				Rotman p.420 -- Ext 1 detects when an extension splits. Specifically, There is an isomorphism between Ext1(B,A) and the group of equivalence classes of short exact sequences that start with A and end with B.
		
		\section{The Ext Functor}

			\subsection{Definition}
			
			\subsection{The First Ext Module}
			
				If you apply Hom to an exact sequence and lose exactness, Ext can be used to `repair' it (Rotman p367)
				
		\section{Example Calculations with Ext}
		

	\chapter{The Universal Coefficient Theorem}
		

	\part{Homological Algebra in Other Categories}
	
	\chapter{Abelian Categories}
	
		Criteria for homological algebra to work.
		
		Derived category construction.
	
	\chapter{Group Cohomology}
	
		Show this as a first application of homological algebra outside \RMod.
	
	\chapter{Sheaf and \Cech Cohomology}

		This is discussed in Rotman 6.3 but rather drily -- it would be nice to show examples (not necessarily of calculations but at least of what it means to find homology in these cases).
		
	\chapter{Spectral Sequences}

		Cartan and Eilenberg include a whole chapter of applications of spectral sequences -- a sample of these might be worth including here.

	\chapter{The Cup and Cap Products}

		I think do this in the context of topology? Not sure whether it really belongs here but the structural difference between them is maybe worth teasing out.
	
	
\end{document}