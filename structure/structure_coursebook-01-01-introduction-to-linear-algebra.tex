\documentclass[oneside,english]{amsbook} 
\usepackage[T1]{fontenc} 
\usepackage[latin9]{inputenc} 
\usepackage{amsmath} 
\usepackage{amstext} 
\usepackage{amsthm} 
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{hyperref}
\usepackage[all]{xy}

\makeatletter 

\numberwithin{section}{chapter} 
\theoremstyle{plain} 
\newtheorem{thm}{\protect\theoremname}   
\theoremstyle{definition}   
\newtheorem{defn}[thm]{\protect\definitionname}

\makeatother

\usepackage{babel}   
\providecommand{\definitionname}{Definition} 
\providecommand{\theoremname}{Theorem}
\providecommand{\Cech}{\v{C}ech }
\providecommand{\Poinacre}{Poincar\'e }
\providecommand{\Kunneth}{K{\"u}nneth }

\newcommand{\catname}[1]{{\normalfont\textbf{#1}}}
\newcommand{\RMod}{\catname{RMod\ }}
\newcommand{\SMod}{\catname{SMod\ }}
\newcommand{\im}{\text{\upshape{im}}}
\newcommand{\kVect}{\mathbf{k}\mathbf{Vect}}


\begin{document}
	
	\title{Introduction to Homological Algebra}
	
	\maketitle
	
	\tableofcontents
	
	\chapter*{Welcome!}

Welcome to one of the best-understood branches of mathematics! It's also
one of the most widely-applied, especially in the sciences, engineering,
finance and computing. And -- luckily for us -- it's a sleek, elegant
body of knowledge that fits together like a beautiful machine.

Linear algebra is, in a sense, the study of space. We will start with
examples and motivations that are \textbf{visual} -- you'll be asked to
use your imagination to \emph{see} the phenomena that we want to
describe. Then we'll see how we can generalize those intuitions in order
to make them more widely useful.

We'll mostly \textbf{stay away from specific applications} or concrete
examples, especially at first. These can be very helpful if (but only
if) they come from a domain that you're already familiar with. That
familiarity will vary from person to person. Throughout we will have one
eye on applications in programming, data science and machine learning --
in the later part of the course these connections get a bit more
explicit.

\textbf{The course works} by giving you material to read through during
the week, then providing time to come together as a group to discuss it.
\emph{\ul{Please bring questions}}: the sessions work best if they're
driven by you. Each session builds on those that came before it so don't
let confusion linger.

\textbf{Don't put off the reading} until the last minute! It might not
look like much but the ideas can be very challenging. You'll probably
need to take it in small bites, let the ideas roll around your head for
a few days and re-read some sections with time for reflection in
between. Don't be surprised if something you found difficult was easy
for someone else or \emph{vice versa}; hopefully you can help someone
this week who will help you the next.

This course is \textbf{not very traditional}. Most introductory linear
algebra courses focus on methods of manual calculation and applications
to other areas of pure maths. As a result they don't get very far with
the theory. In modern applications, on the other hand, computations are
done by computers but things like machine learning, signal and image
processing, computer vision and so on often involve advanced topics that
a traditional course can't cover. Our goal is a clear conceptual
understanding of the linear algebra ``machine''.

We also \textbf{don't do proofs} on this course. This is an essential
skill to learn \emph{if} you intend to go into pure mathematics, but
it's less necessary for other fields. Working through Halmos,
\emph{Finite-Dimensional Vector Spaces} would help you to fill in this
gap if you want to do so. I've included a few proofs where I think
they're enlightening or offer a glimpse of some techniques in action.

If you want \textbf{additional reading}, Halmos is good but the book
closest in spirit to this course is \emph{Linear Algebra via External
	Products} by Sergei Winitzki. We will cover quite a few of the topics he
covers, not in the same order as he does and \emph{much} more gently. I
might also make occasional reference to \emph{The Matrix Cookbook} by
Brandt \& Syskind: \url{https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf}.

Please \textbf{stay in touch} and let me know how you're getting on as
we progress through the course; your input will be much appreciated and
help to steer the course in a way that gets everyone to the destination.
Later ideas build on earlier ones, so don't get left behind! We have
plenty of time and a bit of review usually helps everybody, so don't be
shy about asking to go over something again or persisting with questions
until you've got an idea clear. NOTE: I am, regrettably, human, which
means the probability of this course material containing a mistake is
extremely high. Please let me know if you spot one, or if something just
doesn't seem to make sense. That's a great thing to raise in class, or
you can shoot me an email if you prefer.

\part{The First Six Weeks}

The first half of this course contains lots of theory. The second half
is mostly about exploring and applying it. Here's a very terse summary
of the main theory from the first half of the course in sequential order
without any of the explanations, examples, intuition etc. \emph{When you
	feel lost, this is your map}.

\begin{itemize}
	\item
	Linear algebra is the study of \textbf{vector spaces}. A vector space
	consists of:
	
	\begin{itemize}
		\item
		A set of \textbf{vectors}, which we usually think of as
		displacements from a fixed point of origin
		\item
		A set of \textbf{scalars}, which must behave like numbers. On this
		course we (almost) always use the real numbers, denoted \textbf{R},
		as our scalars. We sometimes say a vector space is ``over
		\textbf{R}'' meaning R is the set of scalars.
		\item
		A set of algebraic rules for adding two vectors and ``multiplying''
		a vector by a scalar. A \textbf{linear combination} of vectors is
		one that involves only scalar multiplication and vector addition.
		\item
		Every vector space contains a \textbf{zero vector}.
		\item
		The scalars \textbf{R} can be thought of as the vectors in their own
		special vector space. If it helps, we can think of their scalars as
		being a ``different copy'' of \textbf{R}.
	\end{itemize}
	\item
	Many vector spaces can be given a \textbf{basis}, which means a choice
	of some of its vectors such that all the others can be made by a
	linear combination of some or all of the basis vectors.
	
	\begin{itemize}
		\item
		Every basis of a vector space contains the same number of elements.
		This is called the \textbf{dimension} of the space.
		\item
		This course is only ever concerned with finite-dimensional vector
		spaces. Many of the claims it contains are \emph{not true} of
		infinite-dimensional spaces.
		\item
		Properties of the objects we study are said to be
		\textbf{geometrical} if they don't change when you change basis.
		Geometrical properties are usually more important because they are
		in some sense ``objective''.
	\end{itemize}
	\item
	Every vector space has a \textbf{dual space}, which we can think of as
	its ``mirror image''.
	
	\begin{itemize}
		\item
		The dual space of V is written V* and is a vector space; its
		dimension is the same as the dimension of V.
		\item
		To distinguish between them, we call the elements of V*
		\textbf{covectors}.
		\item
		A covector in V* can act on a vector in V to produce a scalar. When
		we think of it that way we sometimes call it a \textbf{linear
			functional}.
		\item
		The dual space of V* is V itself.
		\item
		If you have chosen a basis for V, you've also chosen an
		\textbf{induced basis} for V*. This can be used to
		\textbf{transpose} vectors in V to covectors in V* with the same
		coordinates, and \emph{vice versa}.
	\end{itemize}
	\item
	We can define an \textbf{inner product} on a vector space to add a bit
	more geometry to it.
	
	\begin{itemize}
		\item
		Many inner products are possible but the one we use on this course
		makes \textless v, w\textgreater{} equal v*w. Note that this depends
		on the chosen basis!
		\item
		With an inner product we can define lengths, distances and angles in
		vector spaces.
		\item
		In particular, we can define an \textbf{orthonormal basis}, which is
		one whose vectors are all the same length and at right angles to
		each other. Many techniques in linear algebra only work in an
		orthonormal basis.
	\end{itemize}
	\item
	The \textbf{span} of a set of vectors is the set of all their linear
	combinations.
	
	\begin{itemize}
		\item
		If the span is not the whole vector space, it will be a
		\textbf{subspace}.
		\item
		If removing any vector from the set reduces the dimension of its
		span, the set is \textbf{linearly independent}.
		\item
		A basis is a linearly independent set of vectors that spans the
		whole space.
	\end{itemize}
	\item
	The \textbf{tensor product} of two vector spaces V and W is a new
	vector space written V⊗W.
	
	\begin{itemize}
		\item
		If a is a basis vector of V and b is a basis vector of W, a⊗b
		represents a basis vector of V⊗W.
		\item
		Then the whole space V⊗W can be thought of as the space spanned by
		all those vectors, subject to certain algebraic rules.
	\end{itemize}
	\item
	A \textbf{linear operator} on the vector space V is an element of V⊗V*
	
	\begin{itemize}
		\item
		It can be thought of as transforming each vector in V into one of
		the other vectors in V.
		\item
		If you've chosen a basis for V then any linear operator can be
		written out compactly in the form of a \textbf{matrix}.
		\item
		Linear operators can act on covectors as well as vectors, but the
		effect is different. Vectors in V are \textbf{contravariant},
		covectors in V* are \textbf{covariant}.
	\end{itemize}
	\item
	The \textbf{exterior product} V∧V is similar to the tensor product
	except with the rule that v∧w = -w∧v.
	
	\begin{itemize}
		\item
		This rule makes it useful for describing the \textbf{oriented size}
		of objects such as vectors, areas, volumes etc.
		\item
		Exterior products can be taken repeatedly, creating the
		\textbf{exterior algebra}. But the maximum number of times this can
		be done is the dimension of V.
		\item
		It's used to define the \textbf{determinant} of a linear operator,
		which describes how much the space is stretched or squeezed by its
		action. This is done using the \textbf{volume form} or top-level
		element of the exterior algebra.
		\item
		The determinant is also useful in finding the \textbf{inverse} of an
		operator, when it exists. The inverse is the operator that undoes
		the action of the original one.
	\end{itemize}
	\item
	The \textbf{direct sum} of V and W is written V⊕W.
	
	\begin{itemize}
		\item
		This gives us a way to combine two vector spaces with the least
		possible interaction between them.
		\item
		The dimension of V⊕W is always the sum of the dimensions of V and W.
		\item
		We write V⊕V = V\textsuperscript{2}, V⊕V⊕V = V\textsuperscript{3}
		and so on.
	\end{itemize}
	\item
	A \textbf{homomorphism} from V to W is an element of W⊗V* (so linear
	operators are just special examples of homomorphisms)
	
	\begin{itemize}
		\item
		The \textbf{kernel} of a homomorphism is the set of all elements of
		the domain that get sent to the zero vector in the codomain.
		\item
		A homomorphism whose kernel consists of only the zero vector is an
		\textbf{isomorphism}.
		\item
		If an isomorphism exists between two vector spaces they are
		``essentially the same'' as far as linear algebra is concerned and
		we say they are \textbf{isomorphic}.
		\item
		Every finite-dimensional vector space over \textbf{R} is isomorphic
		to \textbf{R}\textsuperscript{n} for some suitable choice of n.
	\end{itemize}
\end{itemize}

\chapter{Scalars and Vectors}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 1.1} introduces \emph{scalars} and \emph{vectors} and
	shows the geometric meaning of adding two vectors and multiplying a
	vector by a scalar.
	\item
	\textbf{Section 1.2} describes the ideas of \emph{origin} and
	\emph{basis}, which together help us to express vectors in a more
	convenient way.
	\item
	\textbf{Section 1.3} summarizes what we've done and abstracts some of
	its structural features into the idea of a \emph{vector space}.
\end{itemize}

Because this is the material for the first session, I will go through it
in class -- this section is just provided for your reference if you need
to review it (which you probably will). Session 2's reading should be
available now; you should read through that carefully before the next
session.

\subsection{The Main Protagonists}

\subsubsection{Scalars}

We assume we have access to some system of numbers that's adequate to
our needs. In linear algebra numbers are called \textbf{scalars} --
that's literally all the word scalar means, it's just jargon for
``number''. Our numbers will be used for \emph{measuring} rather than
\emph{counting}, so they need to include fractional values, not just the
whole numbers.

We will call these numbers \textbf{R}. On a calculus course we have to
define these numbers rather carefully; the result is usually what are
called the ``real numbers'' (that's what the R stands for). But most of
the difficulties posed by the real numbers don't affect linear algebra,
so we will skip that step (or rather, defer it to Introduction to
Calculus). You can safely assume that \textbf{R} contains all the
positive or negative numbers that can be represented by finite or
infinite decimals.

The bold \textbf{R} is supposed to echo the special font used when
\textbf{R} is intended to specifically represent the real numbers, in
which case it's printed as \(\mathbb{R}\). I've avoided doing this as
almost everything on this course also works if your scalars are
something else, and there are applied contexts in which, for instance, a
so-called ``finite field'' is used instead. So there's no need to get
hung up on the idea that \textbf{R} represents any \emph{particular} set
of numbers.

When we want to express something symbolically that involves some number
or other, but we don't want to mention any number specifically, we use a
lower-case letter to represent the numbers just like in ordinary school
algebra.

As to why we call them ``scalars'' -- that will become clearer shortly.

\subsubsection{Vectors}

\paragraph{Displacements}

Geometrically, we will think of a vector as a \textbf{displacement} from
one point to another. We always picture a vector as a ``little arrow'',
as in this image of the displacement (i.e., the vector) from point A to
point B:

\includegraphics[width=1.57014in,height=1.65764in]{media/image1.png}I
tend to call the starting-end of an arrow its ``tail'' and the the
pointy end its ``tip''. Imagine standing at A and looking around you;
every point you could move to can be represented, from your perspective,
by a vector whose tail is at A and whose tip is at the point in
question.

This seems like a simple, childish concept and, like most childish
concepts, it turns out to be very important. Vectors are how we find our
way around in space; in fact, before we even go anywhere we orient
ourselves in space by reference to them, as we'll see.

You might ask here: Wait a minute, what's a \emph{point}? And you'd be
right, we haven't defined points. We'll do that shortly. For now, treat
it as something intuitive; a point is a location in space. Again, you
might ask: What is \emph{space} in this context? And again, we'll get to
that, but for these first examples just imagine a flat, two-dimensional
page on which the vectors are drawn. A point is just a location where we
could stick a very fine-pointed pin into the page.

In these notes I'll often use letters to represent vectors, again when
we aren't concerned with any particular vector or when (as in the next
diagram) there are a few different vectors to keep track of. So we don't
mix them up with scalars I will always underline a letter (such as
\ul{u}) when it represents a vector. In books you'll see various
conventions including italics, bold or drawing a line or arrow over the
letter. I like to use the arrow-over notation when writing by hand but
underlining is much easier to do in Word. Sadly I haven't found a way to
add underlining to Geogebra diagrams to you'll have to imagine it.

\paragraph{Vectors are Displacements, not Arrows}

I suggested picturing a vector as a displacement. Consider the following
situation:

\includegraphics[width=2.08676in,height=2.38989in]{media/image2.png}Is
the displacement from A to B (represented by the vector \ul{u}) the same
as the displacement from P to Q (represented by the vector \ul{r})? It's
a tricky question. On the one hand, they're certainly different
\emph{arrows} since they're in different places. On the other hand,
\ul{u} and \ul{r} represent exactly the same \emph{motion}, just with
different starting-points -- and therefore, inevitably, different
end-points too.

This is a delicate matter that we'll need to revisit in the study of the
calculus. But for our purposes on this course we'll say \ul{u} and
\ul{r} are \emph{the same vector} but \emph{at different points}
(meaning their tails are at different points). You'll be pleased to know
that almost all of our work on this course will involve vectors that all
have the same starting-point, so this ambiguity won't come up often.

\paragraph{Adding Vectors}

\includegraphics[width=2.77847in,height=1.78056in]{media/image3.png}

Imagine you walk along \ul{u} from A to B, leaving me at point A. Again,
you'll see all the points in space as displacements from where you are
now. For example, relative to where \emph{you} are, the displacement to
point C is now given by the green arrow, \ul{v}.

But to \emph{me}, the displacement to the same point is given by the
completely different-looking grey arrow \ul{w}. If you want to tell me
how to get to C, there's no point giving me the displacement \ul{v};
that will lead me astray:

\includegraphics[width=2.77153in,height=1.92292in]{media/image4.png}Looking
at it from my perspective, you can see that I need to follow the vector
\ul{w}, and that will get me to the same place as you got to by
following \ul{u}, then following \ul{v}. So it looks like we can get
\ul{w} by combining \ul{u} and \ul{v}, which sounds a bit like adding
two things together to get their combined effect or value. So we define
adding two displacements using this intuitive picture; ``follow \ul{u}
then \ul{v} and you'll get the same result as following \ul{w}'' can be
written in shorthand as

\begin{quote}
	\ul{u} + \ul{v} = \ul{w}
\end{quote}

At this point, that's all this is: shorthand for a geometrical
intuition. But by combining it with another simple intuition we can turn
it into something more powerful.

\paragraph{Scaling Vectors}

A displacement can be thought of as having two aspects: a
\emph{distance} and a \emph{direction}. We will speak much more
precisely about distance next time, but for now we will rely on our
everyday intuitions.

For example, suppose we're both very familiar with \ul{w}, and you want
to tell me about the vector \ul{t} in the following diagram:

\includegraphics[width=4.68154in,height=1.90825in]{media/image5.png}

Perhaps you'd be inclined to describe \ul{t} by saying something like
this: ``Go along \ul{w}, but only halfway''. The vector \ul{t} has the
same direction as \ul{w} but is only half of it. It makes some sense to
write

\ul{t} = 0.5\ul{w}

at least if you sound it out: ``\ul{t} is a half of \ul{w}''. Here the
number 0.5 is acting to scale down the size of \ul{w} without changing
its direction, and this is where numbers in linear algebra get their
strange name ``scalars''. Linear algebra is mostly about vectors, and
numbers often appear just as factors that scale a vector up or down in
size.

Of course we could just as well write

\ul{w} = 2\ul{t}

This and the previous equation are completely equivalent just as they
would be in ordinary algebra: if \ul{t} is half of \ul{w}, it follows as
a certainty that \ul{w} must be double \ul{t}.

Now, as well as positive numbers we also have negative ones and you
might ask what happens when you multiply a vector by a negative number.
What makes sense here is to have the minus sign reverse the direction of
the arrow. For example, in this situation we have two vectors that point
in opposite directions:

\includegraphics[width=2.02352in,height=1.07799in]{media/image6.png}

Vector \ul{b} is twice as long as \ul{a}, so the vector 2\ul{a} would be
the right length but would point exactly the wrong way; so we say \ul{b}
= -2\ul{a}, with the minus sign meaning ``exactly reverse the
direction''.

A word of warning: most of the explicit examples we see will involve
whole-number scalars simply to make the calculation easier to follow.
Don't forget that scalars can have decimal points; depending on the
application a whole-number scalar could be a very rare sight!

\subsection{Origin and Basis}

\subsubsection{The Origin}

\paragraph{Fixing an Origin Point}

It greatly simplifies the theory if we restrict ourselves to a single
starting-point from which to reference all of our displacements. To do
this we simply fix a single point to represent ``where we are'' (or at
least, a point of reference) and have all our vectors' tails rooted
there -- arrows with tails elsewhere are banned. Because all our
displacements start there, we call this point the origin. This way, for
each displacement there is exactly one arrow.

An example of an origin in real life is the point on the globe that is
zero degrees latitude and longitude. From this point, all other points
on the globe can be referenced by their displacement from it. Another,
more everyday example might be the depot from which a company starts its
good deliveries; every place that needs a delivery might be best thought
of as a displacement -- a distance and a direction -- from the depot.

From a pure theory perspective there is no real way to prefer one point
over another when it comes to choosing an origin; we simply ``pick any
point'' (this is more problematic than might first appear). In real-life
applications there is often an obvious and natural choice of origin
\emph{or} we might be able to make a cunning choice to simplify our
calculations.

\includegraphics[width=2.84018in,height=2in]{media/image7.png}Here is
the situation of the previous example with the point A considered as the
origin. It still makes sense to say that

\ul{u} + \ul{v} = \ul{w}

it's just that now we have to mentally slide the vector \ul{v} into the
right position, with its tail at B.

But in fact we have another option: leave \ul{v} where it is and slide
vector \ul{u} over so its tail is at the tip of \ul{v}:

\ul{v} + \ul{u} = \ul{w}

That's because addition of vectors, like addition of ordinary numbers,
is \textbf{commutative}: it doesn't matter what order we do it in, and
no matter what u and v are it will always be true that

\ul{u} + \ul{v} = \ul{v} + \ul{u}

We can reach any point we like from the origin by following the right
displacement, so we can if we wish identify the points in the space with
the vectors that point at them from the origin. So there's often no harm
in calling the point C by the name \ul{w}, which is the vector that gets
you to C. This can simplify things since we don't need to keep track of
two different kinds of entity -- vectors \emph{and} points -- when just
the vectors will do. But this only works if we've agreed on an origin!
Someone else with a different origin would get completely the wrong
points.

\paragraph{The Zero Vector}

The scheme just described has a small gap in it: if every point is
identified as a displacement from the origin, which displacement defines
the origin itself? After all, that's supposed to be a point in space as
well. We need a notion of the ``go-nowhere'' displacement: an arrow with
no length or direction. That would be absurd, but it's easy enough to
say what it means: it means \emph{stay where you are}.

We call this the zero vector and write it as \ul{0}. It really does
behave like zero when we add it to another vector -- just like the
number zero, adding it does nothing:

\ul{u} + \ul{0} = \ul{u}

\ul{0} + \ul{u} = \ul{u}

This makes sense: the first statement says ``follow \ul{u} then go
nowhere'', which is the same overall displacement as just following
\ul{u}, and ``go nowhere, then follow \ul{u}'' is the same again.

The zero vector isn't very useful for giving directions, but it's very
helpful in keeping the system we're building neat and tidy. For example,
recall the earlier example

\includegraphics[width=2.02352in,height=1.07799in]{media/image6.png}

where \ul{b} = -2\ul{a}. Suppose we have the sum

2\ul{a} + \ul{b}

and want to know which vector this is. It \emph{ought} to be a vector,
since it's just two vectors being added together. But if you actually go
along a but twice as far, then along b, you end up back where you
started. And that makes perfect sense because

2\ul{a} + \ul{b} = 2\ul{a} + -2\ul{a} = 2\ul{a} -- 2\ul{a} = \ul{0}

Without the zero vector we would have a ``hole'' in our theory and we'd
have to keep being careful not to fall into it.

The zero vector actually plugs more than one hole. Here is another;
suppose we have some vector x being multiplies by a scalar y: y\ul{x}.
Now suppose y is zero, which is a perfectly good number. What is the
result? Clearly, it's the zero vector \ul{0}. And this makes perfect
geometrical sense: if you go along a vector but you go zero of the
distance along it (0\ul{x}), you've gone nowhere and you're still at the
origin(\ul{0}).

\subsubsection{Basis}

\paragraph{Expressing Vectors in a Basis}

Even after choosing an origin, there are a \emph{lot} of vectors in most
spaces. Think of all the possible displacements from that point in the
2D space we've been working in. There are certainly an infinite number
of them, since any displacement could be repeatedly multiplied by 0.5
and we would get another, perfectly good displacement that is different
from any of the others without ever reaching the zero vector. And there
are a lot more vectors than those.

So we'll struggle if we just try to give a unique name to each vector.
It would be better to have a system that allows us to refer to any
vector we need. The standard system for this is already familiar to you.
Consider this set of vectors:

\includegraphics[width=1.36528in,height=1.06389in]{media/image8.png}We
can imagine that the vector \ul{n} represents a displacement of, say, 1
metre to the north of the origin while the vector \ul{e} is a
displacement of 1 metre to the east.

I claim that any location can be expressed in the form

p\ul{n} + q\ul{e}

where p and q are scalars multiplying \ul{n} and \ul{e} -- let's look at
some examples:

\includegraphics[width=1.56561in,height=1.3375in]{media/image9.png}Vector
\ul{a} is the displacement you get by going 1m North, then 1m East. So
we can write \ul{a} = \ul{n} + \ul{e}. Here p and q are both 1, so we
can drop them from the notation (multiplying by 1 doesn't do anything).

Vector \ul{b} is much the same, but we have to go 2m East instead of
one. So this time we have \ul{b} = \ul{n} + 2\ul{e}.

It looks like \ul{c} is a problem because we need to go 1m East and then
1m South. But we think of 1m South as -1m North, following the idea
about multiplication by a negative scalar discussed above. So we have
\ul{c} = -\ul{n} + \ul{e}.

We call the set \{\ul{n}, \ul{e}\} a \textbf{basis} for the vectors in
this space (a set is just a mathematical name for a collection of
things, and the curly brackets are used to gather things together into a
set -- more on sets next week). We call the vectors in a basis the
\textbf{basis vectors}. All but the most trivial of vector spaces admit
infinitely many different bases; which we choose depends on convenience
or following a standard convention. As we will see later, it's very easy
to translate our work from one basis to another whenever we need to.

\paragraph{Column Vectors}

Taking these basis vectors in an agreed-upon order enables us to shorten
our notation. In our example, any vector can be written as a linear
combination of \ul{n} and \ul{e}, which means we really only need to
know the scalar multiples of each. Conventionally we write those scalars
in a column, enclosed by round brackets. For example, instead of \ul{b}
= \ul{n} + 2\ul{e}, we can write

\[\underline{b} = \begin{pmatrix}
	1 \\
	2
\end{pmatrix}\]

where the 1 means ``\ul{n} scalar multipled by 1'' (which is just
\ul{n}) and the 2 means ``\ul{e} scalar multipled by 2''. Notice that
just looking at this notation doesn't tell us which way round the scalar
multiples are, and this is important: \ul{n}+ 2\ul{e} is a completely
different vector from \ul{e} + 2\ul{n}. In practical situations this
ordering is something we need to agree on in advance, and often it's
conventional. When it comes to developing the theory it doesn't usually
matter much as long as we're being consistent.

This notation means the basis vectors always have the same appearance:

\[\underline{n} = \begin{pmatrix}
	1 \\
	0
\end{pmatrix},\ \ \ \ \ \ \ \ \ \ \underline{e} = \begin{pmatrix}
	0 \\
	1
\end{pmatrix}\]

In a basis with more basis vectors there'll be more numbers, but all but
one of them will be zeros.

This notation makes it very easy to add vectors -- simply add the
scalars ``horizontally across'':

\[\underline{b} + \underline{c} = \begin{pmatrix}
	1 \\
	2
\end{pmatrix} + \begin{pmatrix}
	- 1 \\
	1
\end{pmatrix} = \begin{pmatrix}
	1 + ( - 1) \\
	2 + 1
\end{pmatrix} = \begin{pmatrix}
	0 \\
	3
\end{pmatrix}\]

This works because in the more laborious notation we can use the
ordinary rules of algebra and get

\[\underline{b} + \underline{c} = \underline{n} + 2\underline{e} + ( - 1)\underline{n} + 1\underline{e} = \underline{n} + ( - 1)\underline{n} + 2\underline{e} + 1\underline{e} = 0\underline{n} + 3\underline{e}\]

Again, check carefully that this makes sense to you, and that you see
the connection with the previous way of calculating it. The previous way
is quite a bit easier!

When working with bases of more general spaces it's conventional to use
the letter e for the basis vector names with a little subscript number
to distinguish them. For example, \{e\textsubscript{1},
e\textsubscript{2}, e\textsubscript{3}, e\textsubscript{4}\} could be a
basis of a four-dimensional space. We'll often use this so it's easy to
tell at a glance that something is a basis vector.

A word of warning: in some textbooks, vectors are written as rows of
numbers interchangeably with columns. On this course these ``row
vectors'' represent something subtly different that you'll meet next
week. We will always be careful to write our vectors in column form.

\paragraph{The Dimension of a Vector Space}

It is a surprising fact that for any given vector space, every basis you
can find will have the same number of vectors in it, at least as long as
the bases are finite (infinite-dimensional vector spaces are rather
different beasts and are beyond the scope of this course). That means if
you can find your way around a space using a basis with two elements,
any basis you can use for this purpose will inevitably have two
elements.

We call the number of vectors in a basis for a space the space's
dimension. The spaces we've been working with have been (implicitly)
two-dimensional, which are the easiest ones to draw on paper. The space
we live in, though, is three-dimensional: to find our way around we need
an extra vector that points up or down. Here are column vectors for
those, with \ul{u} representing the displacement ``1m upwards'':

\[\underline{n} = \begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix},\ \ \ \ \ \ \ \ \ \ \underline{e} = \begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix},\ \ \ \ \ \ \ \underline{u} = \begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix}\]

\subsection{Vector Spaces}

\paragraph{Definition of a Vector Space}

So far we have been thinking about orienting ourselves in space using
the idea of displacements. This is all very physical and intuitive and
it relates to a universal experience of being located at a point in
space and surrounded by other things and locations that are separate
from us. But what we've ended up with is in fact even more powerful than
that.

Let's summarize where we are now:

\begin{itemize}
	\item
	A scalar is a number
	\item
	A vector is a displacement.
	\item
	To avoid ambiguity, we'll draw all our vectors with their tails at a
	point called the ``origin'', understanding that a vector can be slid
	anywhere we like and it will still represent ``the same
	displacement''.
	\item
	We can add two vectors by putting them ``tip to tail''.
	\item
	We can scale a vector by multiplying it by a scalar. If the scalar is
	negative, the vector's direction is reversed.
\end{itemize}

A very typical move in mathematics is to extract the ``structure'' from
something like this, removing anything related to the concrete specifics
of what we're trying to describe. The idea is that other thing might
have the same structure but we'll find that hard to notice if we're
stuck in the mind-set of displacements in physical space. If we miss
this similarity of structure, we'll miss an opportunity to apply the
theory to a new problem.

The abstract definition of a vector space is as follows

\begin{itemize}
	\item
	We have a set of things called \textbf{scalars}. Technically we say
	these form a ``field'', which roughly means that they behave very well
	when we do arithmetic with them, so it always makes sense to think of
	scalars as numbers.
	\item
	We have a set of other things we call \textbf{vectors}, but we do
	\emph{not} demand that a vector must represent a physical displacement
	in space (or anything else). What we do demand is that:
	
	\begin{itemize}
		\item
		Two \textbf{vectors can be added}, producing another vector.
		Technically we say the vectors form an ``abelian group'' under
		addition, which again roughly means that adding vectors behaves the
		same way that adding numbers does.
	\end{itemize}
	\item
	The scalars and vectors are connected by the following operation:
	
	\begin{itemize}
		\item
		A vector can be \textbf{multiplied by a scalar}, producing another
		vector.
		\item
		This multiplication obeys the following so-called ``distributive''
		rules:
		
		\begin{itemize}
			\item
			a(\ul{x} + \ul{y}) = a\ul{x} + a\ul{y}
			\item
			(a + b)\ul{x} = a\ul{x} + b\ul{x}
		\end{itemize}
	\end{itemize}
\end{itemize}

We will never refer to the distributive rules explicitly but we will use
them in calculations; they are the same rules as you find in ordinary
school algebra for ``multiplying out brackets''. You might like to
convince yourself that they really are true for the displacements in
space we've been talking about so far.

Note that the abstract definition says nothing about bases. On this
course we confine ourselves to \textbf{finite-dimensional vector spaces}
-- that is those for which a basis can be found that has a finite number
of elements. There is a more advanced theory of infinite-dimensional
vector spaces, which have some important applications, and there are
philosophical questions about the dimensionality of certain vector
spaces for which we can prove that no basis can ever be specified. But
all this is much too fancy for our needs.

\paragraph{A Financial Example}

This abstraction helps us to clarify the more advanced theory. It also
enables us to apply it to things that aren't displacements in physical
space. While those are important in physics and engineering, linear
algebra applies in a much wider domain. The following example comes from
finance; you don't need to know much about finance to follow it but if
you find it perplexing, don't worry.

Suppose you are managing a portfolio of shares as part of, say, a
pension fund, and suppose your fund is only allowed to buy and sell
shares from a list of 100 (for example, the FTSE 100). At any particular
time you would like to represent the values of all the different shares
it owns. Each of the 100 shares has a current market value, and your
portfolio currently owns a certain number of them. We can assume you
might have any combination of the shares at a given time.

We can easily turn this into a vector space in the following way. Each
basis vector will represent a particularly simple portfolio: one that
holds £1 worth of a single share and nothing else. Your current
portfolio is just a sum of all the basis vectors, multiplied by the
current cash value of that particular share that your hold (which will
be zero if you don't hold any of that share).

Suppose now that the trader who sits next to you manages a portfolio of
exactly the same kind, but perhaps wit a different combination of
shares. It would be very easy to combine your portfolios into one: just
add the vectors together.

Furthermore, if you got an injection of cash you might be asked to
double the value of your portfolio without changing its composition
(that is, the proportion of each share in it). This is easy too: simply
scalar-multiply the vector by 2.

This is a perfectly sensible way to represent the portfolio in a
computer system, but it sounds rather exotic to say that the portfolio
is a vector in 100-dimensional space that moves around in that space as
the market prices fluctuate, and as you change the shares held in it by
buying or selling. Nobody can really thinks about this in a geometrical
way, but the language of linear algebra can be an extremely powerful way
to represent it because the theory is rich and well-understood, which
means it comes with lots of tools already-made.

\paragraph{A Seemingly Silly Example That Is In Fact Very Important}

A vector space needs two sets: a set of scalars and a set of vectors.
But what if we choose the same set for both? It turns out this is fine.
We can let the vectors in our space be elements of \textbf{R}, and you
can check for yourself that all the vector space rules apply in a very
straightforward way just because you can do arithmetic with ordinary
numbers.

Is this a silly example? Yes, but also no. What we have here is a vector
space whose bases all have one element, which is the vector (1). So it's
a one-dimensional space. To think about it visually you might prefer to
think of the space as a number line and the vectors as little arrows
with their tails at zero and their tips at different points along the
line. The vector (1) has its tip at the point that is one unit away from
the origin in the positive direction, and different bases just represent
different choices of the unit of measurement and which direction is
positive.

Consider measurements of temperature. There is a current temperature,
say 15°C, which is the ``location'' we're at right now, measured as a
displacement from the origin, 0°C. If it gets 3°C colder, we add the
vector -3°C to the current location vector and end up at 12°C. If it
gets twice as hot, we multiply the vector 12°C by the scalar 2 and get
the new vector 24°C.

As we'll see next time, we can fit a little bit of geometry into this
space, but not very much (in general the more dimensions you have, the
richer the geometry you can build there). But it's very useful for
representing \emph{time}, which we experience as a one-dimensional
phenomenon with a positive direction (forwards in time) and a
conventional unit of measurement (second, hour, year etc). The language
of linear algebra doesn't help us understand time any better than we can
with plain old arithmetic, but it's handy that we can treat time, or
other one-dimensional phenomena, as vector spaces because it makes it
easier to incorporate them into the general theory.

\chapter{The Dual Space}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 2.1} introduces the necessary jargon about sets and
	maps. If you haven't seen these ideas before, budget some extra time
	for this reading and make sure we go through the parts that aren't
	clear to you in class -- they'll be used constantly in the rest of the
	course.
	\item
	\textbf{Section 2.2} introduces \emph{scalar fields} and the
	\emph{dual} of a vector space, which contains \emph{covectors}. These
	ideas can be a little strange at first and we will no doubt spend time
	clarifying the ideas in class. We will use covectors in the upcoming
	sessions to construct many of the basic elements of linear algebra.
\end{itemize}

Remember you don't need to understand everything here on a first reading
-- that's what the class is for! Make a note of any parts that are
unclear and make sure we dispel all confusion during the session.

\subsection{\texorpdfstring{2.1 Maps and Curves
	}{2.1 Maps and Curves }}\label{maps-and-curves}

\subsubsection{2.1.1 Sets and Maps}\label{sets-and-maps}

\paragraph{2.1.1.1 Sets}\label{sets}

In mathematics a \textbf{set} is a collection of things. We can list the
things explicitly by putting them in curly brackets; for example,

\[A = \left\{ 2,\ 4,\ 6,\ 8 \right\}\]

is the set of all positive even whole numbers less than 10. I've
labelled it A for easy reference. We call the things in the set its
\textbf{elements} and say that two sets are the same if they have
exactly the same elements. Most of the sets we work with on this course
contain a lot of elements, so we'll often describe them rather than list
their elements explicitly.

The set A has four elements; this is called the \textbf{cardinality} of
the set or, if we're being less fancy, its \textbf{size}. The size of A
is written as

\[|A|\]

Since the set of scalars \textbf{R} is the set of all numbers that can
be written as a decimal, \textbar{}\textbf{R}\textbar{} is infinite.
Most of the interesting stuff in set theory concerns infinite sets but
we will not worry about any of that on this course; the set theory we
use will be as brief and boring as possible.

Consider the set of all positive whole numbers less than 10:

\[B = \left\{ 1,\ 2,\ 3,\ 4,\ 5,\ 6,\ 7,\ 8,\ 9 \right\}\]

It's clear that B contains all the elements of A, but they're not equal.
We say A is a \textbf{subset} of B or, less often, that B is a
\textbf{superset} of A. For short we write

\[A \subset B\]

to mean that A is a subset of B. However, often we care about a slightly
weaker condition: that A is either a subset of B or is equal to it. Here
we write

\[A \subseteq B\]

This really just says that the set A doesn't contain any element that
isn't in B -- either A is a subset of B or they're actually equal.

\paragraph{2.1.1.2 Maps}\label{maps}

A map A🡪B joins together two sets A and B by a rule that assigns one
element of B to each element of A. Think of A as a set of diners at a
table in a restaurant and B as the set of main courses on the menu:

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3334}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\textbf{A}
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\textbf{B}
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	Alan & & Fish \& Chips \\
	Betty & & Sausages \& Mash \\
	Carl & & Lasagna \\
	Diane & & \\
\end{longtable}

A map A🡪B is a possible order they could make: each person gets one main
course:

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3334}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\textbf{A}
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\textbf{B}
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	Alan & & Fish \& Chips \\
	Betty & & Sausages \& Mash \\
	Carl & & Lasagne \\
	Diane & & \\
\end{longtable}

It's OK for multiple people to order the same thing -- in the example
above, Alan and Carl both ordered Fish \& Chips while Betty and Diane
both went for the Sausages \& Mash. It's also OK is there's something
that nobody orders; in the example above nobody ordered the Lasagne,
which is fine. But everybody must order something. These are the rules
for maps between sets.

Like everything else we'll usually label a map we want to talk about
with a letter. By convention we'll prefer the letters f, g, h for maps
but you can use anything you like. We write f:A🡪B to mean ``f is the
name of a map from the set A to the set B''. If x is some element of A,
f(x) is the element that f maps x to. So f(Alan) is Fish \& Chips. You
can think of the map as a little machine, x as the thing you put into
the machine and f(x) as the thing that comes out:

It's sometimes useful to have jargon terms for the two sets involved in
a map. The ``set the arrows come out of'' is called the \textbf{domain}
of the map, and the ``set the arrows go into'' is the \textbf{codomain}.
So every map transports or translates everything in the domain to
something in the codomain.

We will develop more notions about maps as we go but these will do for
now.

\paragraph{2.1.1.2 Maps R🡪 R}\label{maps-r-r}

Remember that \textbf{R} is our set of scalars. We can define maps R🡪R
-- that is, maps that take in a number and give back a number. These are
of great importance in all fields of mathematics and are studied a great
deal in precalculus and calculus courses. We will have a quick look at a
few examples just to get used to the idea of a map.

Consider f:R🡪R defined by the rule f(x) = x\textsuperscript{2}. This
maps every number to its square. We must check this against the example
we just looked at:

\begin{itemize}
	\item
	It's OK for multiple people to order the same meal: in this case f(-2)
	and f(2) are both equal to 4, but that doesn't matter.
	\item
	It's OK if there's something nobody orders: in this case, there is no
	x such that f(x) = -1. That also doesn't matter.
	\item
	Everyone must order something: for every possible scalar x, the map
	really does produce an output (this is the important one!).
\end{itemize}

Now consider f:R🡪R defined by the rule f(x) = 2x. This maps every number
to its square. We must check this against the example we just looked at:

\begin{itemize}
	\item
	It's OK for multiple people to order the same meal: but in this case
	that doesn't happen, as there are no two different numbers that, when
	you double them, give the same answer.
	\item
	It's OK if there's something nobody orders: again, this doesn't
	happen, since every number is two times some other number.
	\item
	Everyone must order something: for every possible scalar x, the map
	really does produce a valid output.
\end{itemize}

The map f(x) = 2x can be thought of geometrically as a \emph{stretching}
operation performed on the number line, with 0 staying put and the other
points being pulled away from it in both directions.

By drawing a picture if necessary, you might like to try to give
geometrical descriptions of the following maps f:\textbf{R}🡪\textbf{R}:

\begin{itemize}
	\item
	f(x) = 0.5x
	\item
	f(x) = x + 2
	\item
	f(x) = -x
	\item
	f(x) = round(x), which rounds x to the nearest whole number (with .5
	rounding up).
	\item
	f(x) = sin(x)
\end{itemize}

\paragraph{2.1.1.3 Maps R🡪 V are Curves}\label{maps-r-v-are-curves}

NOTE: We won't have much to say about curves on this course but they're
of great interest in calculus and of course they're essential for many
areas of physics, simulations and computer animation. In this section
and the next, focus on the general idea and don't get lost in the
details.

We often use a single letter (usually V or W) to denote a vector space,
but what we really mean is the set of vectors in the space (the scalars
aren't very interesting). So by a map R🡪V I mean a machine that takes in
a number and gives us back a vector, which we can think of a
displacement from the origin or, more simply, a location in space.

In this context it can help to think of \textbf{R} as representing time.
We then imagine moving forward along R like the current position of a
video playing on a computer. As the position changes, the x going into
the map changes and f(x), the vector coming out, changes too. As x moves
forwards in time, f(x) moves around in space; you can think of that as
the video that's playing, showing the current location of the moving
point.

Any map R🡪V is called a \textbf{curve} and they're usually defined by
choosing a basis for V and saying what the map does to each basis
vector. For example,

\[f(x) = \begin{pmatrix}
	\sin x \\
	\cos x
\end{pmatrix}\]

traces out a circle in the 2D vector space V. You can think of the map
as ``embedding'' the number line R into the vector space V in some way;
in this case it's curled around into a circle an infinite number of
times, so that going forward in time means going round and round the
circle.

Notice that you can add two curves together to get another curve: if f
and g are curves, (f+g)(x) = f(x) + g(x), where the addition on the
right is the one defined by the vector space. And you can scalar
multiply them: (af)(x) = a(f(x)), again where the thing on the right is
already defined since it's just a scalar multiple of a vector. So the
curves in a vector space themselves form a vector space!

\paragraph{2.1.1.4 Linearity}\label{linearity}

Suppose f:R🡪V is a curve. It could do absolutely anything but it's said
to be a \textbf{linear map} if it obeys the following rules for all
values of x:

\begin{itemize}
	\item
	f(x + y) = f(x) + f(y)
	\item
	f(xy) = x(f(y))
\end{itemize}

Let's make sure we understand these rules.

First of all, x and y are scalars, so x + y and xy are just ``normal
arithmetic'' with numbers. So f(x + y) is just the value of the map for
some number that happens to be equal to x + y, and f(xy) similarly for
some number that's x times y. That takes care of the things on the left
of the equals signs.

Over on the right of the equals signs we have vector space operations.
So f(x) and f(y) are vectors, and f(x) + f(y) are the two vectors added
together in the way the vector space defines. And x(f(y)) is the scalar
x scalar-multiplying the vector f(y) -- again, however the vector space
defines that to work.

What the rules for linearity say is that the map ``respects the
structure of the vector space''. Linear algebra is, in large part, the
study of linear maps. And calculus is, to some degree, the science of
transforming non-linear maps into linear ones. Since this is not a
calculus course, almost all the maps that concern us on this course will
be linear.

You may want to convince yourself of the fact that if a curve f is
linear, f(0) = \ul{0} and f will always look like a straight line that
passes through the origin, so you can think of the map as embedding R
into V ``respectfully'', without bending or breaking it. There is
actually one, unique exception: a linear map that does not produce the
image of a line at all. Can you work out what the special case is?

\subsection{2.2 The Dual of a Vector
	Space}\label{the-dual-of-a-vector-space}

\paragraph{\texorpdfstring{2.2.1.1 Maps V🡪 \textbf{R} are Scalar
		Fields}{2.2.1.1 Maps V🡪 R are Scalar Fields}}\label{maps-v-r-are-scalar-fields}

We've seen examples of maps \textbf{R}🡪\textbf{R} and \textbf{R}🡪V. Two
more possibilities arise: mapping V🡪\textbf{R} and mapping V🡪V. The last
one will have to wait for a couple of weeks, but maps V🡪\textbf{R} are
very interesting and will be useful to us right away. These assign a
number from \textbf{R} to every vector in V, which again we'll think of
as locations in space. Such maps have a variety of names in mathematics:
you might see them called linear functionals or even just functions but
we will call them \textbf{scalar fields}.

A good way to picture a scalar field is as a ``heat map'' on the space
V. At a point x, f(x) is the temperature there (a scalar value, of
course). As with curves, all kinds of scalar fields are possible and you
might have seen them visualized as ``surface plots'' where the height of
the surface above or below the 2D vector space indicates the value of
the scalar field there.

\includegraphics[width=2.93607in,height=1.44292in]{media/image10.png}Here
the blue surface is a way to visualize the scalar field over V. The
yellow vector indicates the point A, whose ``temperature'' is the height
of the surface there.

This works when V has dimension 2, but we can't do much to visualize
scalar fields on higher-dimensional spaces directly, even the 3D spaces
that we encounter in everyday life.

As with curves, scalar fields may be linear or non-linear and the rules
are exactly the same:

\begin{itemize}
	\item
	f(\ul{v} + \ul{w}) = f(\ul{v}) + f(\ul{w})
	\item
	f(a\ul{v}) = a(f(\ul{v}))
\end{itemize}

Here \ul{v} and \ul{w} are vectors but a, f(\ul{v}) and f(\ul{w}) are
all scalars. When V is two-dimensional, linear scalar fields on V are
always flat planes that pass through the origin. (Well, again there's
one exception you might be able to figure out).

\includegraphics[width=3.65297in,height=1.57078in]{media/image11.png}Again
you can think of the linearity of the map as respecting the structure of
V; it doesn't deform it in any way, just tilts it. This is only a
visualization, and again it only works when V is two-dimensional, but
it's still a very helpful intuition to keep in mind.

For any vector space V, the set of all linear scalar fields forms a new
vector space with the same scalars as V, and this is called the dual
space of V, written V* (usually just pronounced ``vee star''). We will
make some use of V* over the next two weeks to help us build up the main
tools of linear algebra.

Since V* is a vector space, we could call its elements vectors. But that
would often be confusing when we're also working with V, whose elements
are also vectors, and thinking of the elements of V* not a ``little
arrows'' but as scalar fields on V. So that we don't get confused we can
call the elements of V* \textbf{dual vectors} or \textbf{covectors}. I
will usually use the latter term. When we say ``covector'' we mean
``linear scalar field''.

\paragraph{2.2.1.2 Covectors as ``Row
	Vectors''}\label{covectors-as-row-vectors}

To be concrete, let's explicitly limit ourselves to the 2D space we've
been drawing all along, and assume we've decided on a basis
\{\ul{e\textsubscript{1}}, \ul{e\textsubscript{2}}\}. So our vectors
look like

\[\begin{pmatrix}
	x \\
	y
\end{pmatrix} = x\underline{e_{1}} + y\underline{e_{2}}\]

Recall that the column vector notation on the left is less cumbersome
than the ``algebraic'' notation on the right so we usually prefer it; it
would be nice to have a similar notation for covectors.

A scalar field maps a vector like this to a scalar, and it turns out the
only way to do this linearly is to choose two scalars, say a and b, and
use the formula

\[f\begin{pmatrix}
	x \\
	y
\end{pmatrix} = ax + by\]

You should confirm for yourself that any such map is indeed linear
according to the rules in the previous section. It's harder to see why
there couldn't be any other maps V🡪\textbf{R} that aren't of this form
but are still linear, but that is indeed so.

To specify a linear scalar field, then, we only need to specify the
scalars a and b. And since we sometimes want to think of these as
covectors (i.e. members of the dual vector space) it makes sense to
adopt a vector-like notation. So when V is a vector space whose vectors
are written as columns of scalars, the covectors in V* will be written
as rows: (a b).

This makes evaluating a dual vector at a point in V look rather neat:

\[\begin{pmatrix}
	a & b
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = ax + by\]

Remember, this formula tells you how to calculate the height of the
covector at the point at the tip of the vector. When we label a covector
with a letter, we'll underline it just like any other vector, but we'll
add a star to remind us it's a covector. This allows us to make the
notation above even cleaner if we don't need to see the details:

\[\underline{v} = \begin{pmatrix}
	x \\
	y
\end{pmatrix}\]

\[{\underline{w}}^{*} = \begin{pmatrix}
	a & b
\end{pmatrix}\]

\[{\underline{w}}^{*}v = \begin{pmatrix}
	a & b
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = ax + by\]

All of this is really just notational conventions, but \ul{w}*\ul{v}
probably \emph{feels} a bit different from our previous notation w(v),
and it will smooth out some of the algebraic juggling we do later.

Row vectors behave exactly as you'd expect when you add and
scalar-multiply them:

\[\begin{pmatrix}
	a & b
\end{pmatrix} + \begin{pmatrix}
	c & d
\end{pmatrix} = \begin{pmatrix}
	a + c & b + d
\end{pmatrix}\]

\[p\begin{pmatrix}
	a & b
\end{pmatrix} = \begin{pmatrix}
	pa & pb
\end{pmatrix}\]

So they really do behave just vectors ``turned sideways''.

\paragraph{2.2.1.3 Transposition and the Induced
	Basis}\label{transposition-and-the-induced-basis}

In the previous section it was important that we'd already chosen a
basis for V, since that allowed us to use the column vector notation.
Remember that without a basis, we wouldn't know which scalars to put in
the column, so a choice of basis is essential.

When you choose a basis, its column vectors always have the same form:
ever scalar in the column is 0 except for one, which is 1. So regardless
of what the basis vectors look like geometrically, in a 3D space they'll
always look like this when we write them out as columns:

\[\begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix}\]

We can use a similar approach to writing down the basis covectors -- in
this 3D case we'll get

\[\begin{pmatrix}
	1 & 0 & 0
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \begin{pmatrix}
	0 & 1 & 0
\end{pmatrix}\ \ \ \ \ \ \ \ \begin{pmatrix}
	0 & 0 & 1
\end{pmatrix}\]

When we start with a basis of V and ``turn the vectors sideways'' like
this the result is called the \textbf{induced basis} of V*. This act of
``turning a vector sideways'' is called \textbf{transposition} and works
for any vector \emph{as long as we have a basis}. The transpose of a
vector v is written v\textsuperscript{T}, and is a covector; the
transpose of a covector w* is w*\textsuperscript{T}, and is a vector.
Every vector has exactly one ``mirror image'' covector that is its
transpose and \emph{vice versa}. But \emph{which} covector is paired
with it depends on the choice of basis.

CAVEAT: None of the above is guaranteed to work smoothly in
infinite-dimensional spaces. These do crop up in some applied areas
(e.g. quantum mechanics) as well as maths (e.g. the set of polynomials
in a single variable with coefficients in \textbf{R}). But we're only
concerned with the much-more-common finite-dimensional cases here.

\paragraph{2.2.1.4 What is the Dual of the Dual
	Space?}\label{what-is-the-dual-of-the-dual-space}

You might think that since V* is a vector space it would be interesting
to form V**, whose elements would be co-covectors or something. But the
result is either disappointing or a relief, depending on your mood: it
turns out that V** is just V again. So V* is like the reflection of V in
a mirror: reflect again and you end up back where you started.

The reason for this is quite easy to see. V** must contain linear maps
V*🡪\textbf{R}. But we already have a handy representation for those: the
vectors in V. After all, although we've been saying ``a covector acts on
a vector to produce a scalar'', we could just as well have said the
vector is acting on the covector. If we think of it that way, the
vectors in V can be re-imagined or re-interpreted as the required linear
maps V*🡪\textbf{R}.

You might object that these seem to be different objects from the little
arrows we usually imagine as the elements of V. It really does seem that
way, but this is just a difference in how we think about V. What V
actually is mathematically doesn't include the mental pictures we use to
think about it. There is something deeper to be said about this but we
won't be able to formulate it until the end of the course. Then we'll
speak of an ``isomorphism'' representing, roughly speaking, the same
space viewed in different ways.

\chapter{Geometry in Vector Spaces}\label{geometry-in-vector-spaces}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 3.1} introduces \emph{inner products}, which allow us
	to combine two vectors to produce a number.
	\item
	\textbf{Section 3.2} shows how to use an inner product to define
	notions of \emph{length} and \emph{distance} in a vector space.
	\item
	\textbf{Section 3.3} shows how we can also calculate \emph{angles}
	between vectors using the inner product. More importantly, it
	introduces the idea of an \emph{orthonormal basis} and the \emph{dot
		product}, a special kind of inner product. This section won't be used
	very much later so if you're short of time feel free to skim it.
	\item
	\textbf{Section 3.4} introduces the \emph{span} of a set of vectors
	and a \emph{subspace} of a vector space. We could have included these
	in Session 1 but that session was packed enough already.
\end{itemize}

NOTE: This session is lighter than the previous one and there's less to
read. In class I'll be very happy to take questions / requests connected
with any topics we've covered so far. It's important to feel comfortable
with vectors and covectors, and this session's material is designed to
help with that.

Some sections are \textbf{marked with an asterisk} like this (*). Those
are optional. If time is short or you're finding the material difficult,
feel free to skip any or all of those sections. Starred sections are not
needed later in the course. There will be a few more starred sections in
upcoming sessions and many more in the second half of the course.

\subsection{Inner Products and the Dot
	Product}\label{inner-products-and-the-dot-product}

\subsubsection{3.1.1 Inner Products}\label{inner-products}

Last time we saw that every vector space V has a dual, V*, whose
elements are called covectors. I'll usually write covectors as
underlined vectors with a star to remind us they belong to the dual
space. A covector \ul{w}* can act on a vector \ul{v} to produce a
scalar. As long as we have a basis for V we can write is vectors as
columns of scalars, and we can also write the covectors in V* as rows of
scalars using the induced basis. Furthermore, for every vector \ul{v} in
V we can find the covector \ul{v}\textsuperscript{T} in V* that has the
same coordinates but is just ``\ul{v} turned sideways''.

This leads to the following idea. Take two vectors in V,

\[{\underline{v}}_{1} = \begin{pmatrix}
	a \\
	b
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ {\underline{v}}_{2} = \begin{pmatrix}
	c \\
	d
\end{pmatrix}\]

Now we turn one sideways and apply it to the other:

\[{{\underline{v}}_{1}}^{T}{\underline{v}}_{2} = \begin{pmatrix}
	a & b
\end{pmatrix}\begin{pmatrix}
	c \\
	d
\end{pmatrix} = ac + bd\]

Notice it doesn't matter which way around we do it:

\[{{\underline{v}}_{2}}^{T}{\underline{v}}_{1} = \begin{pmatrix}
	c & d
\end{pmatrix}\begin{pmatrix}
	a \\
	b
\end{pmatrix} = ca + db = ac + bd = {{\underline{v}}_{1}}^{T}{\underline{v}}_{2}\ \]

Perhaps surprisingly, this procedure can be very useful -- in fact, it's
really the only trick we'll use for the whole session. If we ignore the
intermediate step of transposing one of them, it's a map that takes in
two vectors and returns a scalar. A map of the form
v\textsubscript{1}\textsuperscript{T}v\textsubscript{2} is an example of
an \textbf{inner product} (you can define fancier inner products than
just these). We usually write an inner product using angle brackets:

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle = {{\underline{v}}_{2}}^{T}{\underline{v}}_{1}\]

Note that the inner product depends on the choice of basis! If you use a
different basis from me, taking the inner products of the same vectors
will result in different scalars. For example, consider these two
vectors \ul{v}\textsubscript{1} and \ul{v}\textsubscript{2}, along with
a fairly standard-looking basis \{\ul{e}\textsubscript{1},
\ul{e}\textsubscript{2}\}:

\includegraphics[width=1.99543in,height=2.71233in]{media/image12.png}Here
we have

\[{\underline{v}}_{1} = \begin{pmatrix}
	3 \\
	4
\end{pmatrix}\ \ \ \ \ \ \ \ \ {\underline{v}}_{2} = \begin{pmatrix}
	4 \\
	- 2
\end{pmatrix}\]

so

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle = \begin{pmatrix}
	3 & 4
\end{pmatrix}\begin{pmatrix}
	4 \\
	- 2
\end{pmatrix} = 12 - 8 = 4\]

\includegraphics[width=2.17778in,height=2.60694in]{media/image13.png}Now
suppose we create a new basis, which again we'll label
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2}\} in the diagram:

This time we have

\[{\underline{v}}_{1} = \begin{pmatrix}
	- 0.5 \\
	4
\end{pmatrix}\ \ \ \ \ \ \ \ \ {\underline{v}}_{2} = \begin{pmatrix}
	3 \\
	- 2
\end{pmatrix}\]

so

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle = \begin{pmatrix}
	- 0.5 & 4
\end{pmatrix}\begin{pmatrix}
	3 \\
	- 2
\end{pmatrix} = - 1.5 - 8 = - 9.5\]

That's a completely different answer from the previous one! So the value
of the inner product depends on the chosen basis. When something depends
on a basis like this we sometimes say it's ``not geometrical'' because
the choice of basis can change without altering any of the geometric
phenomena (such as individual vectors).

But this is not quite right, since geo-metry involves measurement, and
to measure something we need to choose units and, often, other
conventions to go with them (such as which way is the positive direction
to measure in). This is exactly what a basis does. So in a sense it's
very natural that the part of geometry that concerns measuring things
like lengths, distances and angles involves a basis-dependent operation
like the inner product.

\subsection{Length and Distance}\label{length-and-distance}

\subsubsection{3.2.1 The Euclidean Length of a
	Vector}\label{the-euclidean-length-of-a-vector}

The inner product allows us to define the length of a vector and the
distance between vectors very easily.

For any vector \ul{v}, its length is simply

\[\left\| \underline{v} \right\| = \sqrt{\left\langle \underline{v},\underline{v} \right\rangle}\]

The double bars are the standard notation for length, but you'll see
single bars as well in some texts. Personally I find single bars less
fussy, but double bars are a bit more common so we'll stick with them.
As you can see, the length of a vector is the square root of its inner
product with itself.

You can probably see that this comes from Pythagoras's Theorem -- in 2D:

\[\sqrt{\left\langle \underline{v},\underline{v} \right\rangle} = \sqrt{\begin{pmatrix}
		a & b
	\end{pmatrix}\begin{pmatrix}
		a \\
		b
\end{pmatrix}} = \sqrt{a^{2} + b^{2}}\]

This definition of length is an example of a \textbf{norm}, which is a
more abstract concept of ``anything that behaves like a length
measurement in a vector space''. This one is often called the
\textbf{Euclidean norm} and is by far the most commonly-used, but you
might meet others in the future.

The \textbf{Cauchy-Schwarz Inequality} states that

\[\left| \left\langle {\underline{v}}_{1},{\underline{v}}_{2} \right\rangle \right| \leq \left\| {\underline{v}}_{1} \right\|\left\| {\underline{v}}_{2} \right\|\]

This fact is used in a great many proofs of results in geometry and the
calculus.

\subsubsection{3.2.2 The Euclidean Distance Between Two
	Vectors}\label{the-euclidean-distance-between-two-vectors}

The distance between \ul{v}\textsubscript{1} and \ul{v}\textsubscript{2}
is simply

\[\left\| {\underline{v}}_{1} - \underline{v_{2}} \right\|\]

You should check that this actually makes sense by drawing some vectors,
drawing the distance between them (as a straight line) and comparing the
result with this formula and the geometric intuition about adding
vectors from Session 1.

\includegraphics[width=2.33728in,height=2.13871in]{media/image14.png}From
the Cauchy-Schwartz Inequality it's possible to prove the very
reassuring \textbf{Triangle Inequality}:

\[\left\| {\underline{v}}_{1} + {\underline{v}}_{2} \right\| \leq \left\| {\underline{v}}_{1} \right\| + \left\| {\underline{v}}_{2} \right\|\]

This says that if you want to get from point A to point B, it's never
\emph{quicker} to go via point C. You get equality when point C lies on
the straight line between A and B.

\subsubsection{3.2.3 Normalization and Unit
	Vectors}\label{normalization-and-unit-vectors}

A unit vector is a vector whose length is 1. Of course, what counts as a
unit vector is going to depend on your choice of basis. Sometimes,
especially in physics books, you will see a ``hat'' on a letter
representing a vector that indicates it must be a unit vector in the
basis:

\[\underline{\widehat{v}}\]

(I'm sure this thing has many different names but in mathematical
circles I have never heard it called anything but a ``hat''.) Unit
vectors are often needed in practical problem-solving, especially when
calculus gets involved, and a basis made of vectors that are all the
same length is usually more convenient than one with different-length
vectors in it. If the vectors in a basis are all the same length, that
length will \emph{always} be 1 -- hopefully you'll be able to see why in
a moment.

Luckily it's easy to convert any vector \ul{v} into a unit vector that
points in the same direction:

\[\underline{\widehat{v}} = \frac{1}{\left\| \underline{v} \right\|}\underline{v}\]

which is often written as

\[\underline{\widehat{v}} = \frac{\underline{v}}{\left\| \underline{v} \right\|}\]

even though very pedantically speaking we've only defined scalar
multiplication of vectors, not ``scalar division''. But the meaning is
clear, since dividing by a scalar could only be the same as multiplying
by the fraction with 1 on the top and the scalar on the bottom.

This simple process is called \textbf{normalization}. This is
unfortunate, since ``normal'' is one of the most ambiguous and over-used
words in maths. Later in life you might encounter ``normal vectors''
that haven't been normalized! But we can't change such things.

\subsubsection{3.2.4 A Quick Jargon Note (*)}\label{a-quick-jargon-note}

A vector space with a chosen inner product is called an \textbf{inner
	product space}. As mentioned above, inner products can be more
complicated than the ones we've been looking at (specifically they can
be any ``bilinear forms'').

A vector space with a chosen norm (i.e. a way to measure the lengths of
vectors) is called a \textbf{normed space}. The norm need not be derived
from an inner product: there might be some other way to measure
something analogous to ``length'' for a given, non-geometric
application.

A vector space with a way to measure distances between vectors (i.e.
points) is called a \textbf{metric space}. Again, a metric need not be
derived from an inner product or a norm, although every metric space has
an ``induced norm'' which is just the distance between a vector and the
zero vector.

These distinctions are made because you get a more or less general
theory depending on how much structure you define in a space. General
theories, which come about by requiring as little structure as possible,
are widely applicable but more specialized theories are usually
stronger, meaning they can prove things the more general theory can't.

I mention all this as you may see a phrase like ``Let X be an inner
product space'' and now you'll know it just means ``Let X be a vector
space with an inner product defined for it''.

\subsection{3.4 Angles}\label{angles}

\subsubsection{3.4.1 Orthonormal Bases}\label{orthonormal-bases}

Another quality that basis vectors should ideally have is that they
should all be at right angles to each other. The nicest bases will be at
right angles \emph{and} be made of unit vectors, and these are called
\textbf{orthonormal bases}. There is more than one orthonormal basis --
for example, the vectors might be 1 cm long or 1 mile, and they might be
rotated relative to some other orthonormal basis.

In a sense the \emph{theory} has no way to choose between orthonormal
bases; they're all as good as each other. In \emph{practice}, choosing
the right one matters since it amounts to choosing the right units of
measurement and orientation in space. But when we're speaking only in
theoretical terms we often refer to ``the'' \textbf{Cartesian basis},
meaning any one of the orthonormal bases.

\subsubsection{3.4.2 Angles in Orthonormal
	Bases}\label{angles-in-orthonormal-bases}

By definition, in a vector space with an inner product two vectors
\ul{v}\textsubscript{1} and \ul{v}\textsubscript{2} are said to be
\textbf{orthogonal} if and only if

\[\frac{\left\langle {\underline{v}}_{1},{\underline{v}}_{2} \right\rangle}{\left\| {\underline{v}}_{1} \right\|\left\| {\underline{v}}_{2} \right\|}\]

Notice that this is not defined if either of the vectors is the zero
vector. But otherwise, it is zero only when the inner product on top of
the fraction is zero.

As you might or might not know, in ordinary geometry we say two lines
are \textbf{orthogonal} if they meet at a right angle. But two vectors
that are orthogonal in the sense just defined need not actually meet at
a right angle; that will depend on the basis. If we use an orthonormal
basis, however, the two intuitions agree. Here the inner product as
we've defined it is usually called the \textbf{dot product} and written

\[{\underline{v}}_{1} \bullet {\underline{v}}_{2}\]

Under these circumstances we have the surprising (to me, anyway) result
that

\[\frac{{\underline{v}}_{1} \bullet {\underline{v}}_{2}}{\left\| {\underline{v}}_{1} \right\|\left\| {\underline{v}}_{2} \right\|} = \cos{(a)}\]

where cos(a) is the cosine of the angle between the vectors. That means
we can calculate the angle by taking the inverse cosine of the fraction
on the left of the equals sign. Furthermore, if \emph{any} two vectors
have a zero dot product they are orthogonal in the usual geometric
sense, i.e. they are at right angles to each other, because cos(a) = 0
only when a is 90° or -90°.

\subsubsection{3.4.3 The Gram-Schmidt Process
	(*)}\label{the-gram-schmidt-process}

The Gram-Schmidt process gives us a recipe for solving the following
problem: Given a vector space V equipped with a basis and an inner
product, find an orthonormal basis for V. This is useful because, as
we've already seen, many geometric ideas (such as lengths and angles)
only really work properly in an orthonormal basis. How do we find an
orthonormal basis when we need one?

Suppose our basis is \{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ldots, \ul{e}\textsubscript{n}\}. The process goes through the basis
vectors in order and turns each one into an element of an orthonormal
basis. We'll call the new basis vectors \{\ul{v}\textsubscript{1},
\ul{v}\textsubscript{2}, \ldots, \ul{v}\textsubscript{n}\}. We'll start
by doing the difficult part and making our new set of vectors orthogonal
without worrying about their lengths, then we'll normalize them at the
end. The first one is easy: we can just use e\textsubscript{1}:

\[{\underline{v}}_{1} = {\underline{e}}_{1}\]

\includegraphics[width=2.8441in,height=1.9617in,alt={Shape, arrow Description automatically generated}]{media/image15.png}Next
we need to use \ul{e}\textsubscript{2} to choose a
\ul{v}\textsubscript{2} that's orthogonal to \ul{v}\textsubscript{1}. To
do this we use the \textbf{projection} of \ul{e}\textsubscript{2} onto
\ul{v}\textsubscript{1}, defined by

\[\pi({\underline{e}}_{2},\ {\underline{v}}_{1}) = \frac{\left\langle {\underline{e}}_{2},\ {\underline{v}}_{1} \right\rangle}{\left\langle {\underline{v}}_{1},\ {\underline{v}}_{1} \right\rangle}{\underline{v}}_{1}\]

This is a vector that points in the same direction as
\ul{v}\textsubscript{1}, scaled by the ``shadow'' that
\ul{e}\textsubscript{2} casts on the line spanned by
\ul{e}\textsubscript{1}. It has to be a scalar multiple of
\ul{v}\textsubscript{1}; the inner product of \ul{v}\textsubscript{1}
with \ul{e}\textsubscript{2} tells us how much of
\ul{e}\textsubscript{2} ``lines up'' with \ul{v}\textsubscript{1}. We
then have to factor out the length of \ul{v}\textsubscript{1} itself,
which is what's on the bottom of the fraction. If we were to ``normalize
as we go along'' this wouldn't be necessary but later formulae would
turn into a bit more of a mess.

But anyway this is the \emph{opposite} of what we want! We're looking
for a vector that points at right angles to \ul{v}\textsubscript{1} but
what we have here is a vector that points along it. What we're looking
for is the \textbf{rejection} of \ul{v}\textsubscript{1} from
\ul{e}\textsubscript{2}, or what is better known as the
\textbf{orthogonal projection}:

\[\pi^{\bot}({\underline{e}}_{2},\ {\underline{v}}_{1}) = {\underline{e}}_{2} - \frac{\left\langle {\underline{e}}_{2},\ {\underline{v}}_{1} \right\rangle}{\left\langle {\underline{v}}_{1},\ {\underline{v}}_{1} \right\rangle}{\underline{v}}_{1}\]

This is a vector derived from \ul{e}\textsubscript{2} that's orthogonal
to \ul{v}\textsubscript{1}, so it becomes an element of our basis:

\[{\underline{v}}_{2} = \pi^{\bot}({\underline{e}}_{2},\ {\underline{v}}_{1})\]

Notice that in a 2D space there are really only two directions
\ul{v}\textsubscript{2} could be pointing in, but in higher dimensions
there are an infinite number of possibilities; which one we get depends
on the \ul{e}\textsubscript{2} we started with.

We can proceed in the same way for the other vectors in the orthonormal
basis, but we must always reject \emph{all} the vectors that have gone
before. For \ul{v}\textsubscript{3} this looks like

\[{\underline{v}}_{3} = {\underline{e}}_{3} - \frac{\left\langle {\underline{e}}_{3},\ {\underline{v}}_{1} \right\rangle}{\left\langle {\underline{v}}_{1},\ {\underline{v}}_{1} \right\rangle}{\underline{v}}_{1} - \frac{\left\langle {\underline{e}}_{3},\ {\underline{v}}_{2} \right\rangle}{\left\langle {\underline{v}}_{2},\ {\underline{v}}_{2} \right\rangle}{\underline{v}}_{2}\]

and in general the formula is

\[{\underline{v}}_{n} = {\underline{e}}_{n} - \sum_{i = 1}^{n - 1}{\frac{\left\langle {\underline{e}}_{n},\ {\underline{v}}_{i} \right\rangle}{\left\langle {\underline{v}}_{i},\ {\underline{v}}_{i} \right\rangle}{\underline{v}}_{i}}\]

The final step in the process is to normalize all the vectors in our new
basis (see section 3.2.3). Computer applications of many of the more
advanced algorithms we will encounter often use Gram-Schmidt to express
a linear operator in a convenient basis as a first step, making the rest
of the process simpler or more efficient.

\subsection{3.5 Span, Subspaces and
	Codimension}\label{span-subspaces-and-codimension}

\subsubsection{3.5.1 Linear Independence}\label{linear-independence}

We've seen that we can start with a vector space V and find a basis for
it. A basis is a set of vectors that we can add and scalar multiply in
different combinations to make any vector in V. But this is not the
whole story.

First, such a set is really only a basis if it is as small as possible,
meaning if any vector were to be removed from it we would no longer be
able to make all the same vectors we could before. Another way to say
the same thing is: we can't make any of the vectors in the set by
combining other vectors in the set using addition and scalar
multiplication. Such a set of vectors is called \textbf{linearly
	independent}.

For example, consider these vectors:

\[\underline{a} = \begin{pmatrix}
	3 \\
	1
\end{pmatrix}\ \ \ \ \ \ \underline{b} = \begin{pmatrix}
	2 \\
	2
\end{pmatrix}\ \ \ \ \ \underline{c} = \begin{pmatrix}
	- 1 \\
	4
\end{pmatrix}\]

We \emph{know} these can't form a basis of the space they live in
because they're already written in a basis with two elements, which
means this is a two-dimensional space. And in fact,

\[- 2.5a + 3.25b = - 2.5\begin{pmatrix}
	3 \\
	1
\end{pmatrix} + 3.25\begin{pmatrix}
	2 \\
	2
\end{pmatrix} = \begin{pmatrix}
	- 7.5 \\
	- 2.5
\end{pmatrix} + \begin{pmatrix}
	6.5 \\
	6.5
\end{pmatrix} = \begin{pmatrix}
	- 1 \\
	4
\end{pmatrix} = c\]

Since we can make \ul{c} from \ul{a} and \ul{b}, the set \{\ul{a},
\ul{b}, \ul{c}\} is not linearly independent and so it can't possibly be
a basis.

You may like to convince yourself of the following facts:

\begin{itemize}
	\item
	You can have a vector space that contains only one vector, but it has
	to be the zero vector. This is often called a \textbf{trivial vector
		space}.
	\item
	The only basis this vector space can have is \{\ul{0}\}
	\item
	The zero vector can never appear in a basis of any other vector space.
\end{itemize}

\subsubsection{3.5.2 Span and Subspace}\label{span-and-subspace}

Building on the previous idea, we say the \textbf{span} of a set of
vectors is the vector space you get from all their possible linear
combinations. If the set is linearly independent then it will be a basis
of that space.

It may be that the set of vectors spans the whole vector space they come
from, but that doesn't always happen. If there aren't enough linearly
independent vectors in the set, you will end up with a subspace instead
-- a vector space of lower dimension that ``lives inside'' the original
vector space.

In a 2D vector space, the subspaces are straight lines that pass through
the origin. Each of these is a vector space with a single vector in any
of its bases. The same is true in 3D: lines through the origin are
vector subspaces. The lines \emph{must} go through the origin because
every vector space needs a zero vector!

In 3D, we can picture a tilted plane through the origin, which is a 2D
vector subspace. We used this image to help us visualize a covector in
the previous session, but now we're looking at it as a set of
\emph{points} -- seen that way, each of those is a point in 3D space,
and they form a vector subspace of the larger 3D space.

If U is a subspace if V and we write dim(U) for the dimension of U, we
must have

dim(U) \textless{} dim(V)

since a basis of U must be a set of vectors in V that spans U, and if
the basis is as large as one for V it would span the whole of V. You
will also sometimes see the term \textbf{codimension}: the codimension
of U in V is dim(V) -- dim(U), the number of dimensions of V that are
``left over'' after some of them have been taken up by U. A subspace of
codimension 1 is called a \textbf{hyperplane}. A straight line through
the origin (dimension 1) is a hyperplane in a 2D space, but not in a 3D
space.

Let V be a vector space and \ul{w}* any covector. Then the
\textbf{kernel} of \ul{w}* is the set of all vectors \ul{v} such that
\ul{w}*\ul{v} = 0. If \ul{w}* = (0 0) or the higher-dimensional
equivalent then its kernel is all of V, but otherwise its kernel will be
a hyperplane of V. Hyperplanes arise in many algorithmic applications of
linear algebra. For example, in optimization problems hyperplanes can
represent constraints that ``fence off'' regions of the space of
possible solutions.

\chapter{Linear Operators}\label{linear-operators}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 4.1} introduces the \emph{tensor product}, a way of
	combining two vector spaces into one new one, focusing on two specific
	examples that will be very useful to us. There are some tedious
	calculations that we usually leave to a computer but that can be
	enlightening to work through by hand.
	\item
	\textbf{Section 4.2} shows how to use one of these examples to
	represent a \emph{linear operator}, which is a transformation of a
	vector space that ``respects its structure''. A linear operator is
	represented by a square of numbers called a ``matrix'', which you've
	probably heard of.
\end{itemize}

Section 4.1 will probably feel a bit abstract; the point is to be able
to construct the machinery in Section 4.2. The big picture is:

\begin{itemize}
	\item
	A \textbf{linear operator} on a vector space V is a transformation of
	V; in low dimensions we can think of it as a geometrical
	transformation such as a rotation or reflection.
	\item
	Linear operators are vectors that live in the \textbf{tensor product}
	of V with its dual space
\end{itemize}

Looking ahead, in the next session we will examine linear operators more
closely and in Session 6 we construct more general transformations that
can take us from one vector space to another, totally different one. All
these will be done \emph{via} tensor products.

\subsection{4.1 The Tensor Product}\label{the-tensor-product}

\subsubsection{4.1.1 Motivation}\label{motivation}

The tensor product allows us to combine two (or more) vector spaces to
make a new, usually different one. In this session we will stay focused
on two specific examples:

\begin{itemize}
	\item
	the tensor product of a vector space with itself; and
	\item
	the tensor product of a vector space with its dual.
\end{itemize}

In Session 6 we'll have climbed higher up the mountain and be able to
take in a more panoramic view.

You may have noticed already that although we can add two vectors to get
another vector, when we ``multiply'' two vectors using an inner product
we get a scalar. This seems wrong. Why can't we multiply two vectors and
get a vector? It turns out there's no simple and satisfying answer to
this question.

One thing to try is proceed as if we had such an operation and ask how
it should behave. Let us write

\[\underline{v}\bigotimes\underline{w}\]

for the result of multiplying \ul{v} and \ul{w}, whatever that will be.
Treating the vectors just like letters in ordinary algebra, we would
certainly like to have the following be true (here \emph{a} is a
scalar):

\[\left( a\underline{v} \right)\bigotimes\underline{w} = a\left( \underline{v}\bigotimes\underline{w} \right) = \underline{v}\bigotimes(a\underline{w})\]

\[\left( \underline{u} + \underline{v} \right)\bigotimes\underline{w} = \underline{u}\bigotimes\underline{w} + \underline{v}\bigotimes\underline{w}\]

These are just the rules of addition and multiplication from school
algebra (rewrite them with the tensor product symbol replace by the
ordinary ``times sign'' to see this). But, sadly, we won't find any
vector in our original space that answers to these requirements. The
next best thing we can do is create a new space where these things
\emph{do} exist by definition. The miracle is that this rather
foolish-sounding exercise ends up producing something very useful.
Mathematicians often have to proceed this way, trying things that seem
silly or childish to see whether that bear fruit.

\subsubsection{\texorpdfstring{4.1.2 Definition of
		\(V\bigotimes V\)}{4.1.2 Definition of V\textbackslash bigotimes V}}\label{definition-of-vbigotimes-v}

Let V be any finite-dimensional vector space equipped with a basis
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2}, \ldots,
\ul{e}\textsubscript{n}\}. We now make some new symbols that will act as
vectors in the new space -- these will be ``abstract'' vectors and it's
better not to try to picture them geometrically. The symbols are just
a⊗b, where a and b are replaced by each of the possible basis vectors in
turn.

As an example, suppose V is 4-dimensional so the basis is
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ul{e}\textsubscript{3}, \ul{e}\textsubscript{4}\}. The new symbols we
just described are sixteen in number:

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\centering\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\centering\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\centering\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{1}
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{2}
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{3}
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{4}
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{1} &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{2} &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{3} &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{4} \\
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{1} &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{2} &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{3} &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{4} \\
	\ul{e}\textsubscript{4}⊗\ul{e}\textsubscript{1} &
	\ul{e}\textsubscript{4}⊗\ul{e}\textsubscript{2} &
	\ul{e}\textsubscript{4}⊗\ul{e}\textsubscript{3} &
	\ul{e}\textsubscript{4}⊗\ul{e}\textsubscript{4} \\
\end{longtable}

The tensor product V⊗V is the vector space spanned by this set of
vectors. Since V is 4-dimensional, V⊗V is 16-dimensional, which is neat
because 4x4=16 and we're trying to represent a sort of multiplication.
We usually pronounce ``V⊗V'' as ``vee tensor vee''.

Remember, the point of all this was to be able to calculate
\ul{v}⊗\ul{w} for any vectors \ul{v} and \ul{w} -- but how do we do
that? Let's arbitrarily choose \ul{v} =
3\ul{e}\textsubscript{1}+2\ul{e}\textsubscript{2} and \ul{w} =
\ul{e}\textsubscript{1}+2\ul{e}\textsubscript{4}. We would like to be
able to calculate \ul{v}⊗\ul{w} algebraically but we don't have any
algebraic rules for ``⊗'' -\/- it's just a symbol. So we \emph{define}
the two rules at the end of the last section to be true -- the ``school
algebra'' rules for multiplication. This allows us to calculate
\ul{v}⊗\ul{w} using the ``multiplying out brackets'' rules you probably
learned as a child:

\[{\underline{v}\bigotimes\underline{w} = \left( 3{\underline{e}}_{1} + 2{\underline{e}}_{2} \right) \otimes \left( {\underline{e}}_{1} + 2{\underline{e}}_{4} \right)
}{= 3{\underline{e}}_{1} \otimes {\underline{e}}_{1} + 3{\underline{e}}_{1} \otimes 2{\underline{e}}_{4} + 2{\underline{e}}_{2} \otimes {\underline{e}}_{1} + 2{\underline{e}}_{2} \otimes 2{\underline{e}}_{4}
}{= 3{\underline{e}}_{1} \otimes {\underline{e}}_{1} + 6{\underline{e}}_{1} \otimes {\underline{e}}_{4} + 2{\underline{e}}_{2} \otimes {\underline{e}}_{1} + 4{\underline{e}}_{2} \otimes {\underline{e}}_{4}}\]

Note that although this is weird-looking, it really is just a vector in
V⊗V expressed in terms of the basis we chose; it's just that the basis
vectors are now these rather complicated-looking symbols. But if we
wanted to we could just replace the symbols like
``\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{4}'' with single letters
and this would just be an everyday vector.

In terms of jargon, although V⊗V is just a vector space and its elements
are vectors, because it was made using the tensor product we often call
its elements \textbf{tensors} instead. Just remember that a tensor is
nothing special: it's just a vector. If we call it a tensor, we just
mean to draw attention to the fact that the space it lives in was
arrived at using the tensor product.

NOTE FOR THOSE PEEKING INTO MORE ADVANCED STUFF: Most treatments of the
tensor product are more abstract than this, partly to cater for a level
of generality that's needed in applications in higher mathematics or
theoretical physics but not, usually, outside those fields. In
particular I've taken the liberty of assuming we always have access to a
basis for V. While this is no problem in the world of finite-dimensional
vector spaces, if V is allowed to be something wilder it might be
impossible to find a basis for it. But a slightly more technical version
of the construction given in this section \emph{does} still work in
those settings. We will catch a distant glimpse of this later.

\subsubsection{\texorpdfstring{4.1.3 Definition of
		\(V\bigotimes\mathbb{R}\)}{4.1.3 Definition of V\textbackslash bigotimes\textbackslash mathbb\{R\}}}\label{definition-of-vbigotimesmathbbr}

In 1.3.3 we said that \textbf{R} is itself a vector space. I think it
helps to write the ``vectors'' in this case, which are just numbers,
surrounded by brackets or marked in some other way to distinguish them
from the scalars, which are also just numbers. As a vector space
\textbf{R} is one-dimensional, with (1) being the basis element. There
is no real difference between, say, the scalar 5 and the vector (5), but
the brackets help us keep track of how we're thinking of it.

The examples mentioned in 1.3.3 were temperature and time, but really
any quantity that's measured in units can be thought of this way. When
we say ``It's twice as hot as it was yesterday'', the temperature is a
vector measured in, say, centigrade; but the 2 we're multiplying it by
is a scalar -- it's not another temperature. So even though both are
numbers, the distinction between vector and scalar remains useful.

No matter what V is, the dimension of V⊗\textbf{R} will be the same as
that of V, and in fact we'll find that V⊗\textbf{R} is really just the
same thing as V. Let's see why. Suppose V again has the basis
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ul{e}\textsubscript{3}, \ul{e}\textsubscript{4}\}. Then V⊗\textbf{R}
will have the basis

\begin{longtable}[]{@{}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗(1)
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{2}⊗(1)
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{3}⊗(1)
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{4}⊗(1)
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
\end{longtable}

But if this is the basis of a vector space, it will just be the same as
V but with more cumbersome labels for the basis vectors. Instead of
\ul{e}\textsubscript{1} we have \ul{e}\textsubscript{1}⊗(1), but what
difference does that really make? It's just the name of the basis
vector; nothing has actually changed. We can just erase the ``⊗(1)''
from each basis vector's name and end up back to V again.

So V⊗\textbf{R} = V. This may seem unimportant, but it's the key to the
next section and, indeed, the rest of the course. Let's look at an
example vector in V⊗\textbf{R}, \ul{v}⊗(5) with \ul{v} =
2\ul{e}\textsubscript{1} -- 3\ul{e}\textsubscript{3}:

\[\left( 2{\underline{e}}_{1} - 3{\underline{e}}_{3} \right) \otimes \left( 5(1) \right) = 5\left( \left( 2{\underline{e}}_{1} - 3{\underline{e}}_{3} \right) \otimes (1) \right)\]

\[= 5\left( 2{\underline{e}}_{1} \otimes (1) - 3{\underline{e}}_{3} \otimes (1) \right)\]

\[= 10{\underline{e}}_{1} \otimes (1) - 15{\underline{e}}_{3} \otimes (1)\]

If \ul{e}\textsubscript{1}⊗(1) is just a different label for
\ul{e}\textsubscript{1} and likewise \ul{e}\textsubscript{3}⊗(1) for
\ul{e}\textsubscript{3} then this is just 5\ul{v}. So we have \ul{v}⊗x=
x\ul{v} -- it's really just scalar multiplication with a lot of
unnecessary extra steps!

\subsubsection{\texorpdfstring{4.1.4 Definition of
		\(V\bigotimes V^{*}\)}{4.1.4 Definition of V\textbackslash bigotimes V\^{}\{*\}}}\label{definition-of-vbigotimes-v-1}

The idea of this section is to use everything we've done so far to
define a way to transform vectors in V into other vectors in V using
only linear algebra methods. Geometrically, such a transformation might
double all their lengths, or rotate them a quarter-turn, or squash them
all down to a single line -- we'll discuss these examples and more
shortly. Such transformations are called \textbf{linear operators} and
they're of central importance. They are all maps V🡪V that ``respect the
structure of V'' in ways we've seen before.

Remember that the covectors in V* can act on the vectors in V to produce
scalars in a linear way, i.e. in a way that respects the vector space
structure. And we just saw that tensoring a vector with a scalar is just
scalar multiplication, which is also a linear process. To combine these
we form the vector space V⊗V*. Rather than describing this abstractly,
let's jump straight into an example.

To make our work shorter let's suppose V is three-dimensional and has a
basis \{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ul{e}\textsubscript{3}\}. This time we also want the dual space V* with
its induced basis \{\ul{e}\textsubscript{1}*, \ul{e}\textsubscript{2}*,
\ul{e}\textsubscript{3}*\}. The basis of the space V⊗V* will be

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2919}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3748}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{1}*
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{2}*
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{3}*
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{1}* &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{2}* &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{3}* \\
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{1}* &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{2}* &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{3}* \\
\end{longtable}

Let's look at a typical non-basis vector in V⊗V*. Let \ul{v} =
\ul{e}\textsubscript{1} - 2\ul{e}\textsubscript{2} +
3\ul{e}\textsubscript{3} and let \ul{w}* = \ul{e}\textsubscript{2}* -
5\ul{e}\textsubscript{3}*. Then

\[\underline{v}\bigotimes{\underline{w}}^{*} = \left( {\underline{e}}_{1} - 2{\underline{e}}_{2} + 3{\underline{e}}_{3} \right)\bigotimes\left( {{\underline{e}}_{2}}^{*} - 5{{\underline{e}}_{3}}^{*} \right) = {\underline{e}}_{1}\bigotimes{{\underline{e}}_{2}}^{*} - 2{\underline{e}}_{2}\bigotimes{{\underline{e}}_{2}}^{*} + 3{\underline{e}}_{3}\bigotimes{{\underline{e}}_{2}}^{*} - 5{\underline{e}}_{1}\bigotimes{{\underline{e}}_{3}}^{*} - 10{\underline{e}}_{2}\bigotimes{{\underline{e}}_{3}}^{*} + 15{\underline{e}}_{3}\bigotimes{{\underline{e}}_{3}}^{*}\]

This notation is very heavy, with lots of unnecessary symbols, but we'll
stick with it for now because it's very explicit and lays out all the
grisly details in full view -- shortly we'll see how to write this in a
much more compressed and convenient form.

I claimed that this could be used to transform a vector in V into
another vector in V, but how? The answer is ``just apply it on the left,
like a covector''. Again, let's jump straight to an example to see how
this works. We'll use the vector \ul{u} = \ul{e}\textsubscript{1} +
\ul{e}\textsubscript{2} -2\ul{e}\textsubscript{3} and see what happens.
In outline:

\[{\left( \underline{v}\bigotimes{\underline{w}}^{*} \right)\underline{u} = \underline{v}\bigotimes\left( {\underline{w}}^{*}\underline{u} \right)
}{= \underline{v}\bigotimes k
}{= k\underline{v}}\]

where k stands for the scalar \ul{w}*\ul{u}. But since \ul{w}*, \ul{v}
and \ul{u} need to be written out in the basis, there's a bit of
``multiplying out of brackets'' to do in the first step. Here it is in
detail:

\[{\left( \underline{v}\bigotimes{\underline{w}}^{*} \right)\underline{u} = \left( {\underline{e}}_{1}\bigotimes{{\underline{e}}_{2}}^{*} - 2{\underline{e}}_{2}\bigotimes{{\underline{e}}_{2}}^{*} + 3{\underline{e}}_{3}\bigotimes{{\underline{e}}_{2}}^{*} - 5{\underline{e}}_{1}\bigotimes{{\underline{e}}_{3}}^{*} - 10{\underline{e}}_{2}\bigotimes{{\underline{e}}_{3}}^{*} + 15{\underline{e}}_{3}\bigotimes{{\underline{e}}_{3}}^{*} \right)\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right)
}{= \ {\underline{e}}_{1}\bigotimes{{\underline{e}}_{2}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right) - 2{\underline{e}}_{2}\bigotimes{{\underline{e}}_{2}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right) + 3{\underline{e}}_{3}\bigotimes{{\underline{e}}_{2}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right) - 5{\underline{e}}_{1}\bigotimes{{\underline{e}}_{3}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right) - 10{\underline{e}}_{2}\bigotimes{{\underline{e}}_{3}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right) + 15{\underline{e}}_{3}\bigotimes{{\underline{e}}_{3}}^{*}\left( {\underline{e}}_{1} - {\underline{e}}_{2} - 2{\underline{e}}_{3} \right)
}{= \ {\underline{e}}_{1}\bigotimes( - 1) - 2{\underline{e}}_{2}\bigotimes( - 1) + 3{\underline{e}}_{3}\bigotimes( - 1) - 5{\underline{e}}_{1}\bigotimes( - 2) - 10{\underline{e}}_{2}\bigotimes( - 2) + 15{\underline{e}}_{3}\bigotimes( - 2)
}{= \  - {\underline{e}}_{1} + 2{\underline{e}}_{2} - 3{\underline{e}}_{3} + 10{\underline{e}}_{1} + 20{\underline{e}}_{2} - 30{\underline{e}}_{3}
}{= \ 9{\underline{e}}_{1} + 22{\underline{e}}_{2} - 33{\underline{e}}_{3}}\]

\ldots and \emph{that} is just a vector in V, but certainly not the one
we started with! Now, the above is an absurdly long and fiddly
calculation to do by hand, but I recommend you try it at least once just
to see this little machine at close quarters. (Even though it's tedious,
this is a good thing to ask us to go through in class if you're not sure
how all the steps work.)

\subsection{4.2 Linear Operators}\label{linear-operators-1}

\subsubsection{4.2.1 Matrix of a Linear
	Operator}\label{matrix-of-a-linear-operator}

In the previous section we looked at the tensor

\[\underline{v}\bigotimes{\underline{w}}^{*} = {\underline{e}}_{1}\bigotimes{{\underline{e}}_{2}}^{*} - 2{\underline{e}}_{2}\bigotimes{{\underline{e}}_{2}}^{*} + 3{\underline{e}}_{3}\bigotimes{{\underline{e}}_{2}}^{*} - 5{\underline{e}}_{1}\bigotimes{{\underline{e}}_{3}}^{*} - 10{\underline{e}}_{2}\bigotimes{{\underline{e}}_{3}}^{*} + 15{\underline{e}}_{3}\bigotimes{{\underline{e}}_{3}}^{*}\]

The expression on the right of the equals sign uses 48 different
symbols, which is an awful pain to write out and keep track of. And this
is just when V is three-dimensional; it can get a lot worse. However,
notice that most of the symbolic overload comes from the basis vectors,
which are labelled with things like
\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{2}* -\/- six symbols each!
And in fact, since this thing is just a vector, the only information we
care about is the scalar multiples of all the basis vectors.

Now, we absolutely could write this out as a column vector. But it would
have 9 entries, which is a lot, and it wouldn't quite express the fact
that this thing comes from the space V⊗V*. So instead we use the idea of
writing the basis vectors in a grid:

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2919}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3748}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{1}*
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{2}*
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\ul{e}\textsubscript{1}⊗\ul{e}\textsubscript{3}*
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{1}* &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{2}* &
	\ul{e}\textsubscript{2}⊗\ul{e}\textsubscript{3}* \\
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{1}* &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{2}* &
	\ul{e}\textsubscript{3}⊗\ul{e}\textsubscript{3}* \\
\end{longtable}

Replacing each basis vector with its scalar multiple gives us a grid of
just scalars that completely describes the tensor, as long as we've
agreed on the basis (and the order in which we're writing its elements
down):

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2919}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3748}}
		>{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		0
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		1
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		-5
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	0 & -2 & -10 \\
	0 & 3 & 15 \\
\end{longtable}

This is called the \textbf{matrix} representation of the tensor, and
it's usually written with brackets around it just like a vector or
convector:

\[\begin{pmatrix}
	0 & 1 & - 5 \\
	0 & - 2 & - 10 \\
	0 & 3 & 15
\end{pmatrix}\]

Now, we can apply this matrix to the vector \ul{u} using a rather
complicated-looking rule that is just derived directly from the ``plain
old algebra'' we used in the previous section:

\[\begin{pmatrix}
	0 & 1 & - 5 \\
	0 & - 2 & - 10 \\
	0 & 3 & 15
\end{pmatrix}\begin{pmatrix}
	1 \\
	- 1 \\
	- 2
\end{pmatrix} = \begin{pmatrix}
	0 \times 1 + 1 \times ( - 1) + ( - 5) \times ( - 2) \\
	0 \times 1 + ( - 2) \times ( - 1) + ( - 10) \times ( - 2) \\
	0 \times 1 + 3 \times ( - 1) + 15 \times ( - 2)
\end{pmatrix} = \begin{pmatrix}
	9 \\
	22 \\
	- 33
\end{pmatrix}\]

\ldots which is the same answer as we got before. To decode what's
happening, think of the matrix as a vertical stack of three covectors.
Each acts on the vector in turn and becomes the coordinate of the new
vector in that vertical position. This will be easier to appreciate in
real time so we'll certainly go through a calculation like this in
class.

When working with matrix notation, the vector space operations are
essentially the same as we do with vectors written as columns, or
covectors written as rows. Adding two matrices just means adding their
corresponding components:

\[\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix} + \begin{pmatrix}
	w & x \\
	y & z
\end{pmatrix} = \begin{pmatrix}
	a + w & b + x \\
	c + y & d + z
\end{pmatrix}\]

Scalar multiplication works exactly as you expect as well:

\[x\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix} = \begin{pmatrix}
	xa & xb \\
	xc & xd
\end{pmatrix}\]

This shouldn't be surprising, since the matrix is really just a
representation, in coordinates, of a vector in V⊗V*.

\subsubsection{4.2.2 Geometric Examples}\label{geometric-examples}

What kinds of transformation can a linear operator achieve? In two
dimensions the following types are the ones that ``keep V looking the
same'' -- that is, after the transformation we still have a 2D vector
space:

\begin{longtable}[]{@{}
		>{\centering\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
		>{\centering\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\centering
		\[\begin{pmatrix}
			a & 0 \\
			0 & b
		\end{pmatrix}\]
		
		Scaling (stretching or squeezing) by a factor of a in the direction of
		the first basis vector and b in the direction of the second
	\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
		\[\begin{pmatrix}
			1 & a \\
			b & 1
		\end{pmatrix}\]
		
		Shearing (pushing or pulling) by a factor of a in the direction of the
		first basis vector and b in the direction of the second.
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	\(\begin{pmatrix}
		\cos{(a)} & \sin{(a)} \\
		- \sin{(a)} & \cos{(a)}
	\end{pmatrix}\)
	
	Rotation about the origin by the angle a & \(\begin{pmatrix}
		\cos{(2a)} & \sin{(2a)} \\
		\sin{(2a)} & {- cos}{(2a)}
	\end{pmatrix}\)
	
	Reflection in a line that makes an angle of a with the direction of the
	first basis vector. \\
\end{longtable}

These 2D examples give a flavour of what you can expect in other
dimensions too: stretching, shearing, rotating and reflecting. One very
important matrix (at least for the development of the theory) is the
\textbf{identity matrix}, which always has 1s along its
top-left-to-bottom-right diagonal and zeros everywhere else, e.g.

\[\begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix}\]

As you can see by experimentation, this is the ``do-nothing'' operator
that transforms every vector into itself. This is similar to x + 0 or 1x
in algebra.

You might find this Geogebra resource useful for experimenting with the
transformations in the table above:
\url{https://www.geogebra.org/m/YCZa8TAH}. There's also a cute
visualizer here that shows the effect of the linear operator on an
image: \url{https://web.ma.utexas.edu/users/ysulyma/matrix/}. These only
work for operators on \textbf{R}\textsuperscript{2} but they're still
very useful for building your intuition.

We have more possibilities, too. Consider the matrix

\[\begin{pmatrix}
	1 & 0 \\
	0 & 0
\end{pmatrix}\]

Let's see how this acts on an arbitrary vector:

\[\begin{pmatrix}
	1 & 0 \\
	0 & 0
\end{pmatrix}\begin{pmatrix}
	a \\
	b
\end{pmatrix} = \begin{pmatrix}
	a \\
	0
\end{pmatrix}\]

The result is a vector that's a multiple of the first basis vector only,
so all the results, no matter what vector we started with, will end up
on a single line along the first basis vector's direction. This is
called a \textbf{projection} of the space onto a lower-dimensional space
(the 1D line) -- we will look again at such things in the final session.

These specific examples might be very relevant to you if, for example,
you have an interest in computer graphics and CAD. Or they may not be
important at all if your interest is in, say, machine learning where
dimensions tend to be very high so geometric pictures aren't very
helpful.

Never forget that the matrix representation of a linear operator
\emph{depends on the choice of basis}. The same actual transformation
(e.g. a rotation by 30°) will have a very different matrix if you change
the basis of the space.

\subsubsection{4.2.3 Composition of
	Operators}\label{composition-of-operators}

A linear operator takes a vector in V and transforms it into another
vector in V. What if we took the output of a linear operator and
transformed it with another linear operator? That is, if M and N are
matrices, what if we took a vector \ul{v}, calculated N\ul{v} and then
calculated M(N\ul{v})?

The result is another linear operator -- if you do something linear and
then do something else linear to that, you've still done something
linear. So we should be able to form a single matrix representing the
total net effect, (MN)\ul{v}. Luckily this is quite easy.

Here is an example in 3D; see if you can spot what's happening:

\[\begin{pmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9
\end{pmatrix}\begin{pmatrix}
	10 & 20 & 30 \\
	40 & 50 & 60 \\
	70 & 80 & 90
\end{pmatrix} = \begin{pmatrix}
	1 \times 10 + 2 \times 40 + 3 \times 70 & 1 \times 20 + 2 \times 50 + 3 \times 80 & 1 \times 30 + 2 \times 60 + 3 \times 90 \\
	4 \times 10 + 5 \times 40 + 6 \times 70 & 4 \times 20 + 5 \times 50 + 6 \times 80 & 4 \times 30 + 5 \times 60 + 6 \times 90 \\
	7 \times 10 + 8 \times 40 + 9 \times 70 & 7 \times 20 + 8 \times 50 + 9 \times 80 & 7 \times 30 + 8 \times 60 + 9 \times 90
\end{pmatrix} = \begin{pmatrix}
	300 & 360 & 420 \\
	660 & 810 & 960 \\
	1020 & 1260 & 1500
\end{pmatrix}\]

The number in column x of row y of the answer is the result of applying
the covector that forms row y of the matrix on the left to the vector
that forms column x of the matrix on the right. Check this! This is
called ``matrix multiplication''. This is certainly a \emph{sort} of
vector multiplication, and it obeys most (but not all) of the expected
algebraic laws. But it relies heavily on the ``tensor product
structure'' of V⊗V*. Also note that MN and NM are usually different; for
example, a rotation followed by a reflection is not usually the same as
doing them the other way around.

If we revert to tensor notation and write the first matrix as
\ul{v}\textsubscript{1}⊗\ul{w}\textsubscript{1}* and the second as
\ul{v}\textsubscript{2}⊗\ul{w}\textsubscript{2}*, we get the compact
formula

\[{{\underline{v}}_{1}\bigotimes{{\underline{w}}_{1}}^{*}{\underline{v}}_{2}\bigotimes{{\underline{w}}_{2}}^{*} = {\underline{v}}_{1}\bigotimes{{(\underline{w}}_{1}}^{*}{\underline{v}}_{2})\bigotimes{{\underline{w}}_{2}}^{*}
}{= ({{\underline{w}}_{1}}^{*}{\underline{v}}_{2}){\underline{v}}_{1}\bigotimes{{\underline{w}}_{2}}^{*}}\]

However, as we've seen, doing an actual calculation with this notation
soon gets very cumbersome and messy. The square layout of a matrix helps
us to do calculations by hand with fewer mistakes -- assuming you ever
need to do that.

\subsubsection{4.2.4 Action of a Linear Operator on a
	Covector}\label{action-of-a-linear-operator-on-a-covector}

The same matrix can act on a covector in the dual space, assuming V* is
equipped with the induced basis from the basis we're using for V
(remember, which matrix representation you get for a particular linear
operator depends on the choice of basis).

For reference, I'll reproduce the calculation of a matrix acting on a
vector from earlier:

\[\begin{pmatrix}
	0 & 1 & - 5 \\
	0 & - 2 & - 10 \\
	0 & 3 & 15
\end{pmatrix}\begin{pmatrix}
	1 \\
	- 1 \\
	- 2
\end{pmatrix} = \begin{pmatrix}
	0 \times 1 + 1 \times ( - 1) + ( - 5) \times ( - 2) \\
	0 \times 1 + ( - 2) \times ( - 1) + ( - 10) \times ( - 2) \\
	0 \times 1 + 3 \times ( - 1) + 15 \times ( - 2)
\end{pmatrix} = \begin{pmatrix}
	9 \\
	22 \\
	- 33
\end{pmatrix}\]

We said we consider the matrix to be a vertical stack of covectors, each
of which acts on the vector. When we want to transform a covector
instead, we look at the matrix in the opposite way, as a horizontal
sequence of column vectors:

\begin{quote}
	\[\begin{pmatrix}
		1 & - 1 & - 2
	\end{pmatrix}\begin{pmatrix}
		0 & 1 & - 5 \\
		0 & - 2 & - 10 \\
		0 & 3 & 15
	\end{pmatrix} = \begin{pmatrix}
		1 \times 0 + ( - 1) \times 0 + ( - 2) \times 0 & 1 \times 1 + ( - 1) \times ( - 2) + ( - 2) \times 3 & 1 \times ( - 5) + ( - 1) \times ( - 10) + ( - 2) \times 15
	\end{pmatrix} = \begin{pmatrix}
		0 & - 3 & - 25
	\end{pmatrix}\]
\end{quote}

Notice that this is \emph{not} just the transpose of the previous
answer; M\ul{v} and \ul{v}\textsuperscript{T}M produce completely
different results. This is an important difference that we'll return to
next time. Again, we can do an example calculation in class and seeing
it unfold will probably clarify it a bit. We'll also discuss this
further in Session 9.

\chapter{Properties of Linear Operators}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 5.1} introduces the \emph{exterior product}, which
	we'll use to make the definitions in the following section. Exterior
	products may be important to you in later study but feel free to skim
	over the technical details if you prefer.
	\item
	\textbf{Section 5.2} shows how to use exterior products to define the
	determinant of a matrix, which in a sense is like the length of a
	vector and describes how much the linear operator it represents
	compresses or stretches space. We'll use the determinant to define the
	inverse of a linear operator, which ``undoes'' what the original one
	did.
	\item
	\textbf{Section 5.3} explores one of the most important uses of
	matrices: to represent a change of basis. This leads us to a deeper
	understanding of how vectors, covectors and matrices in V change when
	V is subjected to a linear operator.
\end{itemize}

There is quite a lot in this session's reading -- if you're short of
time, focus on the central ideas of determinant and inverse of a matrix
in Section 5.2. Remember that calculations are shown to help you improve
and test your understanding -- you don't have to develop fluency in
doing them yourself.

\subsection{5.1 The Exterior Product}\label{the-exterior-product}

\subsubsection{5.1.1 Oriented Areas}\label{oriented-areas}

One often hears students bemoaning the fact that they don't know what a
tensor \emph{is}, by which I think they mean they don't know how to
picture it. You can picture a vector as a little arrow and a covector as
a tilted copy of V. But these pictures are not perfect. As you already
know, a vector is just an element of a vector space; since V* is a
vector space, its elements are vectors too. They're only covectors when
viewed from the perspective of V. The vectors in V aren't ``naturally''
little arrows, that's just how we sometimes picture them. Since V⊗V is a
vector space, \emph{its} elements are vectors, too. But that doesn't
answer the question of what they're supposed to \emph{look like}.

We want \ul{v}⊗\ul{w} to be something like a product or multiplication
of \ul{v} and \ul{w}. If we picture them as little arrows, we can see
them as (oriented) lines that meet at the origin and a natural way to
interpret their product is to think of it as the square, rectangle or
(more generally) the parallelogram they define, as in these two
examples:

\includegraphics[width=2.17553in,height=1.14362in]{media/image16.png}\includegraphics[width=1.3354in,height=1.20772in]{media/image17.png}

If this is what we want, it seems that \ul{v}⊗\ul{w} and \ul{w}⊗\ul{v}
ought to represent the same thing. The problem is that these two tensors
have no relationship whatsoever to one another in V⊗V as we've defined
it. So we'll need to tweak V⊗V a bit to make this work. We could just
define \ul{v}⊗\ul{w} = \ul{w}⊗\ul{v}, and this is perfectly legitimate
(it leads to the space of ``symmetric tensors''). But we can squeeze a
bit more value out of this idea if we do it slightly differently.

Recall that picture of a vector as an arrow indicating a displacement
from one point to another. Now, we could have used just a line segment
instead. That would also have indicated the direction and length of the
separation of the two points. Why use an arrow? Because that expresses
more than the line segment would: it adds the idea of an
\textbf{orientation}. The displacement \emph{between} point A and point
B is the same as the displacement \emph{between} B to A. But the
displacement \emph{from} A \emph{to} B is opposite to the displacement
\emph{from} B \emph{to} A -- that requires two different vectors
pointing in exactly opposite directions, so that if \ul{v} points from A
to B, -\ul{v} points back from B to A.

It turns out that it's sometimes useful to consider these parallelogram
regions in the same way: with an orientation that can be positive or
negative. We can't picture this with an arrow but we can think of the
region having a ``clockwise'' or ``anticlockwise'' orientation:

\includegraphics[width=0.57994in,height=0.59028in]{media/image18.png}\includegraphics[width=0.59043in,height=0.59043in]{media/image19.png}\includegraphics[width=1.33472in,height=1.20694in]{media/image20.png}\includegraphics[width=1.33472in,height=1.20694in]{media/image20.png}

We consider these to be two different areas and indicate them with
opposite signs (positive or negative). You'll see how we use this later
in this session, but it's also of great importance in applications in
physics and engineering (where it underpins the ``cross product'') and
also in other areas of mathematics, including calculus.

It makes sense to think of the one on the right's orientation coming
from ``v turning towards w'' (i.e. it's clockwise) and the one on the
left coming from ``w turning towards v'' (anticlockwise). We'd like to
write the first one as \ul{v}⊗\ul{w} (``v turning towards w'') and the
second as \ul{w}⊗\ul{v} (``w turning towards v'').

You can think of the orientation as the thing that changes when you look
at one of these objects in a mirror -- an ``anticlockwise'' region
becomes ``clockwise'' and \emph{vice versa}. And we would like it to be
true that scalar-multiplying one of them by -1 turns it into the other
one. But that is \emph{not} true in V⊗V!

\subsubsection{\texorpdfstring{5.1.2 Defining
		\(V \land V\)}{5.1.2 Defining V \textbackslash land V}}\label{defining-v-land-v}

To fix this we will tweak V⊗V, but we had better use a different symbol
as the resulting space will be very different -- in fact it will have a
much smaller dimension. Tradition dictates that we use the ``wedge''
symbol ``∧'' for this (personally I think this is a bad symbol to use,
for rather obscure reasons, but there's no point fighting it). So we
rename the space V∧V and the tensors in it pick up the same notation:
\ul{v}∧\ul{w} and so on. And we declare that in this space (but not in
V⊗V) it will be true that

\[\underline{v} \land \underline{w} = - \underline{w} \land \underline{v}\]

People usually just pronounce ``∧'' as ``wedge'', since ``exterior
product'' is a bit of a mouthful. You could check that V∧V is still a
vector space and our new rule hasn't broken anything important, but that
would be a terrible chore -- it turns out to be fine.

However, the new rule \emph{does} end up making a lot of tensors that
are different in V⊗V equal to each other in V∧V. So much so that the
dimension is considerably reduced, as we will see in a moment. An
important example to bear in mind is that

\[\underline{v} \land \underline{v} = \underline{0}\]

since by the rule introduced above we have \ul{v}∧\ul{v} =
-\ul{v}∧\ul{v}, which doesn't make any sense unless \ul{v}∧\ul{v} is the
zero vector. This makes geometric sense too: if the two vectors are in
fact the same, the area they span is zero.

The exterior product is also sometimes referred to as the ``wedge
product'', ``alternating product'' or ``antisymmetric product''. We will
always use ``exterior product''.

\subsubsection{5.1.3 Exterior Powers}\label{exterior-powers}

Since V⊗V is just a vector space, we can tensor it with V again:
(V⊗V)⊗V. Luckily for us, you get exactly equivalent results from
V⊗(V⊗V), so it doesn't matter what order we do the two tensoring
operations in and we can just write V⊗V⊗V. By the way, this property is
known as \textbf{associativity}; in ordinary arithmetic with scalars,
addition is associative but subtraction isn't. Since something like
V⊗V⊗V⊗V⊗V is hard to read (and easy to get wrong) we often just write
⊗\textsuperscript{5}V instead -- that's five copies of V
tensor-producted together. We won't need any spaces like that on this
course, though.

We can do the same thing with V∧V, making new vector spaces V∧V∧V,
V∧V∧V∧V and so on, and again to save our sanity we write
∧\textsuperscript{n}V to mean n copies of V wedged together. This is
called the n\textsuperscript{th} \textbf{exterior power} of V and we
\emph{will} need spaces like that. But unlike with the tensor product
there's a limit to how high n can go.

\includegraphics[width=2.15625in,height=2.19514in,alt={Patterns in Pascal\&\#39;s Triangle}]{media/image21.gif}It
turns out that the dimension of ∧\textsuperscript{n}V can be found in
Pascal's Triangle: the row it's on is the dimension of V, and you choose
the n\textsuperscript{th} number on that row, but you must always count
from zero.

For example, suppose V is 4-dimensional and we want to know the
dimension of ∧\textsuperscript{3}V. Look at the fifth row down (since V
is 4D and we're counting from zero) and we get 1 4 6 4 1. Counting along
these up to 3 (starting from zero again) gives us 4. So
∧\textsuperscript{3}V is 4-dimensional. In contrast,
⊗\textsuperscript{3}V would be 4\textsuperscript{3} = 64-dimensional!

(In case you're curious, by definition ∧\textsuperscript{1}V = V and
∧\textsuperscript{0}V = \textbf{R}. But these won't concern us on this
course.)

Notice that the dimension rises from 1 and then falls back down to 1.
Beyond this, all the higher exterior powers vanish -- they become the
trivial vector space that only contains one vector, \ul{0} -- of course,
these are of no real interest. Also notice that ∧\textsuperscript{n}V is
always one-dimensional when n is the dimension of V.

\subsection{5.2 Determinants and
	Inverses}\label{determinants-and-inverses}

\subsubsection[5.2.1 The
Determinant]{\texorpdfstring{\protect\includegraphics[width=3.33194in,height=2.08264in,alt={The Parallelepiped}]{media/image22.gif}5.2.1
		The
		Determinant}{The Parallelepiped5.2.1 The Determinant}}\label{the-parallelepiped5.2.1-the-determinant}

Let V be a k-dimensional vector space with a basis. Then
∧\textsuperscript{k}V is a one-dimensional space with a basis vector
that's the wedge of all the basis vectors of V. This is called the
\textbf{volume element} of V. In two dimensions it's the parallelogram
defined by the basis vectors; in three dimensions it's a cube, cuboid or
a kind of skewed cuboid with the absurd name of ``parallelipiped''. As
usual, in higher dimensions we can't picture it but the general idea is
still useful.

It's useful because, by definition, the size of the volume element is 1
unit. When we apply a linear operator, the vectors get stretched,
squeezed, reflected or rotated (or some combination of those things):
the volume element will probably change size. We can use the size of the
volume element after the operator has been applied to measure the
overall effect of the operator. This quantity is called the
\textbf{determinant} of the operator and it is properly geometric in the
sense that it comes out the same regardless of which basis you work in.

Let's see how this works in practice. As usual we'll have the vector
space V, and let's make it three-dimensional this time with basis
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ul{e}\textsubscript{3}\}. The volume form in the induced basis of
∧\textsuperscript{3}V will simply be
\ul{e}\textsubscript{1}∧\ul{e}\textsubscript{2}∧\ul{e}\textsubscript{3}.
The idea is to use a matrix to transform the basis vectors and then to
figure out what happened to the volume form as a result. Let's use this
matrix (there's nothing special about it):

\[\begin{pmatrix}
	0 & 1 & 1 \\
	2 & 0 & 2 \\
	0 & 2 & 0
\end{pmatrix}\]

Applying it to the three basis vectors is easy (you can just ``read this
off the matrix'' but you should think about \emph{why}):

\[\begin{pmatrix}
	0 & 1 & 1 \\
	2 & 0 & 2 \\
	0 & 2 & 0
\end{pmatrix}\begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix} = \begin{pmatrix}
	0 \\
	2 \\
	0
\end{pmatrix}\]

\[\begin{pmatrix}
	0 & 1 & 1 \\
	2 & 0 & 2 \\
	0 & 2 & 0
\end{pmatrix}\begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix} = \begin{pmatrix}
	1 \\
	0 \\
	2
\end{pmatrix}\]

\[\begin{pmatrix}
	0 & 1 & 1 \\
	2 & 0 & 2 \\
	0 & 2 & 0
\end{pmatrix}\begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix} = \begin{pmatrix}
	1 \\
	2 \\
	0
\end{pmatrix}\]

So the new volume form is

\[\begin{pmatrix}
	0 \\
	2 \\
	0
\end{pmatrix} \land \begin{pmatrix}
	1 \\
	0 \\
	2
\end{pmatrix} \land \begin{pmatrix}
	1 \\
	2 \\
	0
\end{pmatrix}\]

Now for the long part -- we have to write this down in terms of the
volume element. That means breaking the vectors apart into their basis
elements and ``multiplying out brackets''. This produces rather a lot of
algebra that then cancels down to almost nothing because whenever the
same basis vector appears twice in a tensor the whole thing becomes
zero:

\[{\begin{pmatrix}
		0 \\
		2 \\
		0
	\end{pmatrix} \land \begin{pmatrix}
		1 \\
		0 \\
		2
	\end{pmatrix} \land \begin{pmatrix}
		1 \\
		2 \\
		0
	\end{pmatrix} = 2{\underline{e}}_{2} \land \left( {\underline{e}}_{1} + 2{\underline{e}}_{3} \right) \land \left( {\underline{e}}_{1} + 2{\underline{e}}_{2} \right)
}{= \left( 2{\underline{e}}_{2} \land {\underline{e}}_{1} + 2{\underline{e}}_{2} \land 2{\underline{e}}_{3} \right) \land \left( {\underline{e}}_{1} + 2{\underline{e}}_{2} \right)
}{= \left( - 2{\underline{e}}_{1} \land {\underline{e}}_{2} + 4{\underline{e}}_{2} \land {\underline{e}}_{3} \right) \land \left( {\underline{e}}_{1} + 2{\underline{e}}_{2} \right)
}{= - 2{\underline{e}}_{1} \land {\underline{e}}_{2} \land {\underline{e}}_{1} + 4{\underline{e}}_{2} \land {\underline{e}}_{3} \land {\underline{e}}_{1} - 2{\underline{e}}_{1} \land {\underline{e}}_{2} \land 2{\underline{e}}_{2} + 4{\underline{e}}_{2} \land {\underline{e}}_{3} \land 2{\underline{e}}_{2}
}{= 2{\underline{e}}_{1} \land {\underline{e}}_{1} \land {\underline{e}}_{2} + 4{\underline{e}}_{1} \land {\underline{e}}_{2} \land {\underline{e}}_{3} - 2{\underline{e}}_{1} \land {\underline{e}}_{2} \land 2{\underline{e}}_{2} - 4{\underline{e}}_{2} \land 2{\underline{e}}_{2} \land {\underline{e}}_{3}
}{= 4{\underline{e}}_{1} \land {\underline{e}}_{2} \land {\underline{e}}_{3}}\]

So the volume form has been scaled by a factor of 4; this factor is the
determinant of the matrix.

Other introductions to linear algebra spend a lot of time on methods for
calculating the determinant of 2D and 3D matrices. In the twenty-first
century nobody should be calculating determinants by hand, but those
methods are certainly a bit quicker for hand-calculation than the one in
this section. However, if you did ever have to do this you can use the
volume element as described above and \emph{it will work in any
	dimension you like}. If for some reason you have to calculate
determinants of 2D or 3D matrices by hand a lot, however, it may be
worth looking those methods up.

A determinant \textgreater{} 1 means the linear operator has the overall
effect of scaling volumes of space up (making them larger). A
determinant of 1 means the sizes of things stay the same -- for example,
rotations have a determinant of 1. A determinant between 0 and 1 means
volumes are scaled down, so things shrink. We will discuss matrices with
a determinant of zero next time.

What about less than zero? Well, if you drop the minus sign everything
is the same as in the previous paragraph, but the presence of the minus
sign tells you that the matrix has also reflected the space, as if in a
mirror. Things that were clockwise are now anticlockwise. Here is a 2D
example:

\[\begin{pmatrix}
	0 & 1 \\
	1 & 0
\end{pmatrix}\]

The volume form is \ul{e}\textsubscript{1}∧\ul{e}\textsubscript{2} but
the matrix has the effect of swapping the two basis vectors over:

\(\begin{pmatrix}
	0 & 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	1 \\
	0
\end{pmatrix} = \begin{pmatrix}
	0 \\
	1
\end{pmatrix}\) \(
\)\[\begin{pmatrix}
	0 & 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 \\
	1
\end{pmatrix} = \begin{pmatrix}
	1 \\
	0
\end{pmatrix}\]

So the volume form is mapped to
\ul{e}\textsubscript{2}∧\ul{e}\textsubscript{1}, which is equal to -
\ul{e}\textsubscript{1}∧\ul{e}\textsubscript{2}. Therefore the
determinant is -1; this matrix does not affect the sizes of things but
it does turn them into their mirror-images.

Because the determinant of a matrix M in some sense measures its size
(i.e. the size of the effect of the linear operator it represents) we
often use the same notation as we do for the size of a vector:

\[\left\| M \right\|\]

However, you'll see det(M) for the determinant in some books too.
Remember, the determinant of a linear operator can't possibly depend on
the matrix we use to write it down, since the matrix depends on our
choice of basis but the determinant doesn't. This is similar to noticing
that if I ask for a piece of wood that's twice as long as the one you
gave me, it doesn't matter whether we're using metres, feet, cubits or
some other unit of measurement; twice as big is twice as big,
independent of the units chosen.

\subsubsection{5.2.2 Inverses}\label{inverses}

A linear operator moves the vectors in V around to point at new
locations. What if we wanted to reverse it and put them back again? This
would require another linear operator that we call the \textbf{inverse}
of the original. If we write M for the matrix of the original we can
write its inverse as M\textsuperscript{-1}. It's very important in many
practical applications to be able to find the inverses of matrices.

Before we discuss how it's calculated, let's refine our definition a
bit. We said M\textsuperscript{-1} should undo M, which means
M\textsuperscript{-1} Mv = v. Notice we write M\textsuperscript{-1}M to
mean ``apply M to v, then apply M\textsuperscript{-1} to the result''
even though it seems backwards, because matrices act on vectors on the
left. But M\textsuperscript{-1}M is a linear operator in itself; this is
the special ``do-nothing'' linear operator, the \textbf{identity}
operator, written as a capital I. Recall that in any basis its matrix is
always full of zeros except the main top-left-to-bottom-right diagonal,
which is all ones. Some examples in dimensions 2, 3 and 4:

\[\begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix},\ \ \ \ \ \ \begin{pmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & 0 & 1
\end{pmatrix},\ \ \ \ \ \begin{pmatrix}
	\begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix} & \begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} & \begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix}
\end{pmatrix},\ \ \ \ldots\]

So what we want is a linear operator that, when composed with M, results
in the identity operator. Finding such an operator is a task for a
computer, although in up to three dimensions it's reasonable to do by
hand. If you really want to know how to do it by hand in as many
dimensions as you like, the first few sections of Chapter 3 of \emph{The
	Matrix Cookbook} have the cleanest method I've seen and it uses only
techniques we've already covered. If you do, get ready to calculate
\emph{lots} of determinants!

In 2D the formula is very easy:

\[M = \begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix}\]

\[M^{- 1} = \frac{1}{\left\| M \right\|}\begin{pmatrix}
	d & - b \\
	- c & a
\end{pmatrix}\]

Notice that we have to re-scale the matrix by one over the determinant
of M, which undoes the scaling of M (including any reflection). If you
want to know why the components inside the brackets got shuffled around
the way they did, I refer you to the general method in \emph{The Matrix
	Cookbook}; it poses no intellectual challenges but is tedious to do, and
this session's reading is quite long enough already.

You might worry about what happens when the determinant of M is zero, so
we can't divide by it. The answer is that in that case M has no inverse;
it represents a transformation that can't be undone and we say that M is
\textbf{singular}. How that might come about will be made clearer next
time.

\subsubsection{5.3.3 Systems of Linear Equations
	(*)}\label{systems-of-linear-equations}

Feel free to skip this section if time is short; it describes an
application of inverses that is very classical (i.e. it's in every
linear algebra textbook) but that doesn't really connect with anything
else on this course.

Here is a puzzle:

\begin{quote}
	I'm thinking of three numbers. They add up to 4. If you subtract double
	the third number from the second you get -1. If you add double the
	second number to the first, you also get -1. What are the three numbers?
\end{quote}

Perhaps see if you can solve this puzzle yourself before reading on. We
can express the information in equations. Let x, y and z be the unknown
numbers. Then the information we have is:

\[{x + y + z = 4
}{y - 2z = - 1
}{x + 2y = 1}\]

Notice that these equations are all linear; they only involve adding
scalar multiples of the variables. Because they describe the problem
then taken together, they're called a \textbf{system of equations}. It's
not obvious but we can express this as a matrix acting on a vector. The
vector will contain the values x, y and z and the result of the
transformation will be the numbers on the other side of the ``='' signs:

\[\begin{pmatrix}
	1 & 1 & 1 \\
	0 & 1 & - 2 \\
	1 & 2 & 0
\end{pmatrix}\begin{pmatrix}
	x \\
	y \\
	z
\end{pmatrix} = \begin{pmatrix}
	x + y + z \\
	y - 2z \\
	x + 2y
\end{pmatrix} = \begin{pmatrix}
	4 \\
	- 1 \\
	- 1
\end{pmatrix}\]

The idea is to find the inverse of this matrix and apply it to both
sides of the equation. This will solve the puzzle as if by magic. I used
a calculator to find the inverse:

\[\begin{pmatrix}
	1 & 1 & 1 \\
	0 & 1 & - 2 \\
	1 & 2 & 0
\end{pmatrix}^{- 1} = \begin{pmatrix}
	4 & 2 & - 3 \\
	- 2 & - 1 & 2 \\
	- 1 & - 1 & 1
\end{pmatrix}\]

Now we use this to transform both sides:

\[{\begin{pmatrix}
		4 & 2 & - 3 \\
		- 2 & - 1 & 2 \\
		- 1 & - 1 & 1
	\end{pmatrix}\begin{pmatrix}
		1 & 1 & 1 \\
		0 & 1 & - 2 \\
		1 & - 2 & 0
	\end{pmatrix}\begin{pmatrix}
		x \\
		y \\
		z
	\end{pmatrix} = \begin{pmatrix}
		4 & 2 & - 3 \\
		- 2 & - 1 & 2 \\
		- 1 & - 1 & 1
	\end{pmatrix}\begin{pmatrix}
		4 \\
		- 1 \\
		- 1
	\end{pmatrix}
}{\begin{pmatrix}
		x \\
		y \\
		z
	\end{pmatrix} = \begin{pmatrix}
		4 & 2 & - 3 \\
		- 2 & - 1 & 2 \\
		- 1 & - 1 & 1
	\end{pmatrix}\begin{pmatrix}
		4 \\
		- 1 \\
		- 1
	\end{pmatrix}
}{\begin{pmatrix}
		x \\
		y \\
		z
	\end{pmatrix} = \begin{pmatrix}
		17 \\
		- 9 \\
		- 4
\end{pmatrix}}\]

This is the solution: x = 17, y = -9 and z = -4. Try these numbers in
the equations above if you're doubtful!

This method is good for more than just puzzles: systems of linear
equations do come up in a wide variety of technical applications. Often
this method is part of a larger algorithm rather than something you'd
need to implement yourself, but it still doesn't hurt to know about it.
One good thing about it is that it's easy to describe precisely, so it's
easy to turn into code for a computer, in contrast to guess-and-improve
methods a human being might use to try to find a solution. It will also
tell you if the system of linear equations has no unique solution: this
happens when the matrix is singular and so can't be inverted; if so,
there may be no possible solution or there may be many possibilities, so
the system of equations is either self-contradictory or ambiguous.

\subsection{5.3 Change of Basis}\label{change-of-basis}

\subsubsection{5.3.1 Why Change Basis?}\label{why-change-basis}

A basis can be thought of as a ``point of view''. If you use a different
basis from me, that means that although we're standing in the same place
(the origin) we look at the world (the vector space) differently.

\includegraphics[width=1.19514in,height=0.92778in]{media/image23.png}For
example, suppose I think of \ul{e}\textsubscript{2} as ``up'' and
\ul{e}\textsubscript{1} as ``right'', and that's how I refer to things
in the space. I use units of measurement that make
\ul{e}\textsubscript{1} and \ul{e}\textsubscript{2} unit vectors. These
form my basis and I'll describe vectors and covectors in the space in
their terms.

\includegraphics[width=1.18935in,height=1.48669in]{media/image24.png}
Meanwhile, you're tilted at an angle and working with different units of
measurement. Your ``up'' looks to me like 2\ul{e}\textsubscript{1} +
\ul{e}\textsubscript{2}, and your ``right'' looks like
2\ul{e}\textsubscript{1} - 2\ul{e}\textsubscript{2}. But you don't see
my problem; to \emph{you}, my ``up'' and ``right'' look wrong, and when
I measure something as 1 unit long you think it looks much too short.

While differences in units and the like can often be solved by
standardization, the fact is there is no universal unit of length, and
no universal ``up'', ``right'' and so on, as has been understood since
the time of Galileo. We need to be able to change between frames of
reference precisely because there is no single one that is best or most
correct. And in fact, when solving a practical problem it often happens
that changing basis several times along the way can make calculations
easier.

What does it mean to change basis? It means that I translate my
representations of phenomena in the vector space into the ``language''
of the new basis. In this way we can communicate about, say, a vector or
a covector we're interested in without talking at cross-purposes. When I
send you some information, you simply change the basis so you can
understand it in your frame of reference. When you send me information,
I do the same but in reverse.

\subsubsection{5.3.2 A Matrix as a Change of
	Basis}\label{a-matrix-as-a-change-of-basis}

The translation device just described is, of course, a matrix.
Specifically I write down the matrix whose columns are the vectors in
your basis, seen from my point of view:

\includegraphics[width=3.36974in,height=1.7295in]{media/image25.png}

Note that any \emph{non-singular} matrix can be interpreted as a change
of basis -- and you can just read off which basis you'll get by treating
its columns as if they were vectors. A singular matrix doesn't transform
the original basis into a new one for the whole space, so it can't be
interpreted as a change of basis.

\subsubsection{5.3.3 Contravariance and
	Covariance}\label{contravariance-and-covariance}

But there's a complication. We want to use this matrix to translate
things we observe in the original basis (red) into their proper
representations in the new basis (green). The things we want to describe
in this way are vectors, covector and matrices, since these are the main
things we know how to describe using a basis.

With covectors the situation is straightforward. Recall that to
transform a covector with a linear operator we apply to matrix to the
covector on the right. Here's an example using the covector (1 2) in the
original basis:

\includegraphics[width=6.20003in,height=3.03336in]{media/image26.png}

The important thing is that the physical fact stays the same: for
example, that -5 value in the lower left stays in the same physical
location when the basis changes. That's important -- in fact it's the
whole point! When we change bases, we're not changing the facts, only
the way they're represented:

\includegraphics[width=6.07286in,height=3.27295in]{media/image27.png}

So I call this covector (1 2) and you call it (-2 4), but the actual
object stays the same -- a value of -5 in that physical spot. And the
matrix carries out this conversion perfectly for us.

This works differently with vectors, however. Remember that we saw at
the end of last session (4.2.4) that vectors and covectors transform
differently when you apply the same matrix to them? That's going to be a
slight complication here, although in the end it's a good thing. Here's
a vector we'd like to translate into the new basis:

\includegraphics[width=5.91307in,height=2.74629in]{media/image28.png}

If we just apply the matrix to the original representation of the vector
we get

\[\begin{pmatrix}
	2 & 2 \\
	- 2 & 1
\end{pmatrix}\begin{pmatrix}
	4 \\
	- 1
\end{pmatrix} = \begin{pmatrix}
	6 \\
	- 9
\end{pmatrix}\]

which is miles off; that vector is extremely long and points much more
downwards than the one we want. It turns out that we have to use the
\emph{inverse} matrix to transform vectors. Then it works:

\[\begin{pmatrix}
	1/6 & - 1/3 \\
	1/3 & 1/3
\end{pmatrix}\begin{pmatrix}
	4 \\
	- 1
\end{pmatrix} = \begin{pmatrix}
	1 \\
	1
\end{pmatrix}\]

Compare this with the diagram above and you'll see it's right; in the
new (green) basis, the blue vector really is equal to
\ul{f}\textsubscript{1} + \ul{f}\textsubscript{2}.

Because of this difference we say that covectors are \textbf{covariant},
because they vary \emph{with} the change of basis, but vectors are
\textbf{contravariant} because they vary \emph{against} it (using the
inverse matrix). Why is this good? Well it turns out that in real life
there are quantities that are covariant and others that are
contravariant. For example, the velocity of a car is a contravariant
vector. But suppose the car is driving up a slope whose gradient is
represented by a vector pointing in the direction that's ``directly
uphill'' and with a length proportional to the steepness of the slope.
It turns out that such a vector is covariant, so it must be represented
by a covector.

Because situations like this come up a lot in physics, you'll often hear
physicists talking about ``covariant vectors'' and ``contravariant
vectors'' and representing both with little arrows. As we've said
before, this is completely fine; how you picture things can vary
depending on what's useful in a particular context. (Be warned that
there's a bit more subtlety to the way physicists use these terms than
what I've just described.)

\subsubsection{5.3.4 Expressing a matrix in the new
	basis}\label{expressing-a-matrix-in-the-new-basis}

Suppose I have a rotation that I want to describe to you. I express it
as a matrix and send it to you. How do you transform it into your basis
so you can figure out what I'm talking about? A matrix is a tensor
product of a covector and a vector -- can it be covariant and
contravariant at the same time? It can indeed. A matrix is an example of
what's called a \textbf{mixed tensor} precisely because it has covariant
and contravariant elements.

For concreteness let's suppose I have this matrix:

\[\begin{pmatrix}
	0 & - 2 \\
	2 & 0
\end{pmatrix}\]

All we have to do is transform the covariant part by applying the change
of basis matrix on the left and transform the contravariant part by
applying the inverse of the same matrix on the left:

\[\begin{pmatrix}
	1/6 & - 1/3 \\
	1/3 & 1/3
\end{pmatrix}\begin{pmatrix}
	0 & - 2 \\
	2 & 0
\end{pmatrix}\begin{pmatrix}
	2 & 2 \\
	- 2 & 1
\end{pmatrix} = \begin{pmatrix}
	- 2/3 & - 5/3 \\
	8/3 & 2/3
\end{pmatrix}\]

In shorter notation, let M be the change of basis matrix and X the
matrix we want to translate. Then XM is X with its covariant component
translated, and M\textsuperscript{-1}XM finishes off the translation.

If X and Y are two matrices such that we can find a change of basis M
that makes M\textsuperscript{-1}XM = Y, we say X and Y are
\textbf{similar}. They can represent the same geometric transformation
in different bases. If so, X and Y must have the same determinant, but
having the same determinant isn't enough to guarantee they're similar.

As an aside, a matrix has another property called the \textbf{trace},
which is just the sum of the scalars on its top-left-to-bottom-right
diagonal. Surprisingly, if two matrices are similar they will have the
same trace but, again, having the same trace isn't enough to prove they
\emph{are} similar. We will investigate this fact further later in the
course.

\chapter{Vector Space Homomorphisms}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 6.1} introduces the \emph{direct sum}, which can be
	used to combine two vector spaces in a way that's quite different from
	the tensor product.
	\item
	\textbf{Section 6.2} shows how to extend the idea of a linear operator
	acting \emph{within} a vector space to cover linear maps that can
	carry us \emph{between} vector spaces. It also introduces the idea
	that, assuming they use the same scalars, two vector spaces of the
	same dimension are ``essentially the same'' in certain respects.
\end{itemize}

Everything in this week's reading will be important later but if time is
short focus on 6.1.1, 6.1.2, 6.2.1 and 6.2.3.

\subsection{6.1 The Direct Sum}\label{the-direct-sum}

\subsubsection{\texorpdfstring{6.1.1 Definition of
		\(V\bigoplus W\)}{6.1.1 Definition of V\textbackslash bigoplus W}}\label{definition-of-vbigoplus-w}

Like the tensor product, the direct sum allows us to combine two vector
spaces to get another one, which is usually different from either of the
ones we started with. This will be a ``sum'' in the sense that when we
``add together'' V and W, we'll get a new vector space whose dimension
is dim(V) + dim (W); remember that when we tensored V and W together the
result's dimension was dim(V)dim(W), which makes sense for a product.
The direct sum enables us to understand \emph{all} possible vector
spaces, as you'll see in the next section.

The idea is similar to the tensor product but (of course) a bit
different. The basis vectors of the new vector space V⊕W will be all the
symbols we can make in the following two ways:

\begin{quote}
	\ul{e}\textsubscript{i}⊕\ul{0}\textsubscript{w}, where
	\ul{e}\textsubscript{i} is a basis vector of V and
	\ul{0}\textsubscript{w} is the zero vector in W; and
	
	\ul{0}\textsubscript{v}⊕\ul{f}\textsubscript{j}, where
	\ul{f}\textsubscript{j} is a basis vector of W and
	\ul{0}\textsubscript{v} is the zero vector in V
\end{quote}

This brings together the two sets of basis vectors without losing track
of which is which; they aren't ``mixed up together'' because they have
quite different formats: one has the zero vector on the right, the other
on the left.

Now, to make this a vector space we need to know how to add and scalar
multiply these things, and this is easy. The rule for addition is
``componentwise'':

\[\underline{a}\bigoplus\underline{b} + \underline{c}\bigoplus\underline{d} = (\underline{a} + \underline{c})\bigoplus(\underline{b} + \underline{d})\]

The addition of \ul{a} + \ul{c} makes sense because \ul{a} and \ul{c}
are both vectors in V, so we just add them like we would in V; the same
idea goes for \ul{c} and \ul{d}, which are both vectors in W.

Scalar multiplication applies to both components, which also makes sense
because we already know how to scalar-multiply vectors in V and W:

\[x\left( \underline{a}\bigoplus\underline{b} \right) = (x\underline{a})\bigoplus(x\underline{b})\]

Note \ul{e}\textsubscript{i}⊕\ul{f}\textsubscript{j} that is not a basis
vector of this space, since it's equal to
\ul{e}\textsubscript{i}⊕\ul{0}\textsubscript{w} +
\ul{0}\textsubscript{v}⊕\ul{f}\textsubscript{j}; this is what makes the
dimension of V⊕W smaller than that of V⊗W.

In a moment we will see why we care about the direct sum. Before them,
here are some reassuring facts about it. First, the direct sum is, like
the tensor product, associative; that is, (U⊕V)⊕W=U⊕(V⊕W) and we can
dispense with the brackets and just write U⊕V⊕W as in ordinary addition
of numbers. Second, the tensor product \textbf{distributes over} the
direct sum, in the sense that U⊗(V⊕W) = (U⊗V)⊕(U⊗W) -- ignore the
circles and it's just like multiplying out brackets in school.

Finally, note that some authors use the term ``direct product'' for what
we are calling the ``direct sum''. There is a notion of ``direct
product'' in abstract algebra that is different from the tensor or
exterior products, but it is the same as the idea of a direct sum as
we've described it unless you want to combine an infinite number of
vector spaces together. As you might imagine, some subtleties arise when
that happens, but that's not something we need to worry about on this
course (in non-trivial cases, the result is always
infinite-dimensional).

\subsubsection{\texorpdfstring{6.1.2 The Vector Spaces
		\(R^{n}\)}{6.1.2 The Vector Spaces R\^{}\{n\}}}\label{the-vector-spaces-rn}

All the way back in Session 1 we said that \textbf{R} can be considered
a vector space; we think of (x) being a vector and x being a scalar, but
really that's just different notation for the same things. This is a
one-dimensional vector space and is really the simplest one we've seen,
so it makes sense to try to build up some ``bigger'' spaces from it.

So what is the space \textbf{R}⊕\textbf{R}? If we write (1) for the
single basis vector of \textbf{R}, we have two basis vectors in
\textbf{R}⊕\textbf{R}, which are (1)⊕(0) and (0)⊕(1). But since we only
care about the two scalars on either side of the ⊕ symbol, we may as
well just write these as

\[(1)\bigoplus(0) = \begin{pmatrix}
	1 \\
	0
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ \ \ \ (0)\bigoplus(1) = \begin{pmatrix}
	0 \\
	1
\end{pmatrix}\]

What's the difference, besides notation? You can check against the rules
in the previous section and you'll see that these behave the way
two-dimensional vectors always behave; we just came up with some
annoying notation for them. So the two-dimensional vector space, with
scalars \textbf{R}, is just \textbf{R}⊕\textbf{R}. This is usually
written as \textbf{R}\textsuperscript{2}, which I admit is annoying if
you're thinking of ⊕ as a sum rather than a product, but we've inherited
it from older times and it's not going anywhere. (If you think of ⊕ as a
product, and maybe use the symbol × to represent it like older books
often do, then this notation makes more sense but other things suddenly
look arbitrary; you can't win them all.)

But wait -- when I said \emph{the} two-dimensional vector space, didn't
I mean \emph{a} two-dimensional vector space? After all, we've seen at
least two different vector spaces with the same dimension: V and V*.
Isn't \textbf{R}\textsuperscript{2}* different from
\textbf{R}\textsuperscript{2}? The answer is\ldots{} yes and no. We'll
get into that shortly. But as you'll see there, \emph{in a sense}
there's only one vector space of each finite dimension for a given set
of scalars \textbf{R}, and that is the direct sum of n copies of
\textbf{R}, written \textbf{R}\textsuperscript{n}.

This means we can write things like
\textbf{R}\textsuperscript{3}⊕\textbf{R}\textsuperscript{5} \textbf{=
	R}\textsuperscript{8}. But what is that equals sign \emph{really} doing?
And how could it be that V* = V, given the important differences between
them? We'll see shortly.

\subsubsection{6.1.3 Subspaces and Direct
	Complements}\label{subspaces-and-direct-complements}

We have thought of X = V⊕W as a new vector space we can make by
combining two that we already have, like making a box of 7 eggs by
combining 3 from one box and 4 from another: 3 + 4 = 7. But there's
another way to look at it. We could start with a vector space X and
\textbf{decompose} or \textbf{split} it into two parts that sum together
to recreate X again. This is like starting with the box of 7 eggs and
dividing it up into a box of 3 and a box of 4; conceptually, 7 = 3 + 4
instead of 3 + 4 = 7. As you know, arithmetically these equations mean
the same thing, so this is really just a change in viewpoint.

So let's start with a vector space X and ask whether and how we can
split it into a direct sum representation V⊕W. Of course, V and W must
be vector spaces, and the vectors in them must correspond in some sense
to the vectors in X even though they might be labelled differently. If a
vector space lives inside a larger vector space like this we call it a
\textbf{subspace}. So V and W must be subspaces of X.

Let's look at an example, X = \textbf{R}\textsuperscript{3}. Its vectors
are of the form

\[\begin{pmatrix}
	a \\
	b \\
	c
\end{pmatrix}\]

Based on the previous section we expect that we can split
\textbf{R}\textsuperscript{3} as
\textbf{R}\textsuperscript{2}⊕\textbf{R}. Now, of course the vectors in
those spaces look like

\[\begin{pmatrix}
	a \\
	b
\end{pmatrix}\ \ \ \ \ \ \ \ and\ \ \ \ \ \ (c)\]

But there's nothing to stop us writing them as

\[\begin{pmatrix}
	a \\
	b \\
	0
\end{pmatrix}\ \ \ \ \ \ \ \ and\ \ \ \ \ \ \begin{pmatrix}
	0 \\
	0 \\
	c
\end{pmatrix}\]

instead, and now these look like vectors in X. The vectors that match
each of these patterns form a vector subspace of X, and it's easy enough
to show that the direct sum of these two subspaces will be
\textbf{R}\textsuperscript{3}.

It's sometimes said that \textbf{R}\textsuperscript{2} is the
\textbf{direct complement} of \textbf{R} in
\textbf{R}\textsuperscript{3}, and \emph{vice versa}. It's the part you
need to direct-sum to \textbf{R} to make \textbf{R}\textsuperscript{3}.
Splitting spaces using direct sums is a powerful technique in advanced
algebra but finding subspaces and their complements has much wider
applications. For example, if you can limit a problem to a subspace, it
may become much easier to solve.

\subsection{6.2 Homomorphisms of Vector
	Spaces}\label{homomorphisms-of-vector-spaces}

\subsubsection{\texorpdfstring{6.2.1 Definition of
		\(W\bigotimes V^{*}\)}{6.2.1 Definition of W\textbackslash bigotimes V\^{}\{*\}}}\label{definition-of-wbigotimes-v}

We've spent a lot of energy on linear operators, which are
transformations that act within a vector space V, assigning each of its
vectors to another vector also in V. But this is just a small corner of
a much larger universe. Fortunately, we can explore this universe using
the tools we already have.

If V and W are different vector spaces, we can define linear maps
between them just as easily. If I want vectors in W to be transformed
into vectors in V, I can first apply a covector from W to the vector to
get a scalar, then multiply that scalar by a vector in V. So instead of
an element of V⊗V* applying to a vector in V and giving us another
vector in V, we'll have an element of V⊗W* applying to an element of W
and giving us an element of V. That's it!

All maps defined by V⊗W* are linear maps and respect the structures of
the two vector spaces. We call these \textbf{homomorphisms}. Linear
operators are homomorphisms too, just special cases where W and V are
the same vector space. The term ``homomorphism'' is almost universal in
mathematics and reaches far beyond linear algebra; it was a great leap
of imagination in the 19\textsuperscript{th} century to realise that a
field of maths is defined by its homomorphisms far more than by its
objects (in our case, vector spaces). Homomorphisms in linear algebra
are also sometimes called ``linear maps'' or ``linear transformations''.

If the dimensions of V and W are different, the matrix that represents
such a tensor will be rectangular rather than square. Let's write out an
example just for concreteness.

Let V = \textbf{R}\textsuperscript{3} with basis
\{\ul{v}\textsubscript{1}, \ul{v}\textsubscript{2},
\ul{v}\textsubscript{3}\} and W=\textbf{R}\textsuperscript{2} with basis
\{\ul{w}\textsubscript{1}, \ul{w}\textsubscript{2}\}. W* has the induced
basis \{\ul{w}\textsubscript{1}*, \ul{w}\textsubscript{2}*\}, so the
basis vectors of V⊗V* will be
\{\ul{v}\textsubscript{1}⊗\ul{w}\textsubscript{1}*,
\ul{v}\textsubscript{2}⊗\ul{w}\textsubscript{1}*,
\ul{v}\textsubscript{3}⊗\ul{w}\textsubscript{1}*,
\ul{v}\textsubscript{1}⊗\ul{w}\textsubscript{2}*,
\ul{v}\textsubscript{2}⊗\ul{w}\textsubscript{2}*,
\ul{v}\textsubscript{3}⊗\ul{w}\textsubscript{2}*\}. Any linear
combination of these is a tensor in V⊗V*. We'll choose one at random:

\begin{quote}
	2\ul{v}\textsubscript{1}⊗\ul{w}\textsubscript{1}* +
	\ul{v}\textsubscript{2}⊗\ul{w}\textsubscript{1}* -
	2\ul{v}\textsubscript{1}⊗\ul{w}\textsubscript{2}* -
	3\ul{v}\textsubscript{3}⊗\ul{w}\textsubscript{2}*
\end{quote}

We can write this as a matrix as we did in 4.2.1:

\[\begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\]

Notice that it has two columns because W* is two-dimensional, but three
rows because V is three-dimensional. You can remember which way round it
goes because the columns of the matrix are the vectors you'll get when
you apply it to the basis vectors of the space you're starting from (in
this case, W). So the dimension of the columns of the matrix have to
match the dimension of the space you want to end up in.

Let's apply this to a vector in W to check it works -- again, just
picking a vector at random:

\[\begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\begin{pmatrix}
	5 \\
	10
\end{pmatrix} = \begin{pmatrix}
	2 \times 5 + ( - 2) \times 10 \\
	1 \times 5 + 0 \times 10 \\
	0 \times 5 + ( - 3) \times 10
\end{pmatrix} = \begin{pmatrix}
	- 10 \\
	5 \\
	- 30
\end{pmatrix}\]

The vector 5\ul{w}\textsubscript{1} + 10\ul{w}\textsubscript{2} in W was
transformed into the vector -10\ul{v}\textsubscript{1} +
5\ul{v}\textsubscript{2} - 30\ul{v}\textsubscript{3} in V.

\subsubsection{6.2.2 Surjective and Injective
	Maps}\label{surjective-and-injective-maps}

Here is a bit of important jargon about maps. It applies more widely
than just linear algebra; we could have covered it in Session 2 but
there was already a lot there and we only really need it now.

Let V and W be any vector spaces and suppose we have some map W🡪V,
perhaps like the one described in the previous section. Let's call this
map f and write f(\ul{w}) for the vector in V that the vector \ul{w}
gets transformed into (we introduced this notation in 2.1.1.2 but
haven't had much need for it until now).

This section uses the terminology of 5.3.3 on systems of equations; if
you skipped that you might want to read it now, or you can take some of
the arguments here ``on faith'' for the moment.

\paragraph{6.2.2.1 Surjective Maps}\label{surjective-maps}

We say f is a \textbf{surjective} map if every vector in V gets
something mapped to it. To go back to the restaurant analogy, this is a
table of people who, between them, order everything on the menu. The
homomorphism we saw earlier is \emph{not} surjective:

\[\begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\]

To prove it, I'll let you choose any vector in V. Since I don't know
what it is, I'll represent it with letters:

\[\begin{pmatrix}
	a \\
	b \\
	c
\end{pmatrix}\]

The question is, can I find a vector in w that maps to it, no matter
what the values of a, b and c? That amounts to finding values of x and y
such that

\[\begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	a \\
	b \\
	c
\end{pmatrix}\]

Turning this into a system of linear equations, we have

\[{2x - 2y = a
}{x = b
}{- 3y = c}\]

So we see that I can only solve this if

\[2b + \frac{2}{3}c = a\]

This is \emph{not} guaranteed to be true. For example, if you choose b =
1, c = 3 and a = 2 (i.e. the vector \ul{v}\textsubscript{1} +
3\ul{v}\textsubscript{2} + 2\ul{v}\textsubscript{3}), this equation is
false so no vector I choose from W will ever map to that vector in V.

And this is not at all surprising if we think about it geometrically.
This homormorphism takes a 2D space (think of it as a flat, rigid
surface) and transports it into a 3D space. It might get stretched,
rotated and reflected but it won't be able to fill up the whole 3D
space. In fact we can guess (and we'd be right) that no map W🡪V can be
surjective if the dimension of V is greater than the dimension of W.

\paragraph{6.2.2.2 Injective Maps}\label{injective-maps}

We say f:W🡪V is an \textbf{injective} map if every vector in the
starting-space maps to a \emph{unique} vector. That is, suppose \ul{p}
and \ul{q} are vectors in W; then if f(\ul{p}) = f(\ul{q}) and f is
injective, then it must be that \ul{p} = \ul{q}. In the restaurant, this
is equivalent to everybody ordering a \emph{different} dish, so that
when the waiter beings each plate of food it's clear and unambiguous who
it's for.

Let's check whether our example from the previous section is injective.
If it's not, there are two \emph{different} vectors that map to the same
vector

\[\begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\begin{pmatrix}
	a \\
	b
\end{pmatrix} = \begin{pmatrix}
	2 & - 2 \\
	1 & 0 \\
	0 & - 3
\end{pmatrix}\begin{pmatrix}
	c \\
	d
\end{pmatrix}\]

Again, turning this into a system of equations we have

\[{2a - 2b = 2c - 2d
}{a = c
}{- 3b = d}\]

By substituting into the first equation we get

\[2c + \frac{2}{3}d = 2c - 2d\]

The 2c on both sides cancels out, so we have 2/3d = -2d. This is only
possible if d = 0, which implies that b = 0 as well. Then it seems that
a and c can take any value we like, as long as it's the same for both,
but there's no way to follow those rules and get two \emph{different}
vectors (try for yourself!). So this map is indeed injective, because
you can't have two different vectors mapped to the same one.

\paragraph{6.2.2.3 Kernels and Subspaces}\label{kernels-and-subspaces}

Every homomorphism of vector spaces must map the zero vector in the
first space to the zero vector in the second. If the homomorphism is
injective, that's the only vector that gets ``sent to zero''. But in
general there could be many vectors that are ``sent to zero''. The set
of such vectors is called the \textbf{kernel} of the homomorphism, and
it turns out that this is always a subspace of the domain, i.e. the
kernel is always a vector space in its own right. Let's see an example.

Here's a homomorphism
f:\textbf{R}\textsuperscript{5}🡪\textbf{R}\textsuperscript{2}:

\[\begin{pmatrix}
	\begin{matrix}
		1 \\
		0
	\end{matrix} & \begin{matrix}
		2 \\
		- 1
	\end{matrix} & \begin{matrix}
		\begin{matrix}
			1 \\
			1
		\end{matrix} & \begin{matrix}
			\begin{matrix}
				0 \\
				- 2
			\end{matrix} & \begin{matrix}
				2 \\
				0
			\end{matrix}
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

To find the kernel of this map we need to solve the equation

\[\begin{pmatrix}
	\begin{matrix}
		1 \\
		0
	\end{matrix} & \begin{matrix}
		2 \\
		- 1
	\end{matrix} & \begin{matrix}
		\begin{matrix}
			1 \\
			1
		\end{matrix} & \begin{matrix}
			\begin{matrix}
				0 \\
				- 2
			\end{matrix} & \begin{matrix}
				2 \\
				0
			\end{matrix}
		\end{matrix}
	\end{matrix}
\end{pmatrix}\begin{pmatrix}
	v \\
	w \\
	\begin{matrix}
		x \\
		y \\
		z
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	0 \\
	0
\end{pmatrix}\]

which we can write as a system of equations

\[{v + 2w + x + 2z = 0
}{- w + x - 2y = 0}\]

We can rewrite the second equation as

\[x = w + 2y\]

and substitute it into the first:

\[v + 3w + 2y + 2z = 0\]

but now we're stuck. Any collection of values of v, w, y and z that
satisfies this equation will give us a vector that maps to the zero
vector under the homomorphism. In fact we can choose any value at all
for three of these scalars, and that will force a choice for the other
one on us to make the equation above true. Furthermore, it will force a
value of x on us too. For example, if we write

\[v = - 3w - 2y - 2z\]

then we end up with the solution

\[\begin{pmatrix}
	- 3w - 2y - 2z \\
	w \\
	\begin{matrix}
		w + 2y \\
		y \\
		z
	\end{matrix}
\end{pmatrix}\]

Now, if we consider this a label for the vectors that are sent to zero
by the homomorphism, it's not very efficient since the first and third
values are fixed by the other three. We may as well just write it as

\[\begin{pmatrix}
	w \\
	y \\
	z
\end{pmatrix}\]

which is what the vectors in \textbf{R}\textsuperscript{3} look like.
Hence, in a sense we're about to make more rigorous, the kernel of f
``is'' just \textbf{R}\textsuperscript{3}, if we're allowed to change
the way we label our vectors. And this might look suspicious, since
f:\textbf{R}\textsuperscript{5}🡪\textbf{R}\textsuperscript{2} and
\textbf{R}\textsuperscript{5} =
\textbf{R}\textsuperscript{2}⊕\textbf{R}\textsuperscript{3}. In fact
this is an example of a general pattern that we'll describe in the final
section.

(You might recall that we already defined kernels in the context of
covectors. We can now think of a covector as a homomorphism from V to
the one dimensional space \textbf{R}, f:V🡪\textbf{R}; the earlier
definition agrees with this one.)

\subsubsection{6.2.3 Isomorphisms}\label{isomorphisms}

We have to refer to the vectors in our vector spaces somehow; I've been
calling the way we refer to them their ``labels''. The vectors don't
care what we call them, any more than a tree cares whether it's called
``tree'', ``arbre'', ``Baum'' or whatever.

An isomorphism of vector spaces is a homomorphism f:V🡪W that acts as a
``translation'' of the labels on the vectors in V into those on the
vectors in W. In other words, finding an isomorphism between V and W
amounts to discovering that they're essentially the same vector space
that we're just thinking of or talking about in different ways.

The following fact often makes it easy to spot them: any homomorphism
that is both injective and surjective is automatically an isomorphism.

Let's go straight to an important example in the theory -- I claim that
the vector space V is isomorphic to its dual, V*. This is very
straightforward to show because the ``transposing'' map f:V🡪V* defined
by the following rule (letting V be three-dimensional for the sake of
writing it down):

\[f\begin{pmatrix}
	a \\
	b \\
	c
\end{pmatrix} = \begin{pmatrix}
	a & b & c
\end{pmatrix}\]

is obviously both injective and surjective. So while we think of V and
V* as different, they aren't different \emph{as vector spaces}; they're
different in the way we interpret and use them. It will help to think of
the statement ``V is isomorphic to W'' as being a kind of equality, and
we often write it as

\[V \cong W\]

Of course, similarly to normal equality of numbers, if V is isomorphic
to W then W must be isomorphic to V; it's better to say they are
isomorphic vector spaces.

Here are some basic facts about isomorphisms that are well worth knowing
-- you should think about why each one would be true as some are a bit
surprising. In these statements f:V🡪W is always an isomorphism:

\begin{itemize}
	\item
	If two vector spaces have the same dimension, they are isomorphic. (As
	long as they use the same set of scalars and the dimension is finite.)
	\item
	f\textsuperscript{-1}:W🡪V is also an isomorphism
	\item
	If \{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2}, \ldots,
	\ul{e}\textsubscript{n}\} is a basis of V then
	\{f(\ul{e}\textsubscript{1}), f(\ul{e}\textsubscript{2}), \ldots,
	f(\ul{e}\textsubscript{n})\} is a basis of W.
	\item
	If g:W🡪X is also an isomorphism, so is the composite gf:V🡪X (i.e. the
	map that does f and then does g to the result).
\end{itemize}

\part{The Second Six Weeks}

The second half of this course is a bit less intense in terms of theory
but it involves a fair bit of jargon, some of which is frankly not very
descriptive. This is designed to be a glossary you can use to quickly
remind yourself of what a term means if you've forgotten. I've only
included the most important jargon that gets used multiple times (not
just in the section where it's defined). In particular, the final three
sessions are very focused on specific applications and do not introduce
new terminology that's important later.

The \textbf{image} of a homomorphism f is the set of vectors it maps to.
This is always a subspace of the codomain and its dimension is the
\textbf{rank} of f. The set of vectors that f maps to the zero vector is
called the \textbf{kernel} of f. This is a subspace of the domain and
its dimension if the \textbf{nullity} of f.

The \textbf{complex numbers} are a system of scalars that can be written
a + bi, were a and b are real numbers and i is a special number defined
by i\textsuperscript{2} = -1.

An \textbf{eigenvector} of a linear operator A is a vector \ul{v} such
that A\ul{v} is just a scalar multiple of \ul{v}. That scalar is called
the corresponding \textbf{eigenvalue}. The set of all eigenvalues of A
is called its \textbf{spectrum}. If one eigenvalue is larger than any of
the others (in absolute terms, i.e. ignoring any minus signs) it is
called the \textbf{dominant eigenvalue}.

\textbf{Convolution} is the act of applying a linear operator multiple
times. The net effect of applying the operator A n times is
A\textsuperscript{n}.

The \textbf{adjoint} of a linear operator A, written A*, represents the
same linear operator as A acting on the dual space. For real scalars,
the matrix representing A* is just the \textbf{transpose} of the matrix
for A, meaning you reflect it along its main (top-left-to-bottom-right)
diagonal.

A linear operator is called \textbf{Hermitian} if it is its own adjoint,
i.e. A = A*. Over real scalars, the Hermitian matrices are the symmetric
ones. It is called \textbf{unitary} if its inverse is also its adjont,
i.e. A* = A\textsuperscript{-1}.

The \textbf{eigendecomposition} of a linear operator expresses it in a
basis made of its eigenvectors, so that it acts as a scaling along the
basis vectors.

\chapter{The Rank-Nullity Theorem}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 7.1} introduces some ideas and associated jargon about
	homomorphisms. It's important to internalize the picture at the start
	of 7.1.1. The idea of a quotient space is an important idea in general
	algebra but it won't be needed again on this course, so that section
	is starred.
	\item
	\textbf{Section 7.2} shows some number systems that can be built from
	vector spaces. The whole section is optional, although the very basics
	of complex numbers will be used next time and in a few other places.
	The purpose of the section is to provide detailed, applicable examples
	that should help you get a grasp of the (rather abstract) ideas we met
	last week and in 7.1.
\end{itemize}

If time is short, focus on 7.1.1 and 7.1.3. You might need to review
some topics from last week's reading, especially the material in 6.2. If
you feel you're struggling with understanding the ideas and have a
little extra time, try to carefully follow the argument that runs all
the way through 7.2.

\subsection{7.1 The Rank-Nullity
	Theorem}\label{the-rank-nullity-theorem-1}

\subsubsection{7.1.1 Image and Rank of a
	Homomorphism}\label{image-and-rank-of-a-homomorphism}

The \textbf{image} of a homomorphism f:V🡪W is the set of all vectors in
W that get something mapped to them; i.e. the list of all the dishes
that at least one person ordered at the restaurant. The image, often
written as f(V), is equal to W if f is surjective -- that's really the
definition of ``surjective''.

Here (borrowed from Wikipedia) is a common way to picture the kernel and
image of a homomorphism -- sometimes called a ``fried egg diagram'':

\includegraphics[width=3.60347in,height=2.42569in,alt={Diagram, venn diagram Description automatically generated}]{media/image29.png}

The image of a homomorphism is always either the codomain itself or a
subspace of it (not just some random collection of vectors). What's
more, if we've chosen a basis for the domain then the images of those
basis vectors span the image but might not be a basis for it. The reason
they might not be a basis is that there might be some redundant vectors
in there, or the zero vector.

For example, consider the map
f:\textbf{R}\textsuperscript{3}🡪\textbf{R}\textsuperscript{3} given by
the matrix

\[\begin{pmatrix}
	1 & - 1 & 2 \\
	0 & 2 & 0 \\
	2 & 1 & 4
\end{pmatrix}\]

Let's see what this does to the basis vectors -- of course we can just
read this off the columns of the matrix representation but to be clear:

\[{\begin{pmatrix}
		1 & - 1 & 2 \\
		0 & 2 & 0 \\
		2 & 1 & 4
	\end{pmatrix}\begin{pmatrix}
		1 \\
		0 \\
		0
	\end{pmatrix} = \begin{pmatrix}
		1 \\
		0 \\
		2
	\end{pmatrix}
}{\begin{pmatrix}
		1 & - 1 & 2 \\
		0 & 2 & 0 \\
		2 & 1 & 4
	\end{pmatrix}\begin{pmatrix}
		0 \\
		1 \\
		0
	\end{pmatrix} = \begin{pmatrix}
		- 1 \\
		2 \\
		1
	\end{pmatrix}
}{\begin{pmatrix}
		1 & - 1 & 2 \\
		0 & 2 & 0 \\
		2 & 1 & 4
	\end{pmatrix}\begin{pmatrix}
		0 \\
		0 \\
		1
	\end{pmatrix} = \begin{pmatrix}
		2 \\
		0 \\
		4
\end{pmatrix}}\]

The problem is that the third of these vectors is a scalar multiple of
the second. So although we have three different vectors, they don't span
a three-dimensional space.

The dimension of the image can be calculated as the same as the -- (take
a deep breath) -- \emph{dimension} of the \emph{span} of the
\emph{images} of the \emph{domain's basis vectors}. This is called the
\textbf{rank} of the homomorphism. We will take a moment to see how to
compute this systematically (feel free to skip to the next section if
time is short or you find this dull).

This idea is to notice that for any two vectors \ul{v} and \ul{w},
\ul{v}∧\ul{w}=\ul{0} implies that \ul{v} and \ul{w} lie on the same
line, i.e. they're scalar multiples of each other. Let's calculate this
explicitly for all the pairs of our image vectors. First we'll write
them in the basis:

\[{\begin{pmatrix}
		1 \\
		0 \\
		2
	\end{pmatrix} = e_{1} + 2e_{3}
}{\begin{pmatrix}
		- 1 \\
		2 \\
		1
	\end{pmatrix} = {- e}_{1} + 2e_{2} + e_{3}
}{\begin{pmatrix}
		2 \\
		0 \\
		4
	\end{pmatrix} = 2e_{1} + 4e_{3}}\]

Now we can calculate the three possible exterior products (you might
want to review section 5.1.2 if you want to follow the detail of this
calculation):

\[\left( e_{1} + 2e_{3} \right) \land \left( {- e}_{1} + 2e_{2} + e_{3} \right) = {2e}_{1} \land e_{2} + {3e}_{1} \land e_{3} - 4e_{2} \land e_{3}\]

\[\left( 2e_{1} + 4e_{3} \right) \land \left( {- e}_{1} + 2e_{2} + e_{3} \right) = {4e}_{1} \land e_{2} + {6e}_{1} \land e_{3} - 6e_{2} \land e_{3}\]

\[\left( e_{1} + 2e_{3} \right) \land \left( 2e_{1} + 4e_{3} \right) = {4e}_{1} \land e_{3} + 4e_{3} \land e_{1} = 0\]

This calculation shows that we can take either the first and second or
the second and third vectors and they will be linearly independent sets,
so we can use either of these pairs as a basis for a vector space. The
last one comes out as zero, which expresses the fact that these two are
scalar multiples of each other -- they can't coexist in the same basis.

In higher dimensions it's usually necessary to calculate more exterior
products -- see Winitzki section 2.3.5 for a more detailed algorithm
that uses the same basic idea. But to summarize, the rank of a
homomorphism is the rank of the image of the set of basis vectors, which
is also the dimension of the image as a subspace of the codomain.

\subsubsection{7.1.2 The Rank-Nullity
	Theorem}\label{the-rank-nullity-theorem-2}

The Rank-Nullity Theorem makes a very simple statement. Its name comes
from the somewhat old-fashioned term ``nullity'' which in modern jargon
is the dimension of its kernel. In our terminology it says, for a
homomorphism f: The rank of f, plus the dimension of its kernel, equals
the dimension of the domain:

\[dim(\ f(V)\ ) + \dim\left( \ \ker(f)\  \right) = dim(\ V\ )\]

In the previous section's example we had a homomorphism
f:\textbf{R}\textsuperscript{3}🡪\textbf{R}\textsuperscript{3} whose
image was in fact just \textbf{R}\textsuperscript{2}. The theorem tells
us that this means the kernel of f must be \textbf{R}, because dim(f(V))
= 2 and dim(V) = 3, so dim(ker(v)) must be 1. If the kernel of f has
dimension 0, it must contain only the zero vector since every
homomorphism must always map the zero vector in the domain to the zero
vector in the codomain: we call this a \textbf{trivial kernel}.

Suppose that f and g are homomorphisms of ranks R\textsubscript{f} and
R\textsubscript{g} respectively. Then the rank of f ▫ g or g ▫ f can't
be larger than the smaller of the two, i.e.

\[R_{f \circ g} \leq R_{f}\ \ \ \ \ \ and\ \ \ \ \ \ R_{f \circ g} \leq R_{g}\]

\[R_{g \circ f} \leq R_{f}\ \ \ \ \ \ and\ \ \ \ \ \ R_{g \circ f} \leq R_{g}\]

Furthermore, if N\textsubscript{f} and N\textsubscript{g} are the
nullities of f and g, these are at least as large as the nullity of the
composition, i.e.

\[N_{f \circ g} \geq N_{f}\ \ \ \ \ \ and\ \ \ \ \ \ N_{f \circ g} \geq N_{g}\]

\[N_{g \circ f} \geq N_{f}\ \ \ \ \ \ and\ \ \ \ \ \ N_{g \circ f} \geq N_{g}\]

These small facts are often useful in understanding the overall effect
of several homomorphisms applied in sequence.

\subsubsection{7.1.3 Quotient Spaces (*)}\label{quotient-spaces}

Let V be a vector space and U be any subspace of V. Then the set V/U,
called the \textbf{quotient} of V by U, is V but with certain vectors
held to be equal according to the following rule:

\[\underline{v} = \underline{w}\ \ \ \ \ \ \ \ if\ \ \ \ \ \ \underline{v} = \underline{w} + \underline{u}\ \ \ \ for\ some\ \ \underline{u}\ \ in\ U\]

In other words, if the displacement from the point pointed at by \ul{v}
to the point pointed at by \ul{w} is a vector in U, we declare \ul{v}
and \ul{w} to be ``essentially the same''. All the vectors that are
essentially the same, taken together, are called an \textbf{equivalence
	class}. We usually use some symbol or object to represent each
equivalence class; here we'll use the somewhat standard {[}\ul{v}{]} to
mean the equivalence class containing the vector \ul{v}. The subspace U
will be one equivalence class and will include the zero vectors, so we
can write it as {[}\ul{0}{]}. Every other equivalence class can be
written as {[}\ul{0} + \ul{v}{]} for a vector v that isn't in U, but
notice that for many choices of \ul{v} and \ul{w} we will have {[}\ul{0}
+ \ul{v}{]} = {[}\ul{0} + \ul{w}{]}, i.e. both will be in the same
equivalence class.

\includegraphics[width=3.07569in,height=2.42569in,alt={A picture containing chart Description automatically generated}]{media/image30.png}For
example, suppose V = \textbf{R}\textsuperscript{3} with the basis
\{\ul{e}\textsubscript{1}, \ul{e}\textsubscript{2},
\ul{e}\textsubscript{3}\}. Let U be the vector space spanned by
\{\ul{e}\textsubscript{2} + \ul{e}\textsubscript{3},
\ul{e}\textsubscript{2} - \ul{e}\textsubscript{1}\}. This is of course a
two-dimensional space, which we can picture as a tilted plane through
the origin.

If we form V/U, this tilted plane becomes one of the vectors in V/U,
{[}\ul{0}{]}, and the other vectors are the equivalence classes that
consist of planes parallel to it. Vector addition works as {[}\ul{v}{]}
+ {[}\ul{w}{]} = {[}\ul{v} + \ul{w}{]} and scalar multiplication is
a{[}\ul{v}{]} = {[}a\ul{v}{]}. (It may be worth checking that you really
believe these definitions give us a vector space and aren't just
nonsense.)

It is a fact that V/U is the direct complement of U in V, i.e.

\[V = U\bigoplus(V/U)\]

Sometimes we say that U \textbf{splits} V. Splitting a vector space into
a direct sum of subspaces like this can be very useful in reducing a
practical problem to a smaller domain. From this fact it follows
immediately that:

\[\dim(V/U) = \dim(V) - dim(U)\]

Put another way, dim(V/U) is the codimension of U in V. In our example
dim(V/U) = 3 -- 2 = 1, so these stacked-up planes really just behave
like the scalars in \textbf{R}.

A final fact that makes all this much more neat and tidy: it turns out
that every subspace of a vector space V is the kernel of a homomorphism
from V. We can construct an example homomorphism easily, simply choose
f:V🡪V/U with the rule that maps f(\ul{v}) to {[}\ul{v}{]}. You should
convince yourself that this is indeed a homomorphism and that its kernel
is U. This proves the claim. Thus, all kernels are subspaces and all
subspaces are kernels.

\subsection{7.2 Extensions of the
	Scalars}\label{extensions-of-the-scalars}

\subsubsection{7.2.1 The Complex Numbers}\label{the-complex-numbers}

Consider the linear operators on \textbf{R}\textsuperscript{2}, which we
can write as 2x2 matrices. As you already know, Hom(V, W) is isomorphic
to W⊗V*, a vector space whose dimension is equal to dim(V)dim(W). In
this case we have Hom(\textbf{R}\textsuperscript{2},
\textbf{R}\textsuperscript{2}), which has dimension 4. In this section
we'll be interested in a subspace of this vector space, which consists
of all the matrices of the form

\[\begin{pmatrix}
	a & - b \\
	b & a
\end{pmatrix}\]

Note that we need to specify two real numbers, a and b, to identify a
vector in this subspace, so we can expect it to be two-dimensional. We
should check whether these do indeed form a vector subspace, which means
they must be closed under addition and scalar multiplication. Checking
addition first:

\[\begin{pmatrix}
	a & - b \\
	b & a
\end{pmatrix} + \begin{pmatrix}
	c & - d \\
	d & c
\end{pmatrix} = \begin{pmatrix}
	a + c & - b - d \\
	b + d & a + c
\end{pmatrix}\]

This is clearly of the same form as we started with, just with a = a + c
and b = b + d. Scalar multiplication is even easier:

\[k\begin{pmatrix}
	a & - b \\
	b & a
\end{pmatrix} = \begin{pmatrix}
	ka & - kb \\
	kb & ka
\end{pmatrix}\]

This set of matrices also happens to be closed under composition:

\[\begin{pmatrix}
	a & - b \\
	b & a
\end{pmatrix}\begin{pmatrix}
	c & - d \\
	d & c
\end{pmatrix} = \begin{pmatrix}
	ac - bd & - ad - bc \\
	ad + bc & ac - bd
\end{pmatrix}\]

Again, the result is in the right form to be a member of the subspace.
It's often useful to think of the composition of linear operators as a
sort of multiplication. This works because we an ``multiply out
brackets'' like in ordinary school algebra -- technically we say
composition \textbf{distributes} \textbf{over} addition:

\[{\begin{pmatrix}
		x & - y \\
		y & x
	\end{pmatrix}\left\lbrack \begin{pmatrix}
		a & - b \\
		b & a
	\end{pmatrix} + \begin{pmatrix}
		c & - d \\
		d & c
	\end{pmatrix} \right\rbrack = \begin{pmatrix}
		x & - y \\
		y & x
	\end{pmatrix}\begin{pmatrix}
		a + c & - b - d \\
		b + d & a + c
	\end{pmatrix}
}{= \begin{pmatrix}
		xa + xc - yb - yd & - xb - xd + xa - yc \\
		xb + xdya + yc & xa + xc - yb - yd
\end{pmatrix}}\]

So perhaps we can think of the vectors in this subspace of
\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{2}* as numbers
-- we can add and multiply them, after all, rather like the scalars in
\textbf{R} itself. A vector space in which the vectors can also be
multiplied is called (very confusingly) an \textbf{algebra}, and when we
want to specify the scalars we say an \textbf{R-algebra}, where R is the
scalars in question.

As an exercise, you might like to prove that the subspace contains an
additive identity (a ``zero'') along with additive inverses for all of
its elements, and a multiplicative identity (a ``one'') and
multiplicative inverses for all of its elements except ``zero''. These
are all straightforward calculations: the idea is to show that the
matrix you \emph{want} to exist has (or can be written in) the right
form to make it an element of the subspace.

In fact, there's a subspace of this space that is isomorphic to R; the
linear operators whose matrices have b = 0. The most natural isomorphism
would be

\[f\left\lbrack \begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} \right\rbrack = a\]

To confirm that this is a vector space isomorphism we need to check that
it respects scalar multiplication and vector addition:

\[f\left\lbrack k\begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} \right\rbrack = f\left\lbrack \begin{pmatrix}
	ka & 0 \\
	0 & ka
\end{pmatrix} \right\rbrack = ka = kf\left\lbrack \begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} \right\rbrack\]

\[f\left\lbrack \begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} + \begin{pmatrix}
	x & 0 \\
	0 & x
\end{pmatrix} \right\rbrack = f\left\lbrack \begin{pmatrix}
	a + x & 0 \\
	0 & a + x
\end{pmatrix} \right\rbrack = a + x = f\left\lbrack \begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} \right\rbrack + f\left\lbrack \begin{pmatrix}
	x & 0 \\
	0 & x
\end{pmatrix} \right\rbrack\]

Since \textbf{R} is a subspace of this space, and the whole space is
isomorphic to R2, we can expect there to be a second subspace isomorphic
to \textbf{R} that is its direct complement. This is easy to find --
it's the linear operators of the form

\[\begin{pmatrix}
	0 & - b \\
	b & 0
\end{pmatrix}\]

It may be a useful exercise to repeat the verification above that this
is indeed a vector space, and to convince yourself that it is the direct
complement of the other one.

To differentiate between these two copies of R we call the first the
\textbf{real numbers} as usual and the second the \textbf{imaginary
	numbers} (this name is traditional). The multiplicative identity in the
imaginary numbers is

\[\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\]

which is traditionally written as either i or j -- we will use i, which
is a lot more common in mathematical contexts. You can see that every
imaginary number is simply a scalar multiple of this, just as every real
number is a scalar multiple of the identity matrix.

We call the whole vector space, which is the direct sum of these two,
the \textbf{complex numbers}. The set of complex numbers is
traditionally written as \textbf{C}. In practical situations we usually
dispense with the heavy matrix notation and simply write

\[\begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} = a\ \ \ (a\ real\ number)\]

\[\begin{pmatrix}
	0 & - b \\
	b & 0
\end{pmatrix} = ib\ \ \ (an\ imaginary\ number)\]

\[\begin{pmatrix}
	a & - b \\
	b & a
\end{pmatrix} = \ \begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix} + \begin{pmatrix}
	0 & - b \\
	b & 0
\end{pmatrix} = a + \ ib\ \ \ \ (a\ complex\ number)\]

The reason anyone cares about complex numbers is the following:

\[i^{2} = \begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix} = \begin{pmatrix}
	- 1 & 0 \\
	0 & - 1
\end{pmatrix} = - 1\]

Thus the imaginary number i is, in this number system, the square root
of -1 and in general

\[\sqrt{- a} = \sqrt{a}\sqrt{- 1} = \sqrt{a}\ \begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix} = \begin{pmatrix}
	- \sqrt{a} & 0 \\
	0 & - \sqrt{a}
\end{pmatrix} = i\sqrt{a}\]

In the real number system, negative numbers don't have square roots and
this often causes problems in practical situations. It often happens
that while solving a problem we run into the square root of a negative
value and, when working with the real numbers, we're forced to say that
there is no solution and give up. If we work with the complex numbers
instead, there are solutions. Far from being ``imaginary'', these can be
of great significance, especially in solutions of differential
equations. Those lie outside the scope of this course but next week
we'll see another very important example.

\subsubsection{7.2.2 The Complex Numbers as a Kernel
	(*)}\label{the-complex-numbers-as-a-kernel}

In this section we'll disregard the multiplicative structure of the
complex numbers and just think of them as a vector space. This section
is required for understanding the rest of this week's reading but won't
be needed after that.

Consider the homomorphism
f:\textbf{R}\textsuperscript{4}🡪\textbf{R}\textsuperscript{2} defined by

\[f\begin{pmatrix}
	a \\
	b \\
	\begin{matrix}
		c \\
		d
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	a - d \\
	b + c
\end{pmatrix}\]

We can write vectors in the domain in matrix form as elements of
\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{2}*, giving

\[f\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix} = \begin{pmatrix}
	a - d \\
	b + c
\end{pmatrix}\]

The kernel of this homomorphism is the set of vectors that get mapped to
the zero vector, i.e. those for which a -- d = 0 and b + c = 0. But
these equations imply that d = a and c = -b, so the vectors in the
kernel are precisely those of the form

\[\begin{pmatrix}
	a & b \\
	- b & a
\end{pmatrix}\]

That is, they're the complex numbers. Since this is a 2D vector space
over the real numbers, it must be isomorphic to
\textbf{R}\textsuperscript{2}.

So we have the vector space
\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{2}* and a
subspace isomorphic to \textbf{R}\textsuperscript{2} that's the kernel
of a specified homomorphism (this is a common way to identify a
subspace). It's natural to ask: What is the quotient space,
(\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{2}*)/\textbf{R}\textsuperscript{2}?
Since \textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{2}* is
four-dimensional and \textbf{R}\textsuperscript{2} is two-dimensional,
the quotient must be two-dimensional as well. In the quotient space, for
any a and b we will have

\[\begin{pmatrix}
	w & x \\
	y & z
\end{pmatrix} = \begin{pmatrix}
	w + a & x + b \\
	y - b & z + a
\end{pmatrix}\]

For example,

\[\begin{pmatrix}
	5 & 2 \\
	1 & 3
\end{pmatrix} = \begin{pmatrix}
	6 & 3 \\
	2 & 4
\end{pmatrix}\]

This particular class of vectors will all be of the form

\[\begin{pmatrix}
	w & x \\
	x - 1 & w - 2
\end{pmatrix}\]

Varying the values of w or x gives you different vectors that are all
equivalent in the quotient space. It's the -1 and -2 that identify these
are a group. A particularly simple example of each class can be found by
setting w and x to zero, giving

\[\begin{pmatrix}
	0 & 0 \\
	- 1 & - 2
\end{pmatrix}\]

and in general

\[\begin{pmatrix}
	0 & 0 \\
	a & b
\end{pmatrix}\]

where the choice of a and b determined the vector in the quotient space.
This is clearly two-dimensional. Furthermore, the map

\[g\begin{pmatrix}
	0 & 0 \\
	a & b
\end{pmatrix} = \begin{pmatrix}
	a & b \\
	- b & a
\end{pmatrix}\]

is an isomorphism, since its kernel contains only the zero vector. This
justifies us in saying that the quotient space is another copy of
\textbf{C} and, therefore,

\[\mathbb{R}^{2}\bigotimes\mathbb{R}^{2*}\mathbb{\cong C\bigoplus C}\]

where the congruence symbol in the middle means ``these two are
isomorphic as vector spaces over the real numbers''.

\subsubsection{7.2.2 The Quaternions (*)}\label{the-quaternions}

(This section is for interest only, although you may like to know that
quaternions are used heavily in computer graphics and game development,
as well as some branches of engineering, robotics and astronomy.)

Amazingly, we can use the observations we just made to build another
number system. The direct sum of C with itself can be thought of in (at
least) two ways:

\begin{itemize}
	\item
	As we have been so far: as a four-dimensional real vector space
	\item
	In a new way: as a two-dimensional vector space over the complex
	numbers
\end{itemize}

The second way is possible because the complex numbers have such nice
arithmetic properties that they qualify as a ``field'' --it's a routine
matter to check this is true if you know (or look up) the definitions.
What that means is that we can use the complex numbers as scalars, and
this is very commonly done. On this course we try to avoid it, although
there are a few places where it makes things a lot easier.

So not only is \textbf{C}⊕\textbf{C} a four-dimensional real vector
space, it's also a two-dimensional complex vector space. To make things
easier (I can't promise ``easy'') we'll write elements of one side of
the direct sum as

\[\begin{pmatrix}
	a & b \\
	- b & a
\end{pmatrix} = a + bi\]

and elements of the other side as

\[\begin{pmatrix}
	0 & 0 \\
	c & d
\end{pmatrix} = c + di\]

although these are just notational choices; the two copies of \textbf{C}
in the direct sum are identical.

A good question now arises: If we can make \emph{numbers} by putting a
multiplication structure on \textbf{R}\textsuperscript{2}, can we do the
same trick with \textbf{C}\textsuperscript{2}? The answer is ``yes'',
and the result is the quaternions. The rest of this section carries out
the construction explicitly.

Let's find a basis for this space by finding bases for the two copies of
C in the direct sum. Remember, the scalars are complex numbers so
literally any choice other than the zero vector would be OK, but let's
try to be consistent with the argument so far. For the left-hand copy
I'll choose the matrix representing the real number 1:

\[\begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix}\]

and I'll choose the same for the right-hand copy, which is the matrix

\[\begin{pmatrix}
	0 & 0 \\
	1 & 0
\end{pmatrix}\]

Applying the isomorphism g from the previous section to this sends us to
the same representation as the other one, we'll just try to keep them
separate in our heads for as long as possible! One way to get really
confused would be to say that both of these basis elements represent the
real number 1. Each does in the context of its own set of complex
numbers, of course, but in the direct sum they're supposed to be
different.

We'll therefore use the letter j to represent the real number 1 in the
right-hand element of the direct sum. So the basis is \{1, j\} and the
scalars are complex numbers, so every vector in the space is of the form

\[(a + bi)\mathbf{1} + (c + di)\mathbf{j =}a + bi + cj + dij\]

It's convenient to write ij as k, giving us the quaternion

\[a + ib + cj + dk\]

Here a is being multiplied by the real number 1 and b by the imaginary
number i, which squares to -1. What about j and k? We have to make some
definitions to make the algebra work out, but the quickest way to see
them is to know that every quaternion can be represented by a 2x2
complex matrix of the form

\[a + ib + cj + dk = \begin{pmatrix}
	a + bi & c + di \\
	- c + di & a - bi
\end{pmatrix}\]

Notice the similarity with the complex numbers, but also the
differences. Each entry in the matrix is a complex scalar. Since it's a
2x2 matrix, it represents a linear operator on
\textbf{C}\textsuperscript{2}, i.e. an element of
\textbf{C}\textsuperscript{2}⊗\textbf{C}\textsuperscript{2}*. But as
with the complex numbers, the quaternions are only found in a subset of
this space. You can spot this by the fact that the matrix can be
specified by just four \emph{real} numbers (a, b, c and d); so as a
vector space over R this should be four-dimensional. But
\textbf{C}\textsuperscript{2}⊗\textbf{C}\textsuperscript{2}* is
four-dimensional over \textbf{C}; over \textbf{R} it would be
eight-dimensional, since \textbf{C} is a two-dimensional real vector
space and we have four copies of them here.

When you work out the algebra you'll see that ij = -ji and
j\textsuperscript{2} = -1. A version of these identities occurred to
William Rowan Hamilton while out walking near Dublin in 1843, and it is
said that he was so excited (and perhaps worried he'd forget them) that
he carved them into the stone of Broom Bridge, where a plaque now
stands. As far as I know it might be the most advanced mathematics ever
printed on a public commemorative plaque.

\subsubsection{7.2.3 The Frobenius Theorem
	(*)}\label{the-frobenius-theorem}

Hamilton's quaternions, like the complex numbers, are an algebra over
the real numbers: that is, they form a real vector space with a
multiplication operation. But there is a problem.

The real numbers have some nice properties. They are:

\begin{itemize}
	\item
	Ordered, meaning you can think of them as lying along a ``number
	line''
	\item
	Commutative, meaning ab = ba
	\item
	Associative, meaning (ab)c = a(bc)
	\item
	A ``division algebra'', meaning roughly that you can divide by any
	number except zero
	\item
	A norm, which gives a notion of ``size''
\end{itemize}

The complex numbers have all the properties except the first one: they
cannot be ordered in a way that is compatible with their arithmetic
properties. This is the price we have to pay for a two-dimensional
number system in which negative numbers have square roots. Often this
price is worth paying.

Like the complex numbers, the quaternions lack a canonical ordering. But
they are \emph{also} not commutative, as you can test for yourself. So
we must give up two nice features to get our fancy four-dimensional
numbers.

It is possible to make, by a similar process, a set of eight-dimensional
numbers called octonions, but those cannot be ordered, are not
commutative \emph{and} are not associative. This is a heavy price to
pay; the loss of associativity means their algebra doesn't look much
like arithmetic any more. What's more, we can even make a
sixteen-dimensional ``number system'' called the sedenions, but that
even fails to be a division algebra (as well as inheriting all the
undesirable properties of the other numbers). Neither octonions nor
sedenions have found many applications so they haven't been studied as
extensively as the others.

What's more interesting and surprising is that the real numbers, complex
numbers and quaternions are the \emph{only} possible associative
division algebras; this result is called the Frobenius Theorem. The
proof uses quotients, subspaces and the Rank-Nullity Theorem but also
requires some results from other parts of algebra; Wikipedia has a
decent summary of it that may be partially understandable at this point.

\subsubsection{7.2.4 The Dual Numbers (*)}\label{the-dual-numbers}

Just for fun, here's a number system similar to the complex numbers that
you can make in a similar way, but that has some different
characteristics. We will again start with the subspace of
Hom(\textbf{R}\textsuperscript{2}, \textbf{R}\textsuperscript{2}) where
all matrices are of the same form, but this time the form is

\[\begin{pmatrix}
	a & b \\
	0 & a
\end{pmatrix}\]

This is a two-dimensional space with basis

\[{\underline{e}}_{1} = \begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\underline{e}}_{2} = \begin{pmatrix}
	0 & 1 \\
	0 & 0
\end{pmatrix}\]

As with the complex numbers, we'll think of \ul{e}\textsubscript{1} as
the number 1 and \ul{e}\textsubscript{2} as a special, new number. This
time we have

\[{\underline{e}}_{2}^{2} = \begin{pmatrix}
	0 & 1 \\
	0 & 0
\end{pmatrix}\begin{pmatrix}
	0 & 1 \\
	0 & 0
\end{pmatrix} = \begin{pmatrix}
	0 & 0 \\
	0 & 0
\end{pmatrix}\]

so \ul{e}\textsubscript{2} is a non-zero square root of zero. A number
that turns into zero when raised to a power is said to be
\textbf{nilpotent} and such numbers are of great importance in some
fields of geometry and calculus, where they're used to represent
infinitesimally small quantities. They can also be used in computer
programming to perform calculus operations rapidly; techniques related
to this fall under the general title of \textbf{automatic
	differentiation}. The dual numbers have found very specialized
applications in a few other fields such as particle physics and
robotics.

The Frobenius Theorem tells us that the dual numbers can't be an
associative division algebra. They are obviously associative, since
their multiplication is ordinary matrix multiplication and that's
associative. But they don't form a division algebra since it's
impossible to define division by any dual number of the form
0\ul{e}\textsubscript{1} + k\ul{e}\textsubscript{2}; those dual numbers
lack multiplicative inverses. Notice what happens when we multiply any
dual number by k\ul{e}\textsubscript{2}:

\[\begin{pmatrix}
	a & b \\
	0 & a
\end{pmatrix}\begin{pmatrix}
	0 & k \\
	0 & 0
\end{pmatrix} = \begin{pmatrix}
	0 & ak \\
	0 & 0
\end{pmatrix}\]

We lose some information here, specifically what the value of b was.
This means we can never multiply the result by some dual number that
would be the multiplicative inverse of k\ul{e}\textsubscript{2} and get
back to the original number. Like the complex numbers, the dual numbers
are commutative:

\[\begin{pmatrix}
	x & y \\
	0 & x
\end{pmatrix}\begin{pmatrix}
	a & b \\
	0 & a
\end{pmatrix} = \begin{pmatrix}
	xa & xb + ya \\
	0 & xa
\end{pmatrix} = \begin{pmatrix}
	a & b \\
	0 & a
\end{pmatrix}\begin{pmatrix}
	x & y \\
	0 & x
\end{pmatrix}\]

Note that as vector spaces over the real numbers, the dual numbers and
the complex numbers are isomorphic. The difference between them is in
how multiplication is defined, and vector spaces don't ``know'' anything
about that. The two number systems are, however, not isomorphic as
algebras over the real numbers -- algebras do care about multiplication,
and the presence of a full compliment of multiplicative inverses in the
complex numbers but not in the dual numbers is enough to show us they
can't be isomorphic.

\chapter{Spectral Analysis}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 8.1} introduces eigenvalues, which are numbers
	associated with a linear operator that give us information about its
	behaviour.
	\item
	\textbf{Section 8.2} uses eigenvalue techniques to examine the
	behaviour of systems characterized by a linear operator that is
	applied repeatedly. We are usually especially interested in the
	``long-term behaviour'' of such a system.
\end{itemize}

If time is short, focus on the non-starred sections of 8.1. Section 8.2
is intended to help you get an intuitive sense of the topic through some
fairly simple applications; if you find 8.1 difficult, continue through
8.2 before going back to 8.1 again as the examples should help to
provide some context for the theory.

\subsection{Eigenvalues and Eigenvectors}

\subsubsection{8.1.1 Introduction}\label{introduction-1}

An \textbf{eigenvector} of a linear operator is a vector that
``experiences'' the operator's action as if it were just scalar
multiplication. In other words, it's a vector that ends up pointing
along the same line it did before, but perhaps with a change in length
or a flip of direction. Here's an example matrix acting on two of its
eigenvectors:

\[\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\begin{pmatrix}
	1 \\
	1
\end{pmatrix} = \begin{pmatrix}
	3 \\
	3
\end{pmatrix}\]

\[\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\begin{pmatrix}
	- 1 \\
	1
\end{pmatrix} = \begin{pmatrix}
	1 \\
	- 1
\end{pmatrix}\]

The first vector has been transformed by the matrix but it just looks as
if it's been scalar-multiplied by 3; the second seems to have been
scalar-multiplied by -1. These multiples are the \textbf{eigenvalues} of
the matrix. Finding eigenvalues and eigenvectors of a linear operator
can help us understand it and is a step in many algorithms, as you'll
see many times in this course. The set of all the eigenvalues of a
linear operator is called its \textbf{spectrum}.

Any vector that's a scalar multiple of an eigenvector is again an
eigenvector with the same eigenvalue, e.g.

\[\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\left( ( - 2)\begin{pmatrix}
	1 \\
	1
\end{pmatrix} \right) = \begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\begin{pmatrix}
	- 2 \\
	- 2
\end{pmatrix} = \begin{pmatrix}
	- 6 \\
	- 6
\end{pmatrix}\]

Here we've chosen a vector that's the original (top) one scaled by a
factor of -2. But the result of applying the matrix is, again, to scale
that vector by the same eigenvalue of 3.

For any linear operator M its eigenvectors
\textbf{v\textsubscript{1},\ldots, v\textsubscript{n}} and eigenvalues
a\textsubscript{1},\ldots, a\textsubscript{n} must satisfy the equation

\[Mv_{i} = a_{i}v_{i}\]

We can rewrite this in a different way if we insert the identity
operator, I, as follows:

\[Mv_{i} = a_{i}Iv_{i}\]

By rearranging this equation we can obtain

\[{Mv_{i} - a_{i}Iv_{i} = 0
}{\left( M - a_{i}I \right)v_{i} = 0\ }\]

The thing in brackets is a linear operator, and since it sends the
non-zero vector v\textsubscript{i} to the zero vector, its determinant
must be zero. In fact, every eigenvalue a\textsubscript{i} must satisfy
the equation

\[\det\left( M - a_{i}I \right) = 0\]

We saw how to calculate the determinant in Session 5 using the volume
form. This is rather tiresome to do by hand but the algebra involved is
all very simple. Let's review that by calculating it for the example
above, i.e.

\[\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix} - a_{i}\begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix} = \begin{pmatrix}
	1 - a_{i} & 2 \\
	2 & 1 - a_{i}
\end{pmatrix}\]

This would be a good thing to try yourself before reading on! First we
write the volume form down from the columns of the matrix, then we
expand it using the basis and simplify:

\[{\begin{pmatrix}
		1 - a_{i} \\
		2
	\end{pmatrix} \land \begin{pmatrix}
		2 \\
		1 - a_{i}
	\end{pmatrix} = \left( \left( 1 - a_{i} \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} + 2\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \right) \land \left( 2\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} + \left( 1 - a_{i} \right)\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \right)
}{= \left( 1 - a_{i} \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land 2\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} + \left( 1 - a_{i} \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land \left( 1 - a_{i} \right)\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} + 2\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \land 2\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} + 2\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \land \left( 1 - a_{i} \right)\begin{pmatrix}
		0 \\
		1
	\end{pmatrix}
}{= \left( 1 - a_{i} \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land \left( 1 - a_{i} \right)\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} + 2\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \land 2\begin{pmatrix}
		1 \\
		0
	\end{pmatrix}
}{= \left( 1 - a_{i} \right)^{2}\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land \begin{pmatrix}
		0 \\
		1
	\end{pmatrix} + 4\begin{pmatrix}
		0 \\
		1
	\end{pmatrix} \land \begin{pmatrix}
		0 \\
		1
	\end{pmatrix}
}{= \left( \left( 1 - a_{i} \right)^{2} - 4 \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land \begin{pmatrix}
		0 \\
		1
	\end{pmatrix}
}{= \left( a_{i}^{2} - 2a_{i} - 3 \right)\begin{pmatrix}
		1 \\
		0
	\end{pmatrix} \land \begin{pmatrix}
		0 \\
		1
\end{pmatrix}}\]

If a\textsubscript{i} is an eigenvalue the determinant is zero, so the
eigenvalues are the solutions of the equation

\[a_{i}^{2} - 2a_{i} - 3 = 0\]

In this case we can solve it in at least three ways: by noticing we can
rewrite it as (a\textsubscript{i} - 3)(a\textsubscript{i} + 1) = 0, by
using the quadratic formula or by using a computer (the latter is more
practical when the dimensions are higher). The result is what we expect:
the solutions are 3 and -1, the two eigenvalues of the linear operator.

Note that the eigenvalues do not depend on the basis in which the matrix
of the operator was written down. Changing the basis never changes the
eigenvalues. The coordinates of the eigenvectors will change, of course,
since they'll need to be expressed in the new basis.

It follows that if two matrices A and B are similar (see 5.3.4), i.e.
there is a basis transformation T such that A = T\textsuperscript{-1}BT,
they must have the same eigenvalues. But beware! Just because A and B
have the same eigenvalues does not guarantee that they are similar; they
are then said to be \textbf{isospectral} because they share the same
spectrum but are different operators.

\subsubsection{8.1.2 The Determinant is the Product of the Eigenvalues
	(*)}\label{the-determinant-is-the-product-of-the-eigenvalues}

We have seen that the eigenvalues of a linear operator M are the values
of x that satisfy

\[\det(M - xI) = 0\]

We've also seen that when we calculate this we end up having to solve a
polynomial that gives the scalar multiple of the volume form. The zeros
of that polynomial are the eigenvalues. But then it follows that we can
rewrite that polynomial by factorizing it as

\[f(x) = \left( x - a_{1} \right)\ldots\left( x - a_{n} \right) = 0\]

Since these are just two ways to define the determinant, we can identify
the left hand sides:

\[\det(M - xI) = \left( x - a_{1} \right)\ldots\left( x - a_{n} \right)\]

Now, suppose we look at what happens when x = 0. On the left we get

\[\det(M - xI) = \det(M - 0I) = \det(M)\]

and on the right we get the product of the eigenvalues, with a minus
sign if the dimension is odd:

\[\left( 0 - a_{1} \right)\ldots\left( 0 - a_{n} \right) = ( - 1)^{n}a_{1}\ldots a_{n}\]

We therefore arrive at the surprising fact that the determinant of a
linear operator is equal to the product of its eigenvalues, being
careful to include a factor of -1 if there are an odd number of them:

\[\det(M) = ( - 1)^{n}a_{1}\ldots a_{n}\]

\subsubsection{8.1.3 Interpretation of
	Eigenvalues}\label{interpretation-of-eigenvalues}

Remember that, on this course, our scalars are always the real numbers
\textbf{R}, except in some special cases. The example in the previous
section showed a linear operator on \textbf{R}\textsuperscript{2} with
two eigenvalues that are real numbers. It turns out that a linear
operator on \textbf{R}\textsuperscript{n} will always have n
eigenvalues. However, eigenvalues need not be real numbers: some might
be complex numbers. The geometric meaning is a bit different in each
case.

\paragraph{8.1.3.1 Real, Distinct
	Eigenvalues}\label{real-distinct-eigenvalues}

A real eigenvalue can usually be visualized through its eigenvectors.
Any scalar multiple of an eigenvector is another eigenvector with the
same eigenvalue. After all, geometrically the linear operator scalar
multiplies its eigenvectors: that is, it stretches or squeezes them
along a line. It must therefore do the same thing to all the vectors
that lie along that line.

We can therefore speak of an \textbf{eigenline}, which is the line on
which all these scalar-multiplied versions of the eigenvector live.
Every eigenline of a linear operator must be always a straight line that
passes through the origin, and every vector on that line is scaled by
the same eigenvalue.

Another way to say this is that an eigenline consists of the
one-dimensional subspace spanned by any one real eigenvector associated
with it. Eigenlines are \textbf{invariants} of the transformation in the
sense that if a vector is on an eigenline before the matrix is applied,
it will still be on that line afterwards. But note that these are not
necessarily the only lines where this is true: there might be more
invariant lines than there are eigenlines.

\paragraph{8.1.3.2 Real, Repeated
	Eigenvalues}\label{real-repeated-eigenvalues}

The claim about eigenlines works as long as the eigenvalues are
distinct. Consider the following matrix, which scales up every vector by
a factor of 2:

\[\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix}\]

The characteristic equation is

\[{\det\left( M - a_{i}I \right) = 0
}{\left( 2 - a_{i} \right)\left( 2 - a_{i} \right) - 0 = 0
}{a_{i}^{2} - 4a_{i} + 4 = 0}\]

Using the quadratic formula is supposed to yield the two roots of the
polynomial but it only gives us one:

\[\frac{- b \pm \sqrt{b^{2} - 4ac}}{2a} = \frac{4 \pm \ \sqrt{16 - 16}}{2} = 2\]

In this case you might expect there to be one or two eigenvectors that
the operator scales, but if you think about what it does geometrically,
this is wrong: \emph{every} vector is an eigenvector! The repeated
eigenvalue is telling you this. The \textbf{algebraic multiplicity} of
the eigenvalue x is the number of times (a\textsubscript{i} - x) appears
when you factorize the polynomial; in higher dimensions you can have a
range of eigenvalues with different multiplicities.

For each eigenvalue there is a subspace of the domain on which the
operator acts as a scaling. These subspaces are eigenlines or their
higher-dimensional couterparts, simply vector subspaces embedded in the
domain. These are known as \textbf{eigenspaces}. The dimension of the
eigenspace corresponding to an eigenvalue is known as its
\textbf{geometric multiplicity}. It would be nice if geometric and
algebraic multiplicities were equal, but this is not always true. We can
say, however, that the geometric multiplicity can never be greater than
the algebraic. If the two are not equal, the eigenvalue is sometimes
said to be \textbf{defective}.

If an eigenvalue is equal to zero, its corresponding eigenspace is the
space of vectors in the domain that are sent to the zero vector, i.e.
the vectors \ul{v} such that M\ul{v} = \ul{0}. The dimension of the
eigenspace of the zero eigenvalue is therefore equal to the dimension of
the kernel of M. Thus, the algebraic multiplicity of the zero eigenvalue
gives you an upper limit on the dimension of the kernel.

\paragraph{8.1.3.4 Complex Eigenvalues}\label{complex-eigenvalues}

What about complex eigenvalues? Let's work through an example. Consider
the linear operator represented (in a chosen basis, of course) by the
matrix

\[\begin{pmatrix}
	1 & 2 \\
	- 2 & 1
\end{pmatrix}\]

To find its eigenvalues we must solve the equation

\[\det\left( M - a_{i}I \right) = 0\]

i.e.

\[\det\begin{pmatrix}
	1 - a_{i} & 2 \\
	- 2 & 1 - a_{i}
\end{pmatrix} = 0\]

Using the same method as before (or the simplified 2x2 formula ad - bc)
, we would arrive at

\[\det\begin{pmatrix}
	1 - a_{i} & 2 \\
	- 2 & 1 - a_{i}
\end{pmatrix} = \left( 1 - a_{i} \right)\left( 1 - a_{i} \right) - \left( 2 \times ( - 2) \right) = a_{i}^{2} - 2a_{i} + 5\]

To solve this equation we can use the quadratic formula, but we arrive
at a pair of complex numbers as eigenvalues:

\[{\frac{- b \pm \sqrt{b^{2} - 4ac}}{2a} = \frac{2 \pm \ \sqrt{4 - 20}}{2}
}{= \frac{2 \pm \ \sqrt{- 16}}{2}
}{= \frac{2 \pm \ \sqrt{- 1 \times 16}}{2}
}{= \frac{2 \pm \ \sqrt{- 1}\sqrt{16}}{2}
}{= \frac{2 \pm \ 4i}{2}
}{= 1 + 2i\ \ \ \ or\ \ \ \ 1 - 2i}\]

A pair like this, of the form a + bi and a -- bi, are said to be
\textbf{conjugate}. Conjugate pairs of complex numbers arise often and
are important to the theory of complex numbers in general.

Certainly I can't scalar-multiply a vector by a \emph{complex} number if
the vector lives in a \emph{real} vector space: complex numbers simply
aren't available as scalars in the vector space and the resulting vector
wouldn't exist. So there are no eigenvectors or eigenlines associated
with these eigenvalues. You can see this geometrically if you use the
matrix visualizer
(\url{https://web.ma.utexas.edu/users/ysulyma/matrix/}) -- this linear
operator rotates and scales the vectors it acts on, which means no
vector ends up pointing along the same line as it did before. But we'll
say a bit more about this in the next section.

\subsection{8.2 Convolution}\label{convolution}

\subsubsection{8.2.1 Repeated Scaling}\label{repeated-scaling}

Since a linear operator is a map with the same domain and codomain, we
can apply it multiple times. This is known as \textbf{convolution}. For
example, the matrix

\[\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix}\]

scales vectors by doubling their lengths, as we saw in section 8.1.3.2.
If we apply it twice, the overall effect is to scale by a factor of 4
instead:

\[\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix}\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix} = \begin{pmatrix}
	4 & 0 \\
	0 & 4
\end{pmatrix}\]

In many applications, we are interested in the long-term behaviour of
systems that can be described by the repeated application of a linear
operator. Since the composition of linear operators looks like
multuiplication of numbers (and people even sometimes call it ``matrix
multiplication'') we can write the application of the matrix n times as

\[\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix}^{n}\]

In this case we have

\[\begin{pmatrix}
	2 & 0 \\
	0 & 2
\end{pmatrix}^{n} = \begin{pmatrix}
	2^{n} & 0 \\
	0 & 2^{n}
\end{pmatrix}\]

The long-term behaviour of this operator is that the vectors get bigger
and bigger, doubling in size with each convolution. This is represented
by the fact that the overall effect is the linear transformation on the
right, which is a scaling by larger and larger amounts as the value of n
increases.

This one, however, is a bit different:

\[\begin{pmatrix}
	0.5 & 0 \\
	0 & 0.5
\end{pmatrix}^{n} = \begin{pmatrix}
	{0.5}^{n} & 0 \\
	0 & {0.5}^{n}
\end{pmatrix}\]

This represents a shrinking effect -- the repeated eigenvalue is of
course 0.5. This time, as n increases the overall effect gets closer and
closer to the zero transformation

\[\begin{pmatrix}
	{0.5}^{n} & 0 \\
	0 & {0.5}^{n}
\end{pmatrix} \rightarrow \begin{pmatrix}
	0 & 0 \\
	0 & 0
\end{pmatrix}\ \ \ \ \ \ as\ n\ gets\ very\ large\]

As we've seen, when a linear operator has a real eigenvalue it behaves
like a scaling of a subspace whose dimension matches the multiplicity of
the eigenvalue. When it's convolved many times, its long-term effect on
vectors in that subspace will be one of the following:

\begin{itemize}
	\item
	The vectors will grow longer and longer, without limit, if the
	absolute value of the eigenvalue is greater than 1.
	\item
	The vectors will grow smaller and smaller, getting ever closer to the
	zero vector, if the absolute value of the eigenvalue is less than 1.
	\item
	The vectors will not change at all if the eigenvalue is exactly equal
	to 1.
\end{itemize}

More generally, a linear operator is said to be \textbf{stable} if all
its eigenvalues real parts whose absolute value is less than 1. As the
name suggests, a stable matrix will tend to draw the vectors into a
``steady state'' or equilibrium as you convolve it. We'll look more
closely at these ideas in Session 11.

\includegraphics[width=3.32708in,height=2.55139in,alt={Vector Space Projection -\/- from Wolfram MathWorld}]{media/image31.gif}Another
piece of jargon: if M is a linear operator and M\textsuperscript{2} = M,
we say M is \textbf{idempotent}, meaning that applying it more than once
has no additional effect. For example, consider the matrix

\[\begin{pmatrix}
	1 & 0 \\
	0 & 0
\end{pmatrix}\]

It acts on a vector by

\[\begin{pmatrix}
	1 & 0 \\
	0 & 0
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	x \\
	0
\end{pmatrix}\]

Further application of this transformation will just yield the same
result. Geometrically, this matrix represents a ``projection'' of the
vector it acts on onto the one-dimensional subspace spanned by the first
basis vector. Idempotent operators can often be thought of as
projections in this way.

\subsubsection{8.2.2 Repeated Rotations}\label{repeated-rotations}

In section 8.1.3.3 we learned that linear operators representing
rotations have complex eigenvalues. Let's see why that might be the
case. Consider the matrix

\[\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\]

In an orthonormal basis this has the effect of rotating by 90 degrees
clockwise,

\[\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\binom{x}{y} = \binom{- y}{x}\]

(In a non-orthonormal basis, measurements of angles don't correspond to
our ordinary geometric ones -- this was discussed briefly all the way
back in section 3.4)

If we apply it four times, therefore, we'll be back where we started,
and you can check this yourself by hand if you like:

\[\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}^{4} = \begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix} = \begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix}\]

More importantly, if we apply it twice we get rotation by a half-turn,
which is geometrically the same as scaling by -1:

\[\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}^{2} = \begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix}\begin{pmatrix}
	0 & - 1 \\
	1 & 0
\end{pmatrix} = \begin{pmatrix}
	- 1 & 0 \\
	0 & - 1
\end{pmatrix}\]

So in a sense this operator is the ``square root of scaling by -1''.
That is, it's the matrix that, when ``multiplied by itself'' (i.e.
convolved), yields the same as multiplication by -1. And indeed, it's
the same matrix as the one we used to represent the complex number i,
which is the square root of -1, in 7.2.1. This should suggest to you
that the connection between complex eigenvalues and rotations is at
least somewhat reasonable. (There is lots of deeper algebra here that we
won't go into -- you might enjoy working some of it out, or reading an
introductory text on complex numbers and translating everything into the
language of linear operators on \textbf{R}\textsuperscript{2}.)

\subsubsection{8.2.3 The Dominant Eigenvalue and Rayleigh
	Quotient}\label{the-dominant-eigenvalue-and-rayleigh-quotient}

Let's now go back to the operator we started this session off with,

\[\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\]

It has distinct real eigenvalues -1 and 3. What happens in the long term
if we convolve it? The answer is that all vectors in the space move
towards the eigenline for one of the eigenvectors. The eigenvector that
``wins'' is simply the one with the largest absolute value. This is
called the \textbf{dominant eigenvalue}; in this case it's 3. Beware,
though: to be the dominant one, an eigenvalue must be the \emph{unique}
largest of them; that means it must have multiplicity 1 and there can't
be another eigenvalue that's identical to it but with a different sign.

If we are faced with a large matrix and are having difficulty finding an
eigenvector for it, we can use the \textbf{power method} to find an
approximation. Simply guess an eigenvector and repeatedly apply the
matrix to it! After enough convolutions, you will have an approximation
of an eigenvector of the dominant eigenvalue, if there is one. This is a
great task for a computer to do as involves lots of boring calculations.
It's common to help the process along by, at each step, scaling the
transformed vector by 1/k where k is the largest coordinate of the
vector (in terms of absolute value). This slows down any tendency the
process has to cause vectors to ``run away'' in size.

Having approximated an eigenvector, you might be wondering how to get
the corresponding eigenvalue. This can be done using the
\textbf{Rayleigh quotient},

\[\frac{{\underline{v}}^{*}A\underline{v}}{{\underline{v}}^{*}\underline{v}}\]

Here A is the linear operator and \ul{v} is the eigenvector (or an
approximation of it). This number gives you the corresponding eigenvalue
(or an approximation of it). However, this only works if A is
``Hermitian'', a property we will describe next time.

\subsubsection{8.2.4 Eigenvalues of
	Compositions}\label{eigenvalues-of-compositions}

The set of eigenvalues of a linear operator is often called its
\textbf{spectrum}, written Spec(A) for the operator A. It's often of
interest to know Spec(AB), where A and B are operators being composed
together.

Suppose that k is an eigenvalue of AB:

\[AB\underline{v} = k\underline{v}\]

We could apply B to both sides:

\[BAB\underline{v} = B\left( k\underline{v} \right)\]

but this is equivalent to

\[\ (BA)B\underline{v} = k(B\underline{v})\]

which amounts to saying that k is an eigenvalue of BA. Since we could
apply this argument to any eigenvalue of AB or BA, this shows that
Spec(AB) = Spec(BA).

\chapter{Adjoints}

Overview of the reading for this session:

\begin{itemize}
	\item
	\textbf{Section 9.1} introduces the idea of the adjoint of a linear
	operator, which is in a sense its equivalent in the dual space.
	\item
	\textbf{Section 9.2} is in two separate parts. The first part
	discusses a deep result in linear algebra that gives us a chance to
	review some of the theory we covered in sessions 4 and 6. The part
	second describes the idea of matrix vectorization.
	\item
	\textbf{Section 9.3} introduces the trace of a linear operator, a
	number that's often useful in applications.
\end{itemize}

If time is short, focus on 9.1.1, 9.1.2 and the beginning of 9.3.
Skimming 9.2.1 will help you understand some parts of the argument in
9.3 but isn't essential.

\subsection{9.1 Duality and Linear
	Operators}\label{duality-and-linear-operators}

\subsubsection{9.1.1 Review: Linear Operators and
	Covectors}\label{review-linear-operators-and-covectors}

Recall that if V is a vector space, V* is the dual space and its
vectors, which we call ``covectors'', can be written as row vectors. A
linear operator acts on a vector by coming at it from the left:

\[\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	ax + by \\
	cx + dy
\end{pmatrix}\]

but on a covector from the right:

\[\begin{pmatrix}
	x & y
\end{pmatrix}\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix} = \begin{pmatrix}
	ax + cy & bx + dy
\end{pmatrix}\]

Certainly this matrix seems to represent a linear operator acting on the
dual space, V*. First, we should be certain this makes sense. The
original operator was a homomorphism V🡪V, i.e. an element of V⊗V*. This
new one is a homomorphism V*🡪V*, i.e. an element of V*⊗V. These appear
to be different vector spaces, but are they? They are in fact the same;
there are at least three ways we could argue this.

First, if n is the dimension of V then the dimension of V⊗V* is
n\textsuperscript{2} and so is the dimension of V*⊗V. We've seen (6.2.3)
that any two vector spaces over \textbf{R} that have the same finite
dimension are isomorphic. This is an efficient way to prove they're the
same space from a linear algebra perspective, but it isn't very
satisfying.

Second, we could argue as follows. Given a choice of basis for V, we get
the induced basis for V* and this is enough to prove that V and V* are
isomorphic. But then suppose we have A⊗B and C⊗D but we know A is
isomorphic to C and B is isomorphic to D. Then it must be that the
tensor spaces are also isomorphic. If the two pairs of spaces are the
same, combining them in the same way (i.e. using the tensor product)
can't produce different results!

Third, we could notice that the tensor product is commutative: A⊗B and
B⊗A are always going to be isomorphic vector spaces. This is a simple
consequence of the definition of the tensor product and is very easy to
apply once you know it.

It therefore makes sense that a linear operator should have not only
eigenvectors but ``eigencovectors''. But that's not a word: instead we
call them \textbf{right eigenvectors} (the normal ones) and \textbf{left
	eigenvectors} (the dual ones).

You might expect that the right and left eigenvectors would be
transposes of each other, but they're usually not. That's because a
linear operator transforms covectors covariantly but vectors
contravariantly (see 5.3.3 for a reminder). And in general we can expect
the geometric effect of the operator on the dual space to be quite
different from the effect on the original space.

\subsubsection{9.1.2 The Dual of a Linear
	Operator}\label{the-dual-of-a-linear-operator}

Remember that V* is just a vector space, its elements are just vectors
(though we sometimes \emph{call} them covectors) and writing them as row
vectors is just a notational convenience. It would be nice to be able to
rewrite

\[\begin{pmatrix}
	x & y
\end{pmatrix}\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix} = \begin{pmatrix}
	ax + cy & bx + dy
\end{pmatrix}\]

as a linear operator on V instead of V*, i.e.

\[\begin{pmatrix}
	? & ? \\
	? & ?
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	ax + cy \\
	bx + dy
\end{pmatrix}\]

It's pretty clear how to fill in the question marks:

\[\begin{pmatrix}
	a & c \\
	b & d
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	ax + cy \\
	bx + dy
\end{pmatrix}\]

This is the dual of the original operator: that is, it's the operator
that acts on V the way the original one acted on V*. And you can see
that for a 2x2 matrix all we had to do was swap over the values in the
upper right and lower left corners. If we represent the matrix as M, its
dual should logically be written M* and that's the way we'll write it on
this course, although M\textsuperscript{T} is a bit more common.

In general the procedure is very simple. A linear operator on an
n-dimensional space is represented by an n-by-n square matrix. To find
its dual, simply reflect the matrix's entries across the main
top-left-to-bottom-right diagonal so that its rows become columns and
its columns become rows. It's easiest to see this with a simple example:

\[\begin{pmatrix}
	\begin{matrix}
		1 & 2 \\
		5 & 6
	\end{matrix} & \begin{matrix}
		3 & 4 \\
		7 & 8
	\end{matrix} \\
	\begin{matrix}
		9 & 10 \\
		13 & 14
	\end{matrix} & \begin{matrix}
		11 & 12 \\
		15 & 16
	\end{matrix}
\end{pmatrix}^{*} = \begin{pmatrix}
	\begin{matrix}
		1 & 5 \\
		2 & 6
	\end{matrix} & \begin{matrix}
		9 & 13 \\
		10 & 14
	\end{matrix} \\
	\begin{matrix}
		3 & 7 \\
		4 & 8
	\end{matrix} & \begin{matrix}
		11 & 15 \\
		12 & 16
	\end{matrix}
\end{pmatrix}\]

Notice how the roles or rows and columns have been exchanged. This makes
sense as the ``transpose'' of a matrix if you think of its rows and
columns coming from the covectors and vectors in the tensor product,
which is how the matrix notation was first introduced (see 4.2.1).

In a real vector space equipped with a basis the following is true: The
dual of a linear operator is represented by the transpose of its matrix.
Note that if the scalars were complex numbers we would have to take the
conjugates of all the entries as well -- all vector spaces on this
course are real, but this is just something to bear in mind.

The dual of a linear operator is also called its \textbf{adjoint}. In
fact, historically this term applies to two quite similar concepts --
the ``Hermitian adjoint'', which is the one we're talking about, and the
``classical adjoint'', which is also sometimes called the ``adjugate''.
Just be careful when reading other sources that you know which one is
being referred to.

What's meant by the term ``adjoint'' is perhaps best captured by the
following identity:

\[w^{*}(Mv) = \left( w^{*}M^{*} \right)v\]

In words: ``transforming v by M, then letting w* act on it, is the same
as transforming w* by M* and then letting it act on v''. You should
check this is true by calculating with an example in
\textbf{R}\textsuperscript{2} (any example will do but using a matrix
without any zeros will be the most revealing).

In a finite-dimensional vector space V with an inner product, every
linear operator has an adjoint. The inner product is needed to give us a
correspondence between the vectors in V and covectors in V*. In
infinite-dimensional vector spaces there can be linear operators that
have no adjoint.

Here are some reassuring algebraic properties of adjoints -- here A and
B are linear operators on V and k is a scalar:

\begin{itemize}
	\item
	(A + B)* = A* + B*
	\item
	k(A*) = (kA)*
	\item
	(AB)* = B*A* (note the reversal of order!)
\end{itemize}

It's also reasonably obvious from the construction that M** = M -- just
as with the dual vector space, where the dual of the dual is the
original space, so the adjoint of the adjoint is the original operator.

One last note: although this course is about real vector spaces, you may
well encounter complex vector spaces in future. In that case the adjoint
is formed in the same way except that all the entries in the matrix must
also be conjugated -- that is, every a + bi must become an a -- bi and
\emph{vice versa}.

\subsubsection{9.1.3 Hermitian and Unitary
	Operators}\label{hermitian-and-unitary-operators}

A linear operator is said to be \textbf{Hermitian} if it is its own
adjoint, i.e. if M* = M; another, more descriptive term is
\textbf{self-adjoint}. If an operator is Hermitian, any matrix
representing it will have to be symmetrical about its main diagonal;
these are often just called \textbf{symmetric} matrices.

Hermitian operators have the nice property that all their eigenvalues
are real and it's possible to find a set of eigenvectors that form an
orthonormal basis of the space. Real eigenvalues are important in some
applications of linear algebra, as for example in the solution of
differential equations. Note, however, that a matrix can have all real
eigenvalues but fail to be Hermitian. For example,

\[\begin{pmatrix}
	1 & 1 \\
	0 & 2
\end{pmatrix}\]

has eigenvalues 1 and 2 but is clearly not Hermitian -- we can see from
the lack of symmetry.

By contrast, an operator is said to be \textbf{unitary} if its adjoint
is equal to its inverse, i.e.

\[M^{*} = M^{- 1}\]

The easiest non-trivial examples are rotations. In two dimensions and
with an orthonormal basis, a rotation by an angle of k is represented by
the matrix

\[M = \begin{pmatrix}
	cos(k) & - sin(k) \\
	sin(k) & cos(k)
\end{pmatrix}\]

The adjoint can easily be seen to be

\[M^{*} = \begin{pmatrix}
	cos(k) & sin(k) \\
	- sin(k) & cos(k)
\end{pmatrix}\]

The inverse of a rotation would be the one that rotates by the same
angle in the opposite direction, i.e. by the angle -k, which we can
simplify using facts about the trigonometric functions sin and cos that
you might remember from previous studies:

\[{M^{- 1} = \begin{pmatrix}
		\cos( - k) & - \sin( - k) \\
		\sin( - k) & \cos( - k)
	\end{pmatrix}
}{= \begin{pmatrix}
		cos(k) & sin(k) \\
		- sin(k) & cos(k)
\end{pmatrix}}\]

As you can see, in this case the inverse really is equal to the adjoint.

In the world of real vector spaces, unitary matrices are the same things
as \textbf{orthogonal} matrices. An orthogonal matrix has columns that,
if they were column vectors, would be orthogonal to each other;
\emph{and} its rows, if they were row vectors (covectors), would be
orthogonal to each other as well. Usually the term also means the
vectors are all the same length, and so are the covectors. Note that
there has to be an inner product around to make sense of orthogonality
and length, whereas the definition of a unitary matrix doesn't require
that. Any such matrix always have determinant either +1 or -1 -- that
is, it's either a reflection, a rotation or a combination of the two.
It's easy to see that a composition of two unitary operators is itself
unitary.

Both Hermitian and unitary matrices are important in applications. We
will catch a few glimpses of this in upcoming sessions.

\subsection{9.2 Matrices are Vectors (*)}\label{matrices-are-vectors}

\subsubsection{9.2.1 The Adjunction of Tensor to Hom
	(*)}\label{the-adjunction-of-tensor-to-hom}

This section aims to explain the following famous isomorphism, where U,
V and W are any (finite-dimensional) vector spaces:

\[Hom(U\bigotimes V,\ W) \cong Hom(U,\ Hom(V,\ W))\]

Here Hom(V, W) means the set of all homomorphisms V🡪W. The first thing
to notice is that Hom(V, W) is a vector space, since it's just W⊗V*.
Thus the expression on the right really does make sense: it's the vector
space of all homomorphisms from U to W⊗V*.

This isomorphism is known as the \textbf{adjunction of tensor to Hom}.
It has a similar shape to the adjunction of M* to M, although it's not
easy to see at first. It says ``combining U with V, then combining the
result with W is the same as combining V with W, then combining the
result with U \emph{except} in the first case you have to use the tensor
product for the first combination you do, whereas in the second you have
to use Hom''. So the isomorphism says that the tensor product and Hom
stand in the same structural relationship to each other as M* and M do.

It's easy to prove that the isomorphism is true by simple algebraic
manipulations:

\[{Hom(U\bigotimes V,\ W) \cong W\bigotimes(U\bigotimes V)^{*}
}{\cong W\bigotimes U^{*}\bigotimes V^{*}
}{\cong W\bigotimes V^{*}\bigotimes U^{*}
}{\cong {Hom}(V,W)\bigotimes U^{*}
}{\cong Hom(U,\ Hom(V,\ W))}\]

But what's the significance of this mysterious identity?

An issue arises with certain maps f:X⊕Y🡪W. Now, f \emph{could} be
linear; but it also might fail to be linear and still be
\emph{interesting}. One way this can happen is if it's
\textbf{bilinear}, meaning it's ``linear in X'' and ``linear in Y''
instead of ``linear in X⊕Y''. In concrete terms this means the following
are all true (there are really only two rules here, but you get one for
X and one for Y):

\[{f\left( \left( x\underline{a} \right)\bigoplus\underline{b} \right) = xf\left( \underline{a}\bigoplus\underline{b} \right)
}{f\left( \left( {\underline{a}}_{1} + {\underline{a}}_{2} \right)\bigoplus\underline{b} \right) = xf\left( {\underline{a}}_{1}\bigoplus\underline{b} + {\underline{a}}_{2}\bigoplus\underline{b} \right)\ 
}{f\left( \underline{a}\bigoplus\left( x\underline{b} \right) \right) = xf\left( \underline{a}\bigoplus\underline{b} \right)
}{f\left( \underline{a}\bigoplus\left( {\underline{b}}_{1} + {\underline{b}}_{2} \right) \right) = xf\left( \underline{a}\bigoplus{\underline{b}}_{1} + \underline{a}\bigoplus{\underline{b}}_{2} \right)\ }\]

Although ``bilinear'' sounds like it means ``twice as linear'', it's the
opposite -- a bilinear map is usually not as ``good'' as a linear map
because it's not a homomorphism. One important family of bilinear maps
are the \textbf{bilinear forms}, which are maps f:X⊕Y🡪\textbf{R} defined
by a matrix M such that f(x⊕y) = x\textsuperscript{T}My. All inner
products are examples of bilinear forms (you might want to check this
for yourself with the only inner product we've looked at on this course,
which is defined in 3.1.1).

Let us give the name B(X, Y) to the set of all bilinear maps X🡪Y. We
will now show that

\[B(U\bigoplus V,\ W) \cong Hom(U,\ Hom(V,\ W))\]

Before we do, let's make sure we see the larger perspective: the
right-hand-side of this matches the equation at the start of this
section, which means it also says

\[B(U\bigoplus V,\ W) \cong Hom(U\bigotimes V,\ W)\]

That is, it really says that the tensor product gives us a way to turn
bilinear maps into linear maps! This is so fundamentally useful that in
many treatments of the subject it's often taken as the starting-point
for \emph{defining} the tensor product.

But we had better convince ourselves that the previous identity is true,
i.e. that

\[B(U\bigoplus V,\ W) \cong Hom(U,\ Hom(V,\ W))\]

One way to see it is via the computer science technique known as
``currying'', which is a way to turn a function U⊕V🡪W into a function
that, given an element of U, gives you back a \emph{map} V🡪W.

Suppose you're trying to calculate the value of some specific vector,
i.e. f(\ul{u}⊕\ul{v}). The currying approach says this: OK, just tell me
which \ul{u} you've got, and I'll look up the appropriate map V🡪W. You
can then plug in your particular \ul{v} and you'll get the answer.

For example, suppose
f:\textbf{R}\textsuperscript{2}⊕\textbf{R}\textsuperscript{2}🡪\textbf{R}\textsuperscript{2}
has the rule

\[f\left( \begin{pmatrix}
	a \\
	b
\end{pmatrix}\bigoplus\begin{pmatrix}
	c \\
	d
\end{pmatrix} \right) = \begin{pmatrix}
	ad \\
	bc
\end{pmatrix}\]

This map is not linear, but it \emph{is} bilinear. Now suppose you fix
the first vector as

\[x = \begin{pmatrix}
	3 \\
	5
\end{pmatrix}\]

Now I can give you back the much simpler

\[f_{x}\begin{pmatrix}
	c \\
	d
\end{pmatrix} = \begin{pmatrix}
	3d \\
	5c
\end{pmatrix}\]

which is a homomorphism, i.e. an element of
Hom(\textbf{R}\textsuperscript{2}, \textbf{R}\textsuperscript{2}). But
you did have to tell me the value of x for which you wanted to calculate
the answer, and I had to look up f\textsubscript{x} based on the rule of
the map f itself. The place I go to look up that value of
f\textsubscript{x} is precisely a homomorphism
\textbf{R}\textsuperscript{2}🡪Hom(\textbf{R}\textsuperscript{2},
\textbf{R}\textsuperscript{2}).

The isomorphism with which we started with says that the tensor product
converts bilinear maps into linear ones. And it does this in a unique
way. This is a deep fact that extends far beyond the world of
finite-dimensional vector spaces, but in the next section we'll use it
to understand an elementary fact in linear algebra.

\subsubsection{9.2.2 Vectorization (*)}\label{vectorization}

Suppose that V has dimension n and W has dimension m. Then the dimension
of W⊗V* is just nm. Since these are all finite-dimensional real vector
spaces,

\[W\bigotimes V^{*} \cong \ \mathbb{R}^{nm}\]

Usually we would write objects in the space W⊗V* as matrices and objects
in \textbf{R}\textsuperscript{nm} as column vectors, but since the
spaces are isomorphic there must be a way to translate between the two
representations. The intuitively obvious way to do this is to ``stack up
the columns'' of the matrix representation of a linear operator, e.g. in
the case of n = 3 and m = 2, if we have

\[M = \begin{pmatrix}
	\begin{matrix}
		a & b & c
	\end{matrix} \\
	\begin{matrix}
		b & e & f
	\end{matrix}
\end{pmatrix}\]

a vectorization would be

\[vec\begin{pmatrix}
	\begin{matrix}
		a & b & c
	\end{matrix} \\
	\begin{matrix}
		b & e & f
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	a \\
	b \\
	\begin{matrix}
		c \\
		d \\
		\begin{matrix}
			e \\
			f
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

This is very clearly a linear transformation, so it's an element of
\textbf{R}\textsuperscript{nm}⊗(W⊗V*)*. This means that, in a chosen
basis, it can be represented by an nm-by-nm matrix. Note that there are
multiple perfectly good vectorizations: the order of the matrix entries
in the final column vector doesn't really matter as long as we're
consistent about it.

The rest of this section shows how a vectorization can be explicitly
constructed and goes through a simple example; feel free to skip to the
next section if time is short. The construction is rather long-winded
and it's not very obvious why it works until you get to the end but it
does show off some common ``matrix-juggling'' techniques in applied
linear algebra.

We will build the matrix we need from two ``blocks'' -- smaller matrices
that we put together to make the final one. The first block is the zero
matrix on W, i.e. the n-by-n matrix whose entries are all zero, which
will be written Z. The other is the identity matrix on W, written I. In
our running example n = 2 so we have

\[Z = \begin{pmatrix}
	0 & 0 \\
	0 & 0
\end{pmatrix}\ \ \ \ \ \ \ \ \ I = \begin{pmatrix}
	1 & 0 \\
	0 & 1
\end{pmatrix}\]

Now we will form a set of new matrices, written as B\textsubscript{1},
B\textsubscript{2} and so on, by stacking up m copies of Z but replacing
one of the copies of Z with a copy of I. In our example this gives us

\[B_{1} = \begin{pmatrix}
	I \\
	Z \\
	Z
\end{pmatrix} = \begin{pmatrix}
	\begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix}
\end{pmatrix}\ \ \ \ \ \ \ B_{2} = \begin{pmatrix}
	Z \\
	I \\
	Z
\end{pmatrix} = \begin{pmatrix}
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix}
\end{pmatrix}\ \ \ \ \ B_{3} = \begin{pmatrix}
	Z \\
	Z \\
	I
\end{pmatrix} = \begin{pmatrix}
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix}
\end{pmatrix}\]

Note that each of these is an element of
\textbf{R}\textsuperscript{nm}⊗\textbf{R}\textsuperscript{n}*, i.e. in
this case \textbf{R}\textsuperscript{6}⊗\textbf{R}\textsuperscript{2}*.
The next step is to apply each of these in turn to our starting matrix
M, creating a new matrix:

\[B_{1}M = \begin{pmatrix}
	\begin{matrix}
		1 & 0 \\
		0 & 1
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix} \\
	\begin{matrix}
		0 & 0 \\
		0 & 0
	\end{matrix}
\end{pmatrix}\ \begin{pmatrix}
	\begin{matrix}
		a & b & c
	\end{matrix} \\
	\begin{matrix}
		d & e & f
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	a & b & c \\
	d & e & f \\
	\begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

\[{B_{2}M = \begin{pmatrix}
		\begin{matrix}
			0 & 0 \\
			0 & 0
		\end{matrix} \\
		\begin{matrix}
			1 & 0 \\
			0 & 1
		\end{matrix} \\
		\begin{matrix}
			0 & 0 \\
			0 & 0
		\end{matrix}
	\end{pmatrix}\ \begin{pmatrix}
		\begin{matrix}
			a & b & c
		\end{matrix} \\
		\begin{matrix}
			b & e & f
		\end{matrix}
	\end{pmatrix} = \begin{pmatrix}
		0 & 0 & 0 \\
		0 & 0 & 0 \\
		\begin{matrix}
			a \\
			d \\
			\begin{matrix}
				0 \\
				0
			\end{matrix}
		\end{matrix} & \begin{matrix}
			b \\
			e \\
			\begin{matrix}
				0 \\
				0
			\end{matrix}
		\end{matrix} & \begin{matrix}
			c \\
			f \\
			\begin{matrix}
				0 \\
				0
			\end{matrix}
		\end{matrix}
	\end{pmatrix}
}{B_{3}M = \begin{pmatrix}
		\begin{matrix}
			0 & 0 \\
			0 & 0
		\end{matrix} \\
		\begin{matrix}
			0 & 0 \\
			0 & 0
		\end{matrix} \\
		\begin{matrix}
			1 & 0 \\
			0 & 1
		\end{matrix}
	\end{pmatrix}\ \begin{pmatrix}
		\begin{matrix}
			a & b & c
		\end{matrix} \\
		\begin{matrix}
			b & e & f
		\end{matrix}
	\end{pmatrix} = \begin{pmatrix}
		0 & 0 & 0 \\
		0 & 0 & 0 \\
		\begin{matrix}
			0 \\
			0 \\
			\begin{matrix}
				a \\
				d
			\end{matrix}
		\end{matrix} & \begin{matrix}
			0 \\
			0 \\
			\begin{matrix}
				b \\
				e
			\end{matrix}
		\end{matrix} & \begin{matrix}
			0 \\
			0 \\
			\begin{matrix}
				c \\
				f
			\end{matrix}
		\end{matrix}
\end{pmatrix}}\]

Next we apply each of these to the corresponding basis vector of V. In
this case we'll use e\textsubscript{1}, e\textsubscript{2} and
e\textsubscript{3} to represent the basis vectors:

\[B_{1}e_{1} = \begin{pmatrix}
	a & b & c \\
	d & e & f \\
	\begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix}
\end{pmatrix}\begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix} = \begin{pmatrix}
	a \\
	d \\
	\begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

\[B_{2}e_{2} = \begin{pmatrix}
	0 & 0 & 0 \\
	0 & 0 & 0 \\
	\begin{matrix}
		a \\
		d \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		b \\
		e \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix} & \begin{matrix}
		c \\
		f \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix}
\end{pmatrix}\begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix} = \begin{pmatrix}
	0 \\
	0 \\
	\begin{matrix}
		b \\
		e \\
		\begin{matrix}
			0 \\
			0
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

\[B_{3}e_{3} = \begin{pmatrix}
	0 & 0 & 0 \\
	0 & 0 & 0 \\
	\begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			a \\
			d
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			b \\
			e
		\end{matrix}
	\end{matrix} & \begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			c \\
			f
		\end{matrix}
	\end{matrix}
\end{pmatrix}\begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix} = \begin{pmatrix}
	0 \\
	0 \\
	\begin{matrix}
		0 \\
		0 \\
		\begin{matrix}
			c \\
			f
		\end{matrix}
	\end{matrix}
\end{pmatrix}\]

Adding these together gives us our vectorization of the matrix:

\[\sum_{i = 1}^{dim(V)}{B_{i}e_{i} = \begin{pmatrix}
		a \\
		d \\
		\begin{matrix}
			0 \\
			0 \\
			\begin{matrix}
				0 \\
				0
			\end{matrix}
		\end{matrix}
	\end{pmatrix} + \begin{pmatrix}
		0 \\
		0 \\
		\begin{matrix}
			b \\
			e \\
			\begin{matrix}
				0 \\
				0
			\end{matrix}
		\end{matrix}
	\end{pmatrix} + \begin{pmatrix}
		0 \\
		0 \\
		\begin{matrix}
			0 \\
			0 \\
			\begin{matrix}
				c \\
				f
			\end{matrix}
		\end{matrix}
	\end{pmatrix} = \begin{pmatrix}
		a \\
		d \\
		\begin{matrix}
			b \\
			e \\
			\begin{matrix}
				c \\
				f
			\end{matrix}
		\end{matrix}
\end{pmatrix}}\]

We wouldn't want to have to do this procedure by hand too many times,
but it's important for a few reasons. First, the existence of the
procedure, and its obvious correctness, convinces us that such a thing
is possible, so whenever we see a matrix M we know we could turn it into
a vector with vec(M); we don't just have an abstract isomorphism telling
us it must be so. Second, having a procedure like this makes it easy to
programme a computer to do vectorize a matrix, although depending on the
progamming language there might be more efficient ways to do it. Third,
the steps in the procedure are all obviously linear, so we know that the
map M🡪vec(M) is a linear map, i.e. an element of
\textbf{R}\textsuperscript{nm}⊗(W⊗V*)* -\/- although this is fairly
obvious from the start, it's nice to know we can do it explicitly using
only the operations that belong to linear algebra.

It would be a straightforward but fiddly exercise to rewrite this
algorithm for ``covectorization'' (not a standard term), which turns a
matrix into a row vector instead of a column vector. It might be worth
the trouble if you want to be certain you understand all the steps of
this process.

\subsection{9.3 The Trace of a Linear
	Operator}\label{the-trace-of-a-linear-operator}

The trace of a matrix M, written tr(M), is the sum of the entries on its
main diagonal. For example,

\[tr\begin{pmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9
\end{pmatrix} = 1 + 5 + 9 = 15\]

Since the trace only depends on the values on the main diagonal, it is
clear that a matrix and its adjoint must have the same trace, i.e.

\[tr(M) = tr(M^{*})\]

Surprisingly, the trace does not depend on the basis -- it is a properly
geometric property. For example, let's change the basis of the matrix
above. The matrix I chose for the change of basis is pretty arbitrary --
don't forget that a matrix is both contravariant and covariant, so we
must transform it both ways when we change basis:

\[\begin{pmatrix}
	1 & 0 & - 1 \\
	2 & 1 & 0 \\
	0 & 1 & 0
\end{pmatrix}\begin{pmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9
\end{pmatrix}\begin{pmatrix}
	1 & 0 & - 1 \\
	2 & 1 & 0 \\
	0 & 1 & 0
\end{pmatrix}^{- 1} = \begin{pmatrix}
	6 & - 6 & 0 \\
	- 12 & 9 & 0 \\
	- 6 & 5 & 0
\end{pmatrix}\]

The trace is 6 + 9 + 0, which again is 15. It seems like a miracle or a
magic trick. Those two matrices could hardly be more different; if you
view the trace as just ``adding up the diagonal entries of a matrix'',
it's very surprising that the value you get only depends on the linear
operator, not the matrix that's representing it! In fact, as usual, this
matrix-centric view is the wrong way to look at things precisely because
it puts focus on the matrix, which is not geometrical; it depends on the
basis.

(If time is short or you need to review some previous reading, feel free
to stop here.)

Consider the following facts, which you're already familiar with --
these breadcrumbs that will lead us along the trail to a better
definition of the trace:

\begin{itemize}
	\item
	Fact 1: Hom(V, V) is isomorphic to V⊗V*
	\item
	Fact 2: V⊗V* is isomorphic to V*⊗V
	\item
	Fact 3: If \ul{v} is a vector and \ul{w}* is a covector, \ul{w}*\ul{v}
	is a scalar
\end{itemize}

Now suppose you have a square matrix M whose size matches the dimension
of V. We usually take this to represent a linear operator, i.e. an
element of Hom(V, V). But Fact 1 above indicates that it's also an
element of V⊗V*, and when we introduced linear operators on this course
we explicitly took this approach. You know how to write out a linear
operator as an element of the V⊗V*. Let's do an example:

\[{M = \begin{pmatrix}
		3 & 0 & 1 \\
		- 1 & 2 & 2 \\
		1 & 1 & 0
	\end{pmatrix}
}{= 3e_{1}\bigotimes e_{1}^{*} + e_{3}\bigotimes e_{1}^{*} - e_{1}\bigotimes e_{2}^{*} + 2e_{2}\bigotimes e_{2}^{*} + 2e_{3}\bigotimes e_{2}^{*} + e_{1}\bigotimes e_{3}^{*} + e_{2}\bigotimes e_{3}^{*}}\]

Fact 2 above tells us we can rewrite this as an element of V*⊗V; the
easiest way to do this is simply to transpose (swap over) the vectors
and covectors:

\[= 3e_{1}^{*}\bigotimes e_{1} + e_{1}^{*}\bigotimes e_{3} - e_{2}^{*}\bigotimes e_{1} + 2e_{2}^{*}\bigotimes e_{2} + 2e_{2}^{*}\bigotimes e_{3} + e_{3}^{*}\bigotimes e_{1} + e_{3}^{*}\bigotimes e_{2}\]

This is obviously an isomorphism -- many others are possible but the
simplest way is usually the best.

Fact 3 says we can evaluate a covector on a vector. One way to do this
is to ``erase the tensor product symbols'' and write, for example,

\[e_{1}^{*}\bigotimes e_{1} = e_{1}^{*}e_{1}\]

This will always come out as equal to 1 if the two basis vectors match
(as they do in this example) and 0 otherwise. Again, there are many ways
to make this work but this way seems simplest.

If we do this to the transposed tensor representation of M, we get the
trace:

\[3e_{1}^{*}e_{1} + e_{1}^{*}e_{3} - e_{2}^{*}e_{1} + 2e_{2}^{*}e_{2} + 2e_{2}^{*}e_{3} + e_{3}^{*}e_{1} + e_{3}^{*}e_{2} = 3 + 2 + 0 = 5\]

The trace is geometric because it's the evaluation of the special linear
functional you get when you evaluate the elements of Hom(V, V) in this
way. If you change the basis, you do not change the linear functional,
only its basis representation. But the representation of this linear
function and the matrix itself change in a way that keeps everything
geometrically consistent.

We'll conclude this section with some miscellaneous facts about the
trace.

If you like doing a lot of algebra you might enjoy working out an
example to demonstrate the useful fact that, like the determinant, when
you compose linear operators the trace doesn't care which order you do
it in:

\[tr(MN) = tr(NM)\]

In fact the trace is the \emph{only} linear functional on Hom(V, V) that
has this property and also satisfies tr(I) = n where I is the identity
operator and n is the dimension of V. You could take this to be the
definition of the trace if you so desired.

We saw last time (8.1.2) that the determinant of a linear operator is
the product of its eigenvalues. Determinants and eigenvalues are, of
course, geometric; they don't change when the basis changes. The trace,
as it turns out, is always the sum of the eigenvalues. This may seem
surprising since eigenvalues can be complex numbers but since they
always come in conjugate pairs, the imaginary parts cancel out (see
8.1.3.3).

Note, though, that while the trace is the sum of the diagonal entries in
the matrix and it's also the sum of the eigenvalues, it does not follow
that the eigenvalues \emph{are} the diagonal entries! In fact this is
rarely true. We'll pick up this theme next time.

Finally, suppose that M is a linear operator and MM = M, so that
additional convolutions of M don't do anything. This must mean that all
the vectors in the image of M are fixed by it, i.e. if \ul{v} is in the
range of M than M\ul{v} = \ul{v}. Such an operator is said to be a
\textbf{projection} and tr(M) is equal to the rank of M, i.e. the
dimension of the image.

\chapter{Applications of Eigenvalues}

This session describes two applications of the theory we've developed
over the past few sessions.

\textbf{Section 10.1} describes a very classical procedure for
expressing a matrix in a basis in which it has a particularly simple
form. It's used as a step in a great many algorithms and you'll find it
in every linear algebra textbook. Section 10.1.2 deals with a related
detail and provides a good workout for inner products.

\textbf{Section 10.2} covers the more modern technique known as
principal component analysis. This will be of particular interest to
those looking towards data science. The section contains a long
calculation involving many steps; if you're mostly interested in the
theory, feel free to skim over the grisly details.

If time is short, focus on 10.1.1. Section 10.2 won't be needed in the
future sessions although a skim through it should give you some
familiarity with what Principal Component Analysis is about, which will
be very helpful next time.

\subsection{10.1 Eigendecomposition}\label{eigendecomposition}

\subsubsection{10.1.1 Diagonalization of a Linear
	Operator}\label{diagonalization-of-a-linear-operator}

We saw earlier (section 5.3.4) that two matrices A and B are similar if
there's a change of basis M such that

\[M^{- 1}AM = B\]

Thus A and B represent the same linear operator in different choices of
basis. It's not surprising, then, that the trace must be the same for A
and B. What's more, remember that the trace is equal to the sum of the
matrix's eigenvalues.

So for a given matrix, it may be that we can find a change of basis that
eliminates all but the diagonal elements, and that these are literally
just its eigenvalues. This would certainly represent it in a
particularly simple and explicit form. This can be done very easily once
you know a set of eigenvectors for the matrix. The only difficulty is
working out the change of basis M that represents the linear operator
you have in this simplified form.

Let's go back to an example that we've used before -- a matrix of a
linear operator with two distinct, real eigenvalues. I've chosen a pair
of eigenvectors, which I've labelled \ul{p} and \ul{q}:

\[A = \begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\ \ \ \ \ \ \ \ \ \underline{p} = \begin{pmatrix}
	1 \\
	1
\end{pmatrix}\ \ \ \ \ \ \ \ \underline{q} = \begin{pmatrix}
	- 1 \\
	1
\end{pmatrix}\]

The trick is to write A in the basis \{\ul{p}, \ul{q}\} using the method
from section 5.3.2. First form the matrix that has \ul{p} and \ul{q} as
columns:

\[B = \begin{pmatrix}
	1 & - 1 \\
	1 & 1
\end{pmatrix}\]

Then find its inverse:

\[B^{- 1} = \begin{pmatrix}
	0.5 & 0.5 \\
	- 0.5 & 0.5
\end{pmatrix}\]

These enable us to represent A in the new basis:

\[B^{- 1}AB = \begin{pmatrix}
	0.5 & 0.5 \\
	- 0.5 & 0.5
\end{pmatrix}\begin{pmatrix}
	1 & 2 \\
	2 & 1
\end{pmatrix}\begin{pmatrix}
	1 & - 1 \\
	1 & 1
\end{pmatrix} = \begin{pmatrix}
	0.5 & 0.5 \\
	- 0.5 & 0.5
\end{pmatrix}\begin{pmatrix}
	3 & 1 \\
	3 & - 1
\end{pmatrix} = \begin{pmatrix}
	3 & 0 \\
	0 & - 1
\end{pmatrix}\]

This is a similar matrix to A that's also diagonal, and notice that the
eigenvalues are the diagonal entries. This makes it a bit easier to see
what the linear operator does; in the new basis given by the
eigenvectors, it just scales every vector by a factor of 3 in the
direction of the first basis vector and a factor of -1 in the direction
of the second. And, crucially, the matrix B that we found shows us how
to get into that basis and out of it again, since

\[M = B^{- 1}AB\ \ \ \ \ \ \ \ \ is\ equivalent\ to\ \ \ \ \ \ \ A = \ {BMB}^{- 1}\]

Diagonal matrices are also important in computer algorithms when we need
to apply a linear operator many times, since it's very easy to compose
them:

\[\begin{pmatrix}
	3 & 0 \\
	0 & - 1
\end{pmatrix}^{2} = \begin{pmatrix}
	3 & 0 \\
	0 & - 1
\end{pmatrix}\begin{pmatrix}
	3 & 0 \\
	0 & - 1
\end{pmatrix} = \begin{pmatrix}
	3^{2} & 0 \\
	0 & {( - 1)}^{2}
\end{pmatrix} = \begin{pmatrix}
	9 & 0 \\
	0 & 1
\end{pmatrix}\]

This is far fewer calculations that we usually have to do when composing
linear operators, especially if the space is high-dimensional, so it can
make the difference between a calculation taking a few seconds rather
than running for so long that it isn't feasible.

When the power is larger than 2, as often happens in systems involving
convolution, the benefit is even greater. For example,

\[\left( B^{- 1}MB \right)^{4} = B^{- 1}MBB^{- 1}MBB^{- 1}MBB^{- 1}MB = B^{- 1}MMMMB = B^{- 1}M^{4}B\]

because all the BB\textsuperscript{-1} pairs cancel out. Being able to
do this so efficiently becomes more important as number of convolutions
grows larger.

Unfortunately, not every operator can be diagonalized: it only works if
it has real eigenvalues. In a rare example of sensible mathematical
jargon, such operators are said to be \textbf{diagonalizable}. If all
the eigenvalues are distinct then the operator is \emph{guaranteed} to
be diagonalizable, but this is not necessary: some matrices have
repeated eigenvalues but are still diagonalizable. However, for an
operator to be diagonalizable is equivalent to its not being defective
(in the sense of 8.1.3.2): non-defective matrices can be diagonalized,
defective ones can't.

Going a bit beyond what we can do on this course, it's worth knowing
that defective matrices really are exceptions to the general rule: any
defective matrix can be ``perturbed'' into a non-defective one by a very
small transformation (i.e. an operator that's almost the identity). In
practical situations the perturbation can be made small enough that it
doesn't affect our final results, so that we can approximate any
defective operator by a diagonalizable one, then diagonalize it.

\subsubsection{10.1.2 Normal Matrices (*)}\label{normal-matrices}

A linear operator A is said to be \textbf{normal} if A*A = AA*, that is
if it ``commutes with its adjoint''. Every normal linear operator is
diagonalizable, but something stronger is true: we can write A =
B\textsuperscript{-1}MB with B a unitary matrix (see section 9.1.3),
meaning we can also express it as A = B*MB. Whenever a matrix and Y have
this relationship we say they are \textbf{unitary similar}, which is a
stronger condition than just being similar. Remember that, geometrically
speaking, a unitary matrix represents an orthogonal transformation --
one that doesn't distort angles.

A normal linear operator is so-called because when you diagonalize it
you get an orthogonal basis of eigenvectors. ``Orthogonal'' only means
something in the presence of an inner product on the vector space it's
acting on, so what's surprising is that the nature of the inner product
it doesn't actually matter. I'll use this section to very carefully
prove this statement -- don't get bogged down in the details here if you
find it a bit dry.

We'll begin by deepening out understanding of inner products. All the
way back in 3.1.1 we introduced the inner product as

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle = {\underline{v}}_{1}^{T}{\underline{v}}_{2}\]

We said that in an orthonormal basis this becomes the ``dot product''
that is used to recover some basic geometry from the abstract definition
of a vector space. But this is not the only possible inner product. In
fact we can use a matrix A to define an inner product by

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{A} = {\underline{v}}_{1}^{T}{M\underline{v}}_{2}\]

This is called a \textbf{bilinear form}; the theory of these objects is
rather rich and interesting. However, not all operators can be used in
this way: the resulting bilinear form must have two properties if it's
to count as an inner product:

\begin{itemize}
	\item
	It must be \textbf{positive definite}, which means
	\textless­\ul{v}\textsubscript{1},
	\ul{v}\textsubscript{2}\textgreater{} can't be negative, and can only
	be zero if \ul{v}\textsubscript{1}= \ul{v}\textsubscript{2}.
	\item
	It must be \textbf{symmetric}, meaning
	\textless­\ul{v}\textsubscript{1},
	\ul{v}\textsubscript{2}\textgreater{} =
	\textless­\ul{v}\textsubscript{2}, \ul{v}\textsubscript{1}\textgreater.
\end{itemize}

Note that if you have something other than a finite-dimensional real
vector space, you may need a different definition from this. But it'll
be good enough for us.

Note that

\[{\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M} = {\underline{v}}_{1}^{T}{M\underline{v}}_{2}
}{= {\underline{v}}_{2}^{T}{M^{*}\underline{v}}_{1}
}{= \left\langle {\underline{v}}_{2},\ {\underline{v}}_{1} \right\rangle_{M^{*}}
}{= \left\langle {\underline{v}}_{2},\ {\underline{v}}_{1} \right\rangle_{M}}\]

Since this must be true for all pairs of vectors, we must have M = M*,
i.e. M must be Hermitian (see 9.1.3). The inner product we've always
been working with has M equal to the identity operator, which is of
course Hermitian.

One last thing to notice that will be important in a moment is the
following (see 9.1.2):

\[{\left\langle {\underline{v}}_{1},\ A{\underline{v}}_{2} \right\rangle_{M} = {\underline{v}}_{1}^{T}{MA\underline{v}}_{2}
}{= {\underline{v}}_{1}^{T}{(MA\underline{v}}_{2})
}{= \left( {\underline{v}}_{1}^{T}(MA)^{*} \right){\underline{v}}_{2}
}{= {\underline{v}}_{1}^{T}A^{*}M^{*}{\underline{v}}_{2}
}{= {\underline{v}}_{1}^{T}A^{*}M{\underline{v}}_{2}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (because\ M\ is\ Hermitian)
}{= \left\langle A^{*}{\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}\ }\]

OK, back to the proof -- we want to show that the eigenvectors of A are
all orthogonal. We have an inner product
\(\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}\).
If u\textsubscript{1} and u\textsubscript{2} are eigenvalues of A then
there are eigenvectors \ul{v}\textsubscript{1} and
\ul{v}\textsubscript{2} such that

\[A{\underline{v}}_{1} = u_{1}{\underline{v}}_{1}\ \ \ \ \ \ and\ \ \ \ \ \ A{\underline{v}}_{2} = u_{2}{\underline{v}}_{2}\]

We know that u\textsubscript{1} and u\textsubscript{2} are not equal,
because A normal and normal operators are diagonalizable, which means
they have distinct eigenvalues. This is a very important fact that will
come back in a moment: u\textsubscript{1} and u\textsubscript{2}
\emph{cannot} be equal!

Since we have an inner product, let's bring that into the mix. Here's
what happens when we introduce A into an inner product:

\[{\left\langle {\underline{v}}_{1},\ A{\underline{v}}_{2} \right\rangle_{M} = \left\langle {\underline{v}}_{1},\ u_{2}{\underline{v}}_{2} \right\rangle_{M}
}{= {\underline{v}}_{1}^{T}{M(u_{2}\underline{v}}_{2})
}{= u_{2}\left( {\underline{v}}_{1}^{T}M{\underline{v}}_{2} \right)
}{= u_{2}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}
}\]Now, as we saw a few moments ago,

\[\left\langle {\underline{v}}_{1},\ A{\underline{v}}_{2} \right\rangle_{M} = \left\langle A^{*}{\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}\]

What can we say about the quantity on the right? Recall that
\ul{v}\textsubscript{1} is an eigenvalue of A, i.e. a solution to

\[\det(A - xI) = 0\]

By one of the properties at the end of 9.1.3, the adjoint of a sum of
matrices is the sum of the adjoints, so

\[\det(A - xI)^{*} = \det\left( A^{*} - xI^{*} \right)\]

Since xI is a diagonal matrix, xI* = xI and we have.

\[\det(A - xI)^{*} = \det\left( A^{*} - xI \right)\]

Therefore, the values of x that satisfy the eigenvalue equation for A
also satisfy the eigenvalue equation for A* -\/- that is, A and A* have
the same eigenvalues and eigenvectors. It follows that

\[\left\langle A^{*}{\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M} = \left\langle u_{1}{\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}{= u_{1}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle}_{M}\]

So we have (from the bottom of the previous page)

\[\left\langle {\underline{v}}_{1},\ A{\underline{v}}_{2} \right\rangle_{M} = u_{2}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}
\]and from the argument just presented,

\[\left\langle {\underline{v}}_{1},\ A{\underline{v}}_{2} \right\rangle_{M} = u_{1}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}\]

It is a fundamental principle that when two things are each equal to the
same thing, they must be equal to each other. So these two facts imply
that

\[u_{2}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M} = u_{1}\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M}\]

There are only two ways this can be true: if
u\textsubscript{1­}=u­\textsubscript{2}, which \emph{can't} be (the
eigenvalues must be distinct, remember), \emph{or} if

\[\left\langle {\underline{v}}_{1},\ {\underline{v}}_{2} \right\rangle_{M} = 0\]

When you've eliminated the impossible, what remains must be the truth!
The inner product must be equal to zero, and this means the two vectors
are orthogonal by the very definition of the word ``orthogonal''.

\subsection{10.2 Principal Component Analysis
	(*)}\label{principal-component-analysis}

\subsubsection{10.2.1 The Idea}\label{the-idea}

Principal Component Analysis (PCA) is a technique in data science that
can reduce the dimension of a matrix. Picture a table of data, with rows
(called ``observations'') and columns (called ``features''). For
example, suppose we have some medical data: each observation is a
patient, and the features are things like age, height, weight and so on.
We may have a very large number of features that might not all be
useful. In data-intensive applications such as machine learning, you pay
a high price for all those features -- remember how laborious linear
algebra calculations become as the dimensions involved get larger!

Sometimes you can get rid of features right away. If one of the features
is BMI, for instance, we know that BMI is calculated from age, height,
weight and gender; so this feature is completely \textbf{redundant} and
the data scientist can safely delete it. If we ever need to know
someone's BMI, we can calculate it from other data we have. We don't
need to store it as a separate feature.

Another feature might be the patient's short-range vision. We know this
tends to degrade with age, so a plot of the vision feature vs age will
probably show quite a strong \textbf{correlation}: as age increases,
vision gets poorer. However, unlike the BMI, we can't just delete this
feature without losing something: your short-range vision can't be
completely predicted from your age. We would like to somehow
``compress'' these two features into one that captures the variation
between them without the redundant information they share.

This second step is roughly what PCA does. It reimagines the variation
your data as a transformation of a high-dimensional space. Then it
reduces the dimension of the data by ``projecting'' it onto the
eigenlines where the transformation has the largest effect. These are
the most significant sources of variability in your data.

Note that PCA does not simply delete features: it creates a whole new,
smaller set of features that are linear combinations of the original
ones. This often makes them a bit opaque for humans to interpret but
very convenient for algorithmic processes. It also means that although
the original data can be reconstructed from the transformed version (as
we will see), the result is usually highly imperfect.

\subsubsection{10.2.2 Preparing to Calculate the
	PCA}\label{preparing-to-calculate-the-pca}

As usual, we'll explain the process by following an example. Our data
will be the following numbers (we won't worry about where they came from
or what they mean):

\begin{longtable}[]{@{}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2622}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2634}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2159}}
		>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2585}}@{}}
	\toprule\noalign{}
	\begin{minipage}[b]{\linewidth}\raggedright
		12
	\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
		8
	\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
		7
	\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
		19
	\end{minipage} \\
	\midrule\noalign{}
	\endhead
	\bottomrule\noalign{}
	\endlastfoot
	17 & 21 & 2 & 18 \\
	20 & 19 & 3 & 22 \\
\end{longtable}

I'll use o for the number of observations and f for the number of
features. In this case we have 0 = 3 observations with v = 4 features
each. If we wanted to we could express each observation as a covector in
\textbf{R}\textsuperscript{4}*, with each coordinate representing one of
the features. (They're covectors not only because they're rows in the
table, but because it makes everything later consistent as well). Thus
every point in \textbf{R}\textsuperscript{4}* becomes a possible
observation, and as we gather data we're actually gathering points in
this space. Of course, spaces of real-world data often have much higher
dimensions than this.

Since we have 3 covectors, we can express them as a matrix by simply
tensoring them with three basis vectors:

\[\begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix}\bigotimes\begin{pmatrix}
	12 & 8 & \begin{matrix}
		7 & 19
	\end{matrix}
\end{pmatrix} + \begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix}\bigotimes\begin{pmatrix}
	17 & 21 & \begin{matrix}
		2 & 18
	\end{matrix}
\end{pmatrix} + \begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix}\bigotimes\begin{pmatrix}
	20 & 19 & \begin{matrix}
		3 & 22
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	12 & 8 & \begin{matrix}
		7 & 19
	\end{matrix} \\
	17 & 21 & \begin{matrix}
		2 & 18
	\end{matrix} \\
	20 & 19 & \begin{matrix}
		3 & 22
	\end{matrix}
\end{pmatrix}\]

This matrix doesn't mean much as a homomorphism; it's better to consider
it as a vector in the 12-dimensional space
\textbf{R}\textsuperscript{3}⊗\textbf{R}\textsuperscript{4}*; the goal
is to transfer this vector into a lower-dimensional space.

We begin by extracting from our observations a matrix of
``covariances'', which express how each feature varies in relation to
each other feature. This calculation goes in a few steps. First we
\textbf{standardize} the data. This means that we calculate the mean and
standard deviation of each column and then, for each value, subtract the
mean of its column and divide the result by the standard deviation. The
result looks like this:

\[\begin{pmatrix}
	- 1.52 & - 1.62 & \begin{matrix}
		1.6 & - 0.45
	\end{matrix} \\
	0.23 & 1.01 & \begin{matrix}
		- 1.07 & - 1.13
	\end{matrix} \\
	1.28 & 0.61 & \begin{matrix}
		- 0.53 & 1.59
	\end{matrix}
\end{pmatrix}\]

This ensures that every feature has a mean of zero and standard
deviation of 1.

We now calculate the covariance for each pair of features. This is done
by doing down the two columns, dividing the value in the first column by
the value in the second and adding as we go. We then divide the end
result by the number of observations (rows). The covariance matrix is an
f-by-f matrix where each entry is a covariance. In our case we get (to 2
decimal places):

\[C = \begin{pmatrix}
	\begin{matrix}
		1 & 0.87 \\
		0.87 & 1
	\end{matrix} & \begin{matrix}
		- 0.84 & 0.61 \\
		- 1 & 0.14
	\end{matrix} \\
	\begin{matrix}
		- 0.84 & - 1 \\
		0.61 & 0.14
	\end{matrix} & \begin{matrix}
		1 & - 0.09 \\
		- 0.09 & 1
	\end{matrix}
\end{pmatrix}\]

Every covariance matrix has the same symmetry as this one, representing
a Hermitian operator on \textbf{R}\textsuperscript{4} (note the
symmetry). You can think of variance as the amount of information in
your data: data that doesn't vary much isn't very information-rich.
Alternatively, and more relevant to this course, you can see it as
expressing, a bit indirectly, how to transform a cloud of random data
into a sample that exhibits the same variances as the data you actually
have. Specifically, since C is Hermitian it has an eigendecomposition

\[C = M^{- 1}AM\]

Since A is diagonal, we can take its ``square root'' by simply square
rooting the diagonal entries -- for example, in two dimensions

\[\sqrt{\begin{pmatrix}
		a & 0 \\
		0 & b
\end{pmatrix}} = \begin{pmatrix}
	\sqrt{a} & 0 \\
	0 & \sqrt{b}
\end{pmatrix}\ \ \ \ \ \ \ because\ \ \ \ \ \ \ \begin{pmatrix}
	\sqrt{a} & 0 \\
	0 & \sqrt{b}
\end{pmatrix}\begin{pmatrix}
	\sqrt{a} & 0 \\
	0 & \sqrt{b}
\end{pmatrix} = \begin{pmatrix}
	a & 0 \\
	0 & b
\end{pmatrix}\]

So we can write

\[C = M^{- 1}\sqrt{A}\sqrt{A}M\]

Now, suppose we generate some random data and want to make it look like
our original data. It turns out that if we apply the transformation
\(M^{- 1}\sqrt{A}\) to it, it will transform the random data into data
that's distributed like our real data was.

\subsubsection{10.2.3 The PCA Calculation}\label{the-pca-calculation}

Now the PCA begins in earnest! We will find eigenvalues and eigenvectors
of this operator. This was done with Wolfram Alpha. I've again rounded
everything to 2 decimal places except the eigenvalues have three because
two are very small. I've also written the vectors as row vectors to save
space:

\[{e_{1} = 2.929\ \ \ \ \ \ \ \ {\underline{v}}_{1} = \begin{pmatrix}
		2.31 & 2.27 & \begin{matrix}
			- 2.23 & 1
		\end{matrix}
	\end{pmatrix}^{T}
}{e_{2} = 1.07\ \ \ \ \ \ \ \ {\underline{v}}_{2} = \begin{pmatrix}
		0.24 & - 0.32 & \begin{matrix}
			0.37 & 1
		\end{matrix}
	\end{pmatrix}^{T}
}{e_{3} = 0.003\ \ \ \ \ \ \ \ {\underline{v}}_{3} = \begin{pmatrix}
		- 1.85 & - 0.06 & \begin{matrix}
			- 1.52 & 1
		\end{matrix}
	\end{pmatrix}^{T}
}{e_{4} = - 0.002\ \ \ \ \ \ \ \ {\underline{v}}_{4} = \begin{pmatrix}
		- 2.35 & 5.19 & \begin{matrix}
			3.3 & 1
		\end{matrix}
	\end{pmatrix}^{T}\ }\]

If we start with random values, the eigenlines corresponding to the
biggest eigenvalues will be the ones that have be biggest effect in
transforming it to look like our data. Another way to say this is that
those components of the transformation are the ones that are most
characteristic of the variances in the data we have.

So, the next step is to construct the desired number of features in a
way that maximizes the information density. We do this by choosing
features that correspond to some of the eigenlines of C. Which
eigenvalues shall we choose to keep? In general, you choose the biggest
(in terms of absolute value), since they have the largest effect, but
how many? Sometimes the answer is given by the context, but sometimes
it's a choice. If so, you can calculate a number called the
\textbf{variance explained ratio} (VER), which simply tells you how much
a single eigenvalue contributes to the overall variance (expressed by
the sum of the eigenvalues):

\[VER_{i} = \frac{e_{i}}{\sum_{j = 1}^{d}e_{j}}\]

In this case we have two extremely small eigenvalues and two larger
ones, and the VERs are unsurprising:

\[{e_{1} = 2.929\ \ \ \ \ \ \ \ VER_{1} = 0.73225
}{e_{2} = 1.07\ \ \ \ \ \ \ \ VER_{2} = 0.2625
}{e_{3} = 0.003\ \ \ \ \ \ \ \ VER_{3} = 0.00075
}{e_{4} = - 0.002\ \ \ \ \ \ \ \ VER_{4} = - 0.0005}\]

It's pretty clear that e\textsubscript{1} and e\textsubscript{2} explain
a lot of the variance in the data, whereas e\textsubscript{3} and
e\textsubscript{4} are contributing very little. So we'll create a
homomorphism \textbf{R}\textsuperscript{4}🡪\textbf{R}\textsuperscript{2}
that projects the basis vectors of \textbf{R}\textsuperscript{4} (i.e.
our original features) onto the eigenvectors ­\ul{v}\textsubscript{1} and
\ul{v}\textsubscript{2}. However, a small bit of care is needed. The
vectors in our data are rows, not columns, so really our data lives in
\textbf{R}\textsuperscript{4}* and we want to transform it into rows of
two columns each, so the vectors will be in
\textbf{R}\textsuperscript{2}*. Thus we want a homomorphism
\textbf{R}\textsuperscript{4}*🡪\textbf{R}\textsuperscript{2}*, i.e. an
element of \textbf{R}\textsuperscript{2}*⊗\textbf{R}\textsuperscript{4}.

The matrix is easy to construct: simply tensor the eigenvectors we want
to be our new basis vectors with the basis vectors of
\textbf{R}\textsuperscript{2}*:

\[\begin{pmatrix}
	2.31 \\
	2.27 \\
	\begin{matrix}
		- 2.23 \\
		1
	\end{matrix}
\end{pmatrix}\bigotimes\begin{pmatrix}
	1 & 0
\end{pmatrix} + \begin{pmatrix}
	0.24 \\
	- 0.32 \\
	\begin{matrix}
		0.37 \\
		1
	\end{matrix}
\end{pmatrix}\bigotimes\begin{pmatrix}
	0 & 1
\end{pmatrix} = \begin{pmatrix}
	2.31 & 0.24 \\
	2.27 & - 0.32 \\
	\begin{matrix}
		- 2.23 \\
		1
	\end{matrix} & \begin{matrix}
		0.37 \\
		1
	\end{matrix}
\end{pmatrix}\]

This matrix will transform our data into the new set of features,
completing the PCA. The transformation matrix is applied on the right
because, again, we're interested in transforming the rows of the
original data as if they were covectors:

\[\begin{pmatrix}
	- 1.52 & - 1.62 & \begin{matrix}
		1.6 & - 0.45
	\end{matrix} \\
	0.23 & 1.01 & \begin{matrix}
		- 1.07 & - 1.13
	\end{matrix} \\
	1.28 & 0.61 & \begin{matrix}
		- 0.53 & 1.59
	\end{matrix}
\end{pmatrix}\begin{pmatrix}
	2.31 & 0.24 \\
	2.27 & - 0.32 \\
	\begin{matrix}
		- 2.23 \\
		1
	\end{matrix} & \begin{matrix}
		0.37 \\
		1
	\end{matrix}
\end{pmatrix} = \begin{pmatrix}
	- 11.2066 & 0.2956 \\
	4.0801 & - 1.7939 \\
	7.1134 & 1.5059
\end{pmatrix}\]

The data is now represented by a vector in
\textbf{R}\textsuperscript{3}⊗\textbf{R}\textsuperscript{2}*, with is
six-dimensional compared with the 12 dimensions we started with. The
overall amount of data has been halved, but the VERs tell us that these
features explain more than 99\% of the variance in our original data.

The problem this leaves us with is that we have a set of features that
don't mean much to us, the researchers or the audience we want to
communicate with. After working on this data, we will need to transform
it back into its original form. Of course, this will always be a
reconstruction -- we can't expect to recover all of our data, since we
transformed it with a matrix whose rank is less than our original
dimension, meaning some data was lost. There is no general way to encode
12 values with 6 without losing information! We will see how to
(partially) accomplish this using a technique we'll discuss next time.

\chapter{Singular Value Decomposition}

This session describes a procedure similar to eigendecomposition that
can be applied to non-square matrices. It has many applications in areas
of technology that use large amounts of data.

\textbf{Section 11.1} describes singular values, which generalize
eigenvalues to all homomorphisms (not just linear operators).

\textbf{Section 11.2} shows how to calculate the Singular Value
Decomposition in detail.

\textbf{Section 11.3} introduces the idea of separable homomorphisms and
their importance in certain applied fields.

\textbf{Section 11.4} looks at transferring the idea of the inverse of a
linear operator to homomorphisms in general.

If time is short, those with an interest in data science should focus on
11.1 and 11.2. Those whose are mostly here for the theory may prefer to
focus on 11.1, 11.3.1 and 11.4.

\subsection{11.1 Singular Values}\label{singular-values}

The Singular Value Decomposition (SVD) is a generalization of the
eigendecomposition to vector space homomorphisms that are not linear
operators, i.e. elements of W⊗V* where the dimensions of W and V are
different. The matrices representing these homomorphisms are rectangular
(see section 6.2). As a reminder, here's an example of a homomorphism
from V = \textbf{R}\textsuperscript{3} to W
=\textbf{R}\textsuperscript{2} acting on a vector in the domain:

\[\begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix}\begin{pmatrix}
	x \\
	y \\
	z
\end{pmatrix} = \begin{pmatrix}
	x + 2z \\
	- x + 3y - 2z
\end{pmatrix}\]

Note that when this homomorphism acts on the dual space, it does so on
the dual space of its codomain, not its domain:

\[\begin{pmatrix}
	x & y
\end{pmatrix}\begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix} = \begin{pmatrix}
	x - y & 3y & 2x - 2y
\end{pmatrix}\]

This means the adjoint can still be created by reflecting the matrix
along its main diagonal, but now the main diagonal runs from the top
left corner to wherever it hits an ``edge'' of the matrix:

\[\begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix}^{*} = \begin{pmatrix}
	1 & - 1 \\
	0 & 3 \\
	2 & - 2
\end{pmatrix}\]

This matrix represents a linear operator that acts on a covector in the
same way the original acted on a vector and \emph{vice versa}, which is
what we expect from the adjoint -- compare these with the examples given
above:

\[\begin{pmatrix}
	1 & - 1 \\
	0 & 3 \\
	2 & - 2
\end{pmatrix}\begin{pmatrix}
	x \\
	y
\end{pmatrix} = \begin{pmatrix}
	x - y \\
	3y \\
	2x - 2y
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \begin{pmatrix}
	x & y & z
\end{pmatrix}\begin{pmatrix}
	1 & - 1 \\
	0 & 3 \\
	2 & - 2
\end{pmatrix} = \begin{pmatrix}
	x + 2z & - x + 3y - 2z
\end{pmatrix}\]

Notice that composing a homomorphism with its adjoint always produces a
linear operator. If M is a map V🡪W then M*M means ``apply M, then M*'',
which takes us from V to W, then back from W to V, so the net effect is
a linear operator on V. By the same logic, MM* is a linear operator on
W.

\[MM^{*} = \begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix}\begin{pmatrix}
	1 & - 1 \\
	0 & 3 \\
	2 & - 2
\end{pmatrix} = \begin{pmatrix}
	5 & - 5 \\
	- 5 & 14
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ \ \ M^{*}M = \begin{pmatrix}
	1 & - 1 \\
	0 & 3 \\
	2 & - 2
\end{pmatrix}\begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix} = \begin{pmatrix}
	2 & - 3 & 4 \\
	- 3 & 9 & - 6 \\
	4 & - 6 & 8
\end{pmatrix}\]

It doesn't make sense to ask for the eigenvalues of a non-square matrix
like M because it can't possibly have any eigenvectors: M\ul{v} is of a
completely different dimension from \ul{v}, so it can never be just a
scalar multiple of it. However, we replace the idea of an eigenvalue
with a \textbf{singular value}, which is a scalar k such that

\[M\underline{v} = k\underline{u}\ \ \ \ \ \ \ \ \ \ M^{*}\underline{u} = k\underline{v}\]

where \ul{v} is a unit vector in the domain of M and \ul{u} is a unit
vector in the codomain. Traditionally \ul{v} is called a
\textbf{left-singular vector} of M and \ul{u} is called a
\textbf{right-singular vector}.

Suppose k is a singular value of M. Then

\[{MM^{*}\underline{u} = M\left( k\underline{v} \right)
}{= kM\underline{v}
}{= k^{2}\underline{u}}\]

and

\[{M^{*}M\underline{v} = M^{*}\left( k\underline{u} \right)
}{= kM^{*}\underline{u}
}{= k^{2}\underline{v}}\]

Thus the singular values of a matrix M ought to be equal to the square
roots of the eigenvalues of either MM* or M*M. But which one? These are
different linear operators acting on spaces of different dimensions, but
they're obviously very closely related to each other. In fact the
non-zero eigenvalues of the two will be equal. For example, in our case:

\begin{itemize}
	\item
	MM* has eigenvalues approximately equal to 16.23 and 2.77
	\item
	M*M has eigenvalues approximately equal to 16.23, 2.77 and 0
\end{itemize}

MM* is a 2x2 matrix, i.e. a linear operator on
\textbf{R}\textsuperscript{2} with up to two distinct eigenvalues,
whereas M*M is acting on \textbf{R}\textsuperscript{3} and so has up to
three. However, we can use a fact from the Rank-Nullity Theorem (see
7.1.2), which is that the ranks of MM* and M*M must be equal, and they
can't be more than the ranks of either M and M* individually. Hence we
will always get a set of shared eigenvalues and some zeros representing
the nullity of M, which can be discarded. The singular values of M are
the square roots of those non-zero eigenvalues, which are approximately
4.03 and 1.67.

\subsection{11.2 Calculating the SVD}\label{calculating-the-svd}

This section shows how to perform a Singular Value Decomposition by
hand, with a little computer assistance. Usually of course we would use
a computer to do all the work, especially as we'd usually be dealing
with a very large matrix. But going through a process like this manually
is a good way to check there are no mysterious steps hidden behind a
superficial explanation.

As usual, we'll let M be a homomorphism V🡪W, so it's an element of W⊗V*.
Since the dimensions of V and W will be important, we'll write them as
D\textsubscript{v} and D\textsubscript{w} respectively. We'll use the
standard terminology of ``a p-by-q matrix'' meaning one with p rows and
q columns. For example, M must have D\textsubscript{v} columns (since it
acts on V like a stack of covectors, and each covector is an element of
V*) and D\textsubscript{w} rows (since it acts on the result like a
stack of vectors, each of which is in W) so it's a
D\textsubscript{w}-by-D\textsubscript{v} matrix. You just need to
remember that it's the same way round as the tensor product: a
homomorphism in W⊗V* is represented by a
D\textsubscript{w}-\emph{by}-D\textsubscript{v} matrix. A silly way to
remember this is that, in the cinema, a big audience for a film ensures
lots of media coverage: rows (of people in seats) buy columns (of
newsprint); rows by columns.

The SVD is the closest a rectangular matrix can get to having an
eigendecomposition. Recall that in that case we have

\[M = \ A^{- 1}SA\]

where S is a diagonal matrix whose entries along the diagonal are the
eigenvalues of M and A is an invertible matrix. The overall effect is of
geometrically transforming the vector space, scaling and shearing along
the eigenvectors, then rotating and reflecting again.

The SVD rewrites M as a composition of three special matrices:

\[M = ASB\]

The idea is that B acts on the \emph{domain} of M by a rotation or
reflection, then S embeds the vectors of V into the codomain W in a
particular way, then finally A acts to rotate or reflect the
\emph{codomain} W again to complete the transformation. This means that
A and B are both unitary operators (see 9.1.3) -- they're very simple
and don't do much violence to the space they act on.

The matrices A, S and B are all build from the singular values and
vectors of M. Specifically:

\begin{itemize}
	\item
	The matrix S is the diagonal matrix whose entries along the main
	diagonal are the singular values of M.
	\item
	The columns of the matrix A are the left-singular vectors of M. These
	are the eigenvectors of
	\item
	The matrix B is the adjoint of B*, whose rows are the right-singular
	vectors of M.
\end{itemize}

Let's go through an example, assisted by the computer for the more
tiresome parts. Note that everything we do will be rounded off to two
decimal places to make things easier to read, which will give us an
approximate result. We'll use the same matrix as in the rest of this
section,

\[M = \begin{pmatrix}
	1 & 0 & 2 \\
	- 1 & 3 & - 2
\end{pmatrix}\]

Earlier we saw that the singular values are

\[s_{1} \approx 4.03\ \ \ \ \ \ \ \ s_{2} \approx 1.67\]

Therefore we can immediately write down

\[S = \begin{pmatrix}
	4.03 & 0 & 0 \\
	0 & 1.67 & 0
\end{pmatrix}\]

The matrix B is derived from a set of unit eigenvectors of M*M, also
called the \textbf{right singular vectors} of M itself. Here are a set
provided by Wolfram Alpha, which I've already normalized (see 3.2.3):

\[\begin{pmatrix}
	0.33 \\
	- 0.68 \\
	0.65
\end{pmatrix}\ \ \ \ \ \ \ \begin{pmatrix}
	0.3 \\
	0.73 \\
	0.61
\end{pmatrix}\ \ \ \ \ \ \ \begin{pmatrix}
	- 0.89 \\
	0 \\
	0.45
\end{pmatrix}\]

Recall that right singular vectors of M are its ``eigen-covectors'', so
it makes sense to think of these as living in the dual space. We can
turn these into a matrix by forming the tensor product of these with the
basis vectors:

\[\begin{pmatrix}
	1 \\
	0 \\
	0
\end{pmatrix}\bigotimes\begin{pmatrix}
	0.33 & - 0.68 & 0.65
\end{pmatrix} + \begin{pmatrix}
	0 \\
	1 \\
	0
\end{pmatrix}\bigotimes\begin{pmatrix}
	0.3 & 0.73 & 0.61
\end{pmatrix} + \begin{pmatrix}
	0 \\
	0 \\
	1
\end{pmatrix}\bigotimes\begin{pmatrix}
	- 0.89 & 0 & 0.45
\end{pmatrix} = \begin{pmatrix}
	0.33 & - 0.68 & 0.65 \\
	0.3 & 0.75 & 0.61 \\
	- 0.89 & 0 & 0.45
\end{pmatrix}\]

Note that this practically speaking this is just the matrix you form by
``stacking the covectors vertically'' -- the tensor product just
expresses what's happening in a more algebraic way.

We chose the eigenvectors of M*M rather than of MM* because that gives
us the right number of columns for our matrix. After all, the SVD is M =
ASB, so SB has to make sense; for this, B must have as many columns as S
has rows.

Now we need to find A, but this is done by an analogous process: this
time we find a set of eigenvectors of MM*, which are also called the
\textbf{left singular vectors} of M. Again these were calculated by
Wolfram Alpha, normalized and rounded to 2 decimal places as we've been
doing all along:

\[\begin{pmatrix}
	- 0.41 \\
	0.91
\end{pmatrix}\ \ \ \ \ \ \ \ \ \begin{pmatrix}
	0.91 \\
	0.41
\end{pmatrix}\]

This time we tensor them with the basis covectors to form a matrix
(which we could also think of as stacking them side-by-side as columns
of the matrix):

\[\begin{pmatrix}
	- 0.45 \\
	0.91
\end{pmatrix}\bigotimes\begin{pmatrix}
	1 & 0
\end{pmatrix} + \begin{pmatrix}
	0.91 \\
	0.41
\end{pmatrix}\bigotimes\begin{pmatrix}
	0 & 1
\end{pmatrix} = \begin{pmatrix}
	- 0.45 & 0.91 \\
	0.91 & 0.41
\end{pmatrix}\]

However, there's a problem. We can check our work by calculating ASB and
when we do we'll find it is \emph{not} equal to M. We'd expect them to
be approximately equal, not exactly, since we've been rounding values
off all the way along but in this case the result is miles off. What
went wrong is a common problem when eigenvectors are involved and you're
using a mixture of manual and computer techniques.

Recall that if \ul{v} is an eigenvector, so is -\ul{v}. Even when we
normalize the eigenvector the computer gives us, multiplying it by -1
gives another unit eigenvector for the same eigenvalue. In theory we
could multiply any of the unit left- and right-singular vectors we used
to build A and B by -1. If you do the entire calculation by hand, you'll
likely choose a set of eigenvectors that are consistent with each other,
with the left eigenvector having the same sign as the corresponding
right eigenvector. And if you let the computer do the whole SVD for you,
it will get it right. But if you do part of the calculation by hand and
part by computer, the right-singular vector we use to build B might not
match the left-singular vector we use to build A because one is
multiplied by -1. While you won't be calculating the SVD by hand more
than once in your life, it's worth noticing and watching out for next
time you do something like this by hand!

I fixed this by multiplying the first right singular vector by -1,
obtaining

\[B = \begin{pmatrix}
	- 0.33 & 0.68 & - 0.65 \\
	0.3 & 0.75 & 0.61 \\
	- 0.89 & 0 & 0.45
\end{pmatrix}\]

A quick calculation by Wolfram Alpha confirms we have an approximate SVD
of our original matrix:

\[{ASB = \begin{pmatrix}
		1.05437 & - 0.093405 & 2.10579 \\
		- 1.0048 & 3.00729 & - 1.96608
	\end{pmatrix}
}{\approx \begin{pmatrix}
		1 & 0 & 2 \\
		- 1 & 3 & - 2
\end{pmatrix}}\]

Our rounding errors got compounded pretty badly, meaning the value in
the upper right is more than 5\% off what it should be, but that's not
very surprising when you think how many times we're multiplying numbers
when composing matrices. In a real-life application the computer will
keep a lot more precision and the SVD will be much closer.

A final note: In most discussions of SVD the matrices A, S and B are
written as U, Σ and V* respectively. I chose to use ASB in the hope of
making it a bit clearer and avoiding ambiguity with the V* we usually
use to represent the dual space of V.

\subsection{11.3 Separable Models (*)}\label{separable-models}

\subsubsection{11.3.1 Separable
	Homomorphisms}\label{separable-homomorphisms}

We know that any homomorphism V🡪W can be thought of as an element of the
vector space W⊗V*. Given a covector \ul{v}* in V and a vector \ul{w} in
W we can identify a corresponding tensor \ul{w}⊗\ul{v}*, which is a
homomorphism. However, most elements of W⊗V* cannot be written down in
that form. As an example, consider the matrix

\[\begin{pmatrix}
	1 & 0 \\
	1 & - 1 \\
	0 & - 1
\end{pmatrix}\]

representing a homomorphism
\textbf{R}\textsuperscript{3}🡪\textbf{R}\textsuperscript{2}, i.e. an
element of in
\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{3}*. We can
write it out as a tensor easily enough, using \{\ul{e}\textsubscript{1},
\ul{e}\textsubscript{2}\} as the basis of the domain and
\{\ul{e}\textsubscript{1}*, \ul{e}\textsubscript{2}*,
\ul{e}\textsubscript{3}*\} as the basis of the dual space of the
codomain:

\[\begin{pmatrix}
	1 & 0 \\
	1 & - 1 \\
	0 & - 1
\end{pmatrix} = {\underline{e}}_{1}\bigotimes{\underline{e}}_{1}^{*} + {\underline{e}}_{1}\bigotimes{\underline{e}}_{2}^{*} - {\underline{e}}_{2}\bigotimes{\underline{e}}_{2}^{*} - {\underline{e}}_{2}\bigotimes{\underline{e}}_{3}^{*}\]

but there's no way to further simplify this so that we could express it
as a single tensor product of one vector with one covector.

A tensor that can be simplified in such a way is known as a \textbf{pure
	tensor} and a homomorphism that can be written as a pure tensor is said
to be \textbf{separable}. The main motivation for noticing these is that
they're especially simple and adding them together is a very easy
operation, so the SVD will enable us to break apart a homomorphism we
don't understand into simpler parts that we do, or to build one up from
basic elements.

Let's continue to work with homomorphisms in
\textbf{R}\textsuperscript{2}⊗\textbf{R}\textsuperscript{3}*. A
separable homomorphism will be a pure tensor, i.e.

\[{\underline{w}\bigotimes{\underline{v}}^{*} = \begin{pmatrix}
		x \\
		y
	\end{pmatrix}\bigotimes\begin{pmatrix}
		a & b & c
	\end{pmatrix}
}{= \left( x{\underline{e}}_{1} + y{\underline{e}}_{2} \right)\bigotimes\left( a{\underline{e}}_{1}^{*} + b{\underline{e}}_{2}^{*} + c{\underline{e}}_{3}^{*} \right)
}{= x{\underline{e}}_{1}\bigotimes a{\underline{e}}_{1}^{*} + x{\underline{e}}_{1}\bigotimes b{\underline{e}}_{2}^{*} + x{\underline{e}}_{1}\bigotimes c{\underline{e}}_{3}^{*} + y{\underline{e}}_{2}\bigotimes a{\underline{e}}_{1}^{*} + y{\underline{e}}_{2}\bigotimes b{\underline{e}}_{2}^{*} + y{\underline{e}}_{2}\bigotimes c{\underline{e}}_{3}^{*}
}{= ax{\underline{e}}_{1}\bigotimes{\underline{e}}_{1}^{*} + bx{\underline{e}}_{1}\bigotimes{\underline{e}}_{2}^{*} + cx{\underline{e}}_{1}\bigotimes{\underline{e}}_{3}^{*} + ay{\underline{e}}_{2}\bigotimes{\underline{e}}_{1}^{*} + by{\underline{e}}_{2}\bigotimes{\underline{e}}_{2}^{*} + cy{\underline{e}}_{2}\bigotimes{\underline{e}}_{3}^{*}
}{= \begin{pmatrix}
		ax & \begin{matrix}
			bx & cx
		\end{matrix} \\
		ay & \begin{matrix}
			by & cy
		\end{matrix}
	\end{pmatrix}
}\]Note the way the entries in the matrix are related to each other --
each column is a scalar multiple of each other column, which means the
rows must be scalar multiples of each other too. if you pick a random
matrix, it's very unlikely to exhibit this pattern!

Such a matrix cannot have rank greater than 1. Look at how it operates
on the basis vectors in the domain:

\[{\begin{pmatrix}
		ax & \begin{matrix}
			bx & cx
		\end{matrix} \\
		ay & \begin{matrix}
			by & cy
		\end{matrix}
	\end{pmatrix}\begin{pmatrix}
		1 \\
		0 \\
		0
	\end{pmatrix} = \begin{pmatrix}
		ax \\
		ay
	\end{pmatrix} = a\begin{pmatrix}
		x \\
		y
	\end{pmatrix}
}{\begin{pmatrix}
		ax & \begin{matrix}
			bx & cx
		\end{matrix} \\
		ay & \begin{matrix}
			by & cy
		\end{matrix}
	\end{pmatrix}\begin{pmatrix}
		0 \\
		1 \\
		0
	\end{pmatrix} = \begin{pmatrix}
		bx \\
		by
	\end{pmatrix} = b\begin{pmatrix}
		x \\
		y
	\end{pmatrix}
}{\begin{pmatrix}
		ax & \begin{matrix}
			bx & cx
		\end{matrix} \\
		ay & \begin{matrix}
			by & cy
		\end{matrix}
	\end{pmatrix}\begin{pmatrix}
		0 \\
		0 \\
		1
	\end{pmatrix} = \begin{pmatrix}
		cx \\
		cy
	\end{pmatrix} = c\begin{pmatrix}
		x \\
		y
\end{pmatrix}}\]

As you can see, all three basis vectors get sent to the same vector,
meaning the image of this homomorphism is one-dimensional, so the rank
is 1. Since every vector in the domain is a linear combination of these,
every one must get mapped into the same one-dimensional subspace of the
codomain.

We can therefore think of separable homomorphisms as projections onto
lines through the origin, which are one-dimensional subspaces.

\subsubsection{11.3.2 The Separable Models
	Perspective}\label{the-separable-models-perspective}

An alternative view of the Singular Value Decomposition is that it
decomposes our starting homomorphism M , which again represents a
homomorphism in W⊗V*. Then the SVD of M is

\[M = \sum_{i = 1}^{n}{s_{i}{\underline{a}}_{i}\bigotimes{\underline{b}}_{i}^{*}}\]

where:

\begin{itemize}
	\item
	n is the number of singular values of M
	\item
	each s\textsubscript{i} is one of those singular values
	\item
	each \ul{a}\textsubscript{i} is a left singular vector of M
	\item
	each \ul{b}\textsubscript{i}* is the dual of a right singular vector
	of M
\end{itemize}

Let's confirm that this gets us the same result as we reaching in
section 11.2. As a reminder, the singular values are:

\[s_{1} \approx 4.03\ \ \ \ \ \ \ \ s_{2} \approx 1.67\]

The left singular vectors are:

\[{\underline{a}}_{1} = \begin{pmatrix}
	- 0.41 \\
	0.91
\end{pmatrix}\ \ \ \ \ \ \ \ {\underline{a}}_{2} = \begin{pmatrix}
	0.91 \\
	0.41
\end{pmatrix}\]

The right singular vectors are covectors:

\[{{\underline{b}}_{1}^{*} = \begin{pmatrix}
		- 0.33 & 0.68 & - 0.65
	\end{pmatrix}\ \ \ \ \ \ \ 
}{{\underline{b}}_{2}^{*} = \begin{pmatrix}
		0.3 & 0.73 & 0.61
	\end{pmatrix}
}{{\underline{b}}_{3}^{*} = \begin{pmatrix}
		0.68 & 0.75 & 0
\end{pmatrix}}\]

Putting these together to the formula gives us:

\[{M = \sum_{i = 1}^{2}{s_{i}{\underline{a}}_{i}\bigotimes{\underline{b}}_{i}^{*}}\ 
}{= 4.03\begin{pmatrix}
		- 0.41 \\
		0.91
	\end{pmatrix}\bigotimes\begin{pmatrix}
		- 0.33 & 0.68 & - 0.65
	\end{pmatrix} + 1.67\begin{pmatrix}
		0.91 \\
		0.41
	\end{pmatrix}\bigotimes\begin{pmatrix}
		0.3 & 0.73 & 0.61
	\end{pmatrix}
}{= \begin{pmatrix}
		- 0.54 & - 1.12 & 1.07 \\
		- 1.21 & 2.49 & - 2.38
	\end{pmatrix} + \begin{pmatrix}
		0.46 & 1.11 & 0.93 \\
		0.21 & 0.5 & 0.42
	\end{pmatrix}
}{= \begin{pmatrix}
		1 & - 0.01 & 2 \\
		- 1.01 & 2.99 & - 1.97
	\end{pmatrix}
}{\approx \begin{pmatrix}
		1 & 0 & 2 \\
		- 1 & 3 & - 2
\end{pmatrix}}\]

Since we're rounding to 2 decimal places as usual, we do have some
rounding errors in the final answer but aside from those we've recovered
out original matrix. A fully computerized calculation would, of course,
retain a lot more precision; if we left all the square roots in
algebraic form the we get an exact answer but it would look like quite a
mess.

This form of the SVD decomposes our linear operator into a sum of rank 0
or rank 1 linear operators. Each of these must be a projection of the
domain onto either a single point (the origin) or a line through the
origin. Since each is a pure tensor, every row is a scalar multiple of
every other row and every column is a scalar multiple of every column. A
way to visualize this is by way of image processing -- see this
Stackexchange answer for a very nice illustrated example:

\url{https://stats.stackexchange.com/questions/177102/what-is-the-intuition-behind-svd/179042\#179042}

\subsection{11.4 The Moore-Penrose Pseudoinverse
	(*)}\label{the-moore-penrose-pseudoinverse}

As we said at the start, the SVD is a lot like eigendecomposition. The
difference is that eigendecomposition is only possible for non-defective
square matrices, whereas SVD works for any matrix. When M is square, the
two decompositions are the same.

The SVD of a homomorphism gives us immediate information about its
kernel and image. Specifically,

\begin{itemize}
	\item
	The right singular vectors (\ul{b}\textsubscript{i}) corresponding to
	zero singular values span the kernel of M.
	\item
	The left-singular vectors (\ul{a}\textsubscript{i}) corresponding to
	non-zero singular values span the image of M.
\end{itemize}

In this case there is one such right-singular vector,
\ul{b}\textsubscript{3}, so the kernel of M is one-dimensional; there
are two non-zero left-singular vectors so the rank of M is two. This
fits with the Rank-Nullity Theorem, since 1 + 2 = 3, the dimension of
the domain.

Since M represents a homomorphism between two different vector spaces,
\textbf{R}\textsuperscript{3}🡪\textbf{R}\textsuperscript{2}, the concept
of an inverse as we've developed it so far doesn't make a lot of sense.
However, it still makes sense to ask whether the action of M can be
``undone'' by another homomorphism going in the opposite direction,
\textbf{R}\textsuperscript{2}🡪\textbf{R}\textsuperscript{3}. The answer
is obviously ``no'', because the kernel isn't trivial; there's no way
for us to know how to put the vectors in the kernel back where they
started, since after the action of M they'll all be mapped to the zero
vector.

Nevertheless, it is possible to construct a homomorphism that is ``as
close as possible'' to an inverse for M, no matter what M's rank and
nullity. This has the fancy name of the \textbf{Moore-Penrose
	Pseudoinverse} of M, often written M\textsuperscript{+}.

If M is diagonal, M\textsuperscript{+} is calculated by taking the
reciprocal of each non-zero diagonal entry, then transposing the whole
matrix. Continuing with the matrices we've been working with,

\[S = \begin{pmatrix}
	4.03 & 0 & 0 \\
	0 & 1.67 & 0
\end{pmatrix}\ \ \ \ \ \ \ \ \ \ \ S^{+} = \begin{pmatrix}
	\frac{1}{4.03} & 0 \\
	0 & \frac{1}{1.67} \\
	0 & 0
\end{pmatrix}\]

Notice that it really does behave a bit like an inverse:

\[{SS^{+} = \begin{pmatrix}
		4.03 & 0 & 0 \\
		0 & 1.67 & 0
	\end{pmatrix}\begin{pmatrix}
		\frac{1}{4.03} & 0 \\
		0 & \frac{1}{1.67} \\
		0 & 0
	\end{pmatrix} = \begin{pmatrix}
		1 & 0 \\
		0 & 1
	\end{pmatrix}
}{S^{+}S = \begin{pmatrix}
		\frac{1}{4.03} & 0 \\
		0 & \frac{1}{1.67} \\
		0 & 0
	\end{pmatrix}\begin{pmatrix}
		4.03 & 0 & 0 \\
		0 & 1.67 & 0
	\end{pmatrix} = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & 1 & 0 \\
		0 & 0 & 0
\end{pmatrix}}\]

In the first example the inversion is perfect: SS\textsuperscript{+} is
a linear operator on \textbf{R}\textsuperscript{2} that's equivalent to
the identity. But in the second example, S\textsuperscript{+}S acts on
\textbf{R}\textsuperscript{3} and loses some information: it acts like
the identity on the plane spanned by the first two basks vectors but
collapses the rest of the space to zero. This is the best we can hope
for with a homomorphism with a non-trivial kernel.

For non-diagonal matrices, a standard way to calculate the pseudoinverse
is from the SVD. This is actually very easy -- replace A and B with
their adjoints, swap them over and replace S with its pseudoinverse:

\[M^{+} = B^{*}S^{+}A^{*}\]

Let's calculate this for our running example:

\[{M^{+} = \begin{pmatrix}
		- 0.33 & 0.68 & - 0.65 \\
		0.3 & 0.75 & 0.61 \\
		- 0.89 & 0 & 0.45
	\end{pmatrix}^{*}\begin{pmatrix}
		\frac{1}{4.03} & 0 \\
		0 & \frac{1}{1.67} \\
		0 & 0
	\end{pmatrix}\begin{pmatrix}
		- 0.45 & 0.91 \\
		0.91 & 0.41
	\end{pmatrix}^{*}
}{= \begin{pmatrix}
		- 0.33 & 0.3 & - 0.89 \\
		0.68 & 0.75 & 0 \\
		- 0.65 & 0.61 & 0.45
	\end{pmatrix}\begin{pmatrix}
		\frac{1}{4.03} & 0 \\
		0 & \frac{1}{1.67} \\
		0 & 0
	\end{pmatrix}\begin{pmatrix}
		- 0.45 & 0.91 \\
		0.91 & 0.41
	\end{pmatrix}
}{= \begin{pmatrix}
		0.2 & 0 \\
		0.33 & 0.33 \\
		0.4 & 0
\end{pmatrix}}\]

Let's check the result, remembering that yet again we'll have some
rounding errors:

\[{MM^{+} = \begin{pmatrix}
		1 & 0 & 2 \\
		- 1 & 3 & - 2
	\end{pmatrix}\begin{pmatrix}
		0.2 & 0 \\
		0.33 & 0.33 \\
		0.4 & 0
	\end{pmatrix} = \begin{pmatrix}
		1 & 0 \\
		0 & 0.99
	\end{pmatrix}
}{M^{+}M = \begin{pmatrix}
		0.2 & 0 \\
		0.33 & 0.33 \\
		0.4 & 0
	\end{pmatrix}\begin{pmatrix}
		1 & 0 & 2 \\
		- 1 & 3 & - 2
	\end{pmatrix} = \begin{pmatrix}
		0.2 & 0 & 0.4 \\
		0 & 0.99 & 0 \\
		0.4 & 0 & 0.8
\end{pmatrix}}\]

We can see that in the first case the pseudoinverse does a good job;
again, this is because the codomain has lower dimension than the domain
so no information is being lost. Composing with the pseudoinverse on the
left attempts to recover information lost in the kernel of the
homomorphism, and this is a hopeless task; the pseudoinverse doesn't
work well in that case.

The pseudoinverse of any homomorphism M satisfies the following
properties:

\[{MM^{+}M = M\ \ \ \ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ M^{+}MM^{+} = M^{+}
}{\left( MM^{+} \right)^{*} = MM^{+}\ \ \ \ \ and\ \ \ \ \ \ \left( M^{+}M \right)^{*} = M^{+}M}\]

The second property says that MM\textsuperscript{+} and
M\textsuperscript{+}M are both Hermitian. It turns out that for any
homomorphism V🡪W we can find exactly one homomorphism W🡪V that has these
properties; this can be taken to be a formal definition of the
Moore-Penrose pseudoinverse. The following properties will hopefully be
reassuring:

\[{\left( M^{+} \right)^{+} = M
}{\left( M^{*} \right)^{+} = \left( M^{+} \right)^{*}
}{(kM)^{+} = \frac{1}{k}\left( M^{+} \right)
}{M^{+} = M^{- 1}\ \ \ \ \ if\ M^{- 1}\ exists}\]

Less obviously, M\textsuperscript{+}M and MM\textsuperscript{+} are
Hermitian and idempotent (see the end of 8.2.1).

As an application of the pseudoinverse, let's go back to the end of the
last session, 10.2.3, where we started with the data matrix

\[D = \begin{pmatrix}
	- 1.52 & - 1.62 & \begin{matrix}
		1.6 & - 0.45
	\end{matrix} \\
	0.23 & 1.01 & \begin{matrix}
		- 1.07 & - 1.13
	\end{matrix} \\
	1.28 & 0.61 & \begin{matrix}
		- 0.53 & 1.59
	\end{matrix}
\end{pmatrix}\]

and transformed it into a new set of features represented by a matrix
I'll call T here:

\[{T = DP
}{= \begin{pmatrix}
		- 11.2066 & 0.2956 \\
		4.0801 & - 1.7939 \\
		7.1134 & 1.5059
\end{pmatrix}}\]

We said we would like to be able to reverse this and recover our
original data; the transformation that does this can also be used to
transform any analytical results we got when studying T. It seems that
we should be able to apply P\textsuperscript{+} on the right, getting
TP\textsuperscript{+} = DPP\textsuperscript{+}, getting us back to D as
best we can since PP\textsuperscript{+} ought to be ``identity-like''.
Here's the result (I got P\textsuperscript{+} using Wolfram Alpha):

\[{DPP^{+} = TP^{+}
}{= \begin{pmatrix}
		- 11.2066 & 0.2956 \\
		4.0801 & - 1.7939 \\
		7.1134 & 1.5059
	\end{pmatrix}\begin{pmatrix}
		0.14 & \begin{matrix}
			0.14 & - 0.14 & 0.06
		\end{matrix} \\
		0.18 & \begin{matrix}
			- 0.25 & 0.29 & 0.77
		\end{matrix}
	\end{pmatrix}
}{= \begin{pmatrix}
		- 1.52 & - 1.64 & \begin{matrix}
			1.65 & - 0.44
		\end{matrix} \\
		0.25 & 1.02 & \begin{matrix}
			- 1.09 & - 1.14
		\end{matrix} \\
		1.27 & 0.62 & \begin{matrix}
			- 0.56 & 1.59
		\end{matrix}
\end{pmatrix}}\]

As you can see, the reconstruction of the data is quite impressive! If
we were keeping more decimal places along the way it would be even
better. We can always measure how far off we are by calculating a matrix
of \textbf{residues}, i.e. differences between the original matrix and
the reconstructed one:

\[{DPP^{+} - D = \begin{pmatrix}
		- 1.52 & - 1.64 & \begin{matrix}
			1.65 & - 0.44
		\end{matrix} \\
		0.25 & 1.02 & \begin{matrix}
			- 1.09 & - 1.14
		\end{matrix} \\
		1.27 & 0.62 & \begin{matrix}
			- 0.56 & 1.59
		\end{matrix}
	\end{pmatrix} - \begin{pmatrix}
		- 1.52 & - 1.62 & \begin{matrix}
			1.6 & - 0.45
		\end{matrix} \\
		0.23 & 1.01 & \begin{matrix}
			- 1.07 & - 1.13
		\end{matrix} \\
		1.28 & 0.61 & \begin{matrix}
			- 0.53 & 1.59
		\end{matrix}
	\end{pmatrix}
}{= \begin{pmatrix}
		0 & - 0.02 & \begin{matrix}
			0.05 & 0.01
		\end{matrix} \\
		0.02 & 0.01 & \begin{matrix}
			- 0.02 & - 0.01
		\end{matrix} \\
		0.01 & 0.01 & \begin{matrix}
			- 0.03 & 0
		\end{matrix}
\end{pmatrix}}\]

If we needed our final result to be correct to the nearest whole number
(i.e. we calculated with 2 extra decimal places, which is pretty
common), all these errors will disappear when we round our results off.

\chapter{Graphs and Markov Processes}

This session describes some further applications of linear algebra.
These are a bit different in character from those we've already seen.
Although we only see the very beginning of the study of these objects,
you will see how convolution, adjoints and eigenvalues arise immediately
and help us answer very natural problems.

\textbf{Section 12.1} defines the mathematical objects called ``graphs''
and explains how any graph can be fully described by a matrix.

\textbf{Section 12.2} describes how processes that unfold in time can be
described using linear algebra even when we don't know exactly what they
will do, only the probabilities of each possible outcome.

\textbf{Section 12.3} points towards some areas of mathematics that
connect with or extend what we've studied on this course, as a
suggestion of what to look at next.

If time is short simply start at the beginning and see how far you get.

\subsection{12.1 Graphs}\label{graphs}

\includegraphics[width=2.21042in,height=1.95486in,alt={How to Generate a Random Directed Acyclic Graph for a Given Number of ...}]{media/image32.png}A
\textbf{directed graph} is an abstract structure consisting of a
collection of \textbf{nodes} (usually labelled by letters or numbers)
and a collection of \textbf{edges}. Each edge connects two nodes and has
a direction. In diagrams we usually show the nodes as points or
``blobs'' and the edges as arrows connecting them, as in the image on
the left.

Directed graphs come up in a bewildering array of technical
applications, from city infrastructure planning to linguistic analysis
to game design. A great number of problem-solving algorithms in
computing make use of graphs in some way or another, often explicitly.
Because of all this, we know a great deal about them.

As its name suggests, a directed graph is a special type of
\textbf{graph}. You can also have undirected graphs whose the edges are
just links -- we'd draw them as lines rather than arrows. The theory of
graphs belongs mostly to topology.

In many situations it's desirable to limit ourselves to \textbf{finite},
\textbf{simple} graphs. A graph is finite if it has a finite number of
nodes and edges; it's simple if the following two criteria are met:

\begin{itemize}
	\item
	No two edges have the same start-point and the same end-point. (If the
	graph is undirected, no two nodes are joined together by more than one
	edge)
	\item
	No edge starts and ends at the same node.
\end{itemize}

Our interest in graphs arises from the fact that any finite simple graph
can be fully described by its \textbf{adjacency matrix}, which has a 1
in row x column y if an edge exists that starts at node x and ends at
node y, and a 0 otherwise. For example, here is the adjacency matrix for
the graph diagrammed above:

\[A = \begin{pmatrix}
	0 & 1 & 1 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 & 1 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 1 \\
	0 & 0 & 0 & 0 & 0 & 0 & 1 \\
	0 & 0 & 0 & 0 & 1 & 0 & 1 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0
\end{pmatrix}\]

The adjacency matrix of an undirected graph is always symmetrical, since
in that case if Node X is joined to Node Y, then Node Y is automatically
joined to Node X. For a directed graph this is usually not the case --
an arrow can point from X to Y without any corresponding arrow pointing
from Y to X.

We can think of the graph as a network of ``places'' that once can
travel through only by using the edges (and only in the proper
direction, if there is one). We could use a vector in
\textbf{R}\textsuperscript{7} to represent, for example, the current
location of something in the network; then applying the adjacency matrix
gives us all the places it could have come from. For example, if we're
currently at node 5, the adjacency matrix tells us we must have come
from either node 2 or node 6:

\[A\underline{v} = \begin{pmatrix}
	0 & 1 & 1 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 & 1 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 1 \\
	0 & 0 & 0 & 0 & 0 & 0 & 1 \\
	0 & 0 & 0 & 0 & 1 & 0 & 1 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0
\end{pmatrix}\begin{pmatrix}
	0 \\
	0 \\
	0 \\
	0 \\
	1 \\
	0 \\
	0
\end{pmatrix} = \begin{pmatrix}
	0 \\
	1 \\
	0 \\
	0 \\
	0 \\
	1 \\
	0
\end{pmatrix}\]

Furthermore, the adjoint of A tells us where we can go next; in this
case it's only node 7:

\[A^{*}\underline{v} = \begin{pmatrix}
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 & 0 & 1 & 0 \\
	0 & 0 & 1 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 & 1 & 1 & 0
\end{pmatrix}\begin{pmatrix}
	0 \\
	0 \\
	0 \\
	0 \\
	1 \\
	0 \\
	0
\end{pmatrix} = \begin{pmatrix}
	0 \\
	0 \\
	0 \\
	0 \\
	0 \\
	0 \\
	1
\end{pmatrix}\]

Because all the information in a graph can be encoded by a matrix. Graph
theory, which appears to be an impressive town in its own right, is
nothing but a minor suburb of the great city of linear algebra.

You may notice that these matrices have a lot of zero entries; these are
called \textbf{sparse} matrices. Since the zeros don't hold much
information, they are good candidates for optimization (when it comes to
algorithms involving them) and compression (when it comes to storing
them). There is a large amount of technical literature on this subject,
in part because of the practical importance of adjacency matrices.

Since A* tells us our options for moving to the next node in the
network, its powers tell us our options for moving with multiple steps.
For example, A*\textsuperscript{3} tells us how many ways there are to
travel between each pair of nodes in three steps:

\[A^{*3} = \begin{pmatrix}
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	3 & 0 & 1 & 0 & 0 & 0 & 0
\end{pmatrix}\]

This tells us that if we want to make a journey that involves travelling
along three edges, there are only a few ways to do it. There's exactly
one way to get from Node 1 to Node 5 (1-3-6-5) and one way to get from
Node 3 to Node 7 (3-6-5-7). There are also three ways to get from Node 1
to Node 7 -- see if you can find them!

A particular application of this is that the diagonal entries in
A\textsuperscript{2} tell you all the ways to make a circular trip --
that is, starting and ending at the same node -- in two steps. If the
graph is simple and undirected, the only way to do this is to follow the
same edge out and back again. Thus, the diagonal entries in
A\textsuperscript{2} say how many edges are connected to each node; this
number is called the node's \textbf{degree}.

\includegraphics[width=2.23056in,height=1.61389in,alt={connectedness - Weakly Connected Graphs - Mathematics Stack Exchange}]{media/image33.png}
In the same way, the diagonal entries of A\textsuperscript{3} count
\textbf{triangles} in the graph -- that is, circular paths that go
around the edges of a triangle. But in this case there's a catch. For
example, the graph on the left has adjacency matrix

\[M = \begin{pmatrix}
	0 & 1 & 1 & 1 & 0 \\
	1 & 0 & 0 & 1 & 0 \\
	1 & 0 & 0 & 1 & 1 \\
	1 & 1 & 1 & 0 & 1 \\
	0 & 0 & 1 & 1 & 0
\end{pmatrix}\ \ \ \ so\ \ \ \ M^{3} = \begin{pmatrix}
	4 & 5 & 7 & 7 & 3 \\
	5 & 2 & 3 & 6 & 3 \\
	7 & 3 & 4 & 7 & 5 \\
	7 & 6 & 7 & 6 & 6 \\
	3 & 3 & 5 & 6 & 2
\end{pmatrix}\]

The matrix is saying there are 4 triangles starting at Node A, but you
might object that you can only see two: a-b-d-a and a-d-c-a. But we must
also consider these in the reverse direction: a-d-b-a and a-c-d-a. This
accounts for the four triangles in total. If we want to count
``geometric'' triangles only, we can divide the count by 2 since there
are only two ways to go around a triangle: clockwise or anticlockwise.
This is the beginning of something \emph{much} deeper that belongs to
the field of topology but also crops up in even elementary applications
of the calculus.

If you want to count all the triangles in a graph, it's very easy;
simply take tr(M\textsuperscript{3}), divide by 2 (to factor out the two
different ways we can go round each triangle) and then divide by 3. Can
you see why you'd have to do this last step?

Since graphs are nothing but matrices, we can apply all the techniques
of linear algebra to them. That could easily provide enough content for
a whole course (and is the subject of many books). For example, the
eigenvalues of the adjacency matrix can be used to discover (or
estimate) many geometric properties of the graph; as long as it's
undirected, its adjacency matrix is Hermitian and so its eigenvalues are
all real. In applications the graphs that arise often have hundreds of
nodes (or more) and are very difficult to analyse by other means, and in
these cases spectral analysis can be extremely helpful.

\subsection{12.2 Stochastic Matrices}\label{stochastic-matrices}

\subsubsection{12.2.1 Markov Processes}\label{markov-processes}

This section is about processes that unfold over time -- a classic
example is the fluctuating populations of predators and prey in an
ecosystem. We can sometimes model such processes by the repeated
application of a linear transformation at fixed time intervals.

A \textbf{linear iteration} can be defined by

\[\underline{v}\left( t_{i + 1} \right) = A\underline{v}(t_{i})\]

where v is a function that takes in a positive whole number and returns
a vector and A is a linear operator. Here time is not continuous but is
represented in intervals -- e.g. hours -- so that for example if
t\textsubscript{i} is the time now then t\textsubscript{i+1} will be the
time after the next interval. The definition says that the \emph{next}
vector will be equal to A times the \emph{current} vector.

For example, suppose that

\[\underline{v}\left( t_{1} \right) = \begin{pmatrix}
	1 \\
	0.5
\end{pmatrix}\ \ \ \ \ \ \ \ \ A = \ \begin{pmatrix}
	0.9 & 0.6 \\
	0.1 & 0.4
\end{pmatrix}\]

Then we have

\[{\underline{v}\left( t_{2} \right) = A\underline{v}\left( t_{1} \right)
}{= \begin{pmatrix}
		0.9 & 0.6 \\
		0.1 & 0.4
	\end{pmatrix}\begin{pmatrix}
		1 \\
		0.5
	\end{pmatrix}
}{= \begin{pmatrix}
		1.2 \\
		0.3
\end{pmatrix}}\]

In general,

\[\underline{v}\left( t_{n} \right) = A^{n}\underline{v}\left( t_{1} \right)\]

The matrix A has entries that are all numbers between 0 and 1 inclusive.
That means they can represent probabilities. Suppose the two dimensions
of the space are two possible states a system could be in. For
concreteness, let's assume vector \ul{e}\textsubscript{1} represents the
state of being at the pub and \ul{e}\textsubscript{2} the state of being
at home. We'll assume time moves in 1-hour intervals.

Suppose there's a 30\% chance I'm at the pub at 9pm on a given day. It
follows there's a 70\% chance I'm at home. I can represent this by

\[\underline{v}\left( t_{1} \right) = 0.3{\underline{e}}_{1} + 0.7{\underline{e}}_{2} = \ \begin{pmatrix}
	0.3 \\
	0.7
\end{pmatrix}\]

Where will I be at 11pm, i.e. two hours later? The model says:

\[{\underline{v}\left( t_{2} \right) = A^{2}\underline{v}\left( t_{1} \right)
}{= \begin{pmatrix}
		0.87 & 0.78 \\
		0.13 & 0.22
	\end{pmatrix}\begin{pmatrix}
		0.3 \\
		0.7
	\end{pmatrix}
}{= \begin{pmatrix}
		0.81 \\
		0.19
\end{pmatrix}}\]

Even though it's quite likely (70\%) I'll be at home at 9pm on the night
in question, it's very likely (81\%) I'll be in the pub by 11!

A matrix is called a \textbf{stochastic matrix} if it meets the
following criteria:

\begin{itemize}
	\item
	It's square, i.e. it represents a linear operator;
	\item
	All of its entries are in the range from 0 to 1, i.e. they can be
	interpreted as probabilities;
	\item
	The sum of entries down each column is equal to 1, i.e. a column
	represents a set of mutually exclusive and collectively exhaustive
	possibilities.
\end{itemize}

The term ``stochastic'' comes from the Greek word for dice, indicating
its connection with randomness. These matrices are also often called
``transition matrices'' or ``Markov matrices''. The term ``Markov'' is
used to describe a process that has no ``memory''. That is, the next
state of a Markov process depends only on its current state, not the
history of how it got there. Markov processes are often referred to as
\textbf{Markov chains}. The definition can be extended to cover
processes that change state in continuous time (rather than at discrete
intervals) although that makes the theory a lot more complicated.
Continuous time Markov processes are of great importance in mathematical
finance.

A \textbf{stochastic vector} meets the same criteria as a stochastic
matrix except the first one -- i.e. it looks like a column of a
stochastic matrix. All its entries must be between 0 and 1 and they must
sum to 1. A vector or matrix is said to be \textbf{positive} if it
contains no negative or zero entries; as you will see shortly, some
results about stochastic matrices and vectors require them to be
positive.

\includegraphics[width=0.86538in,height=1.69435in]{media/image34.png}We
can think of a stochastic matrix as the adjacency matrix of a graph in
which the edges are ``weighted''. Instead of a 1 or 0 in the adjacency
matrix, we put the weight of the edge, which in this case is just the
probability that we'll travel along it in the next time interval. In
this context the matrix is often referred to as a \textbf{transition
	matrix}.

Weighted graphs have many applications beyond probability. The weight
could represent the financial cost or time taken to traverse the edge,
as in applications to logistics and optimization problems.

\subsubsection{12.2.2 Steady States}\label{steady-states}

We are often interested in the long-term behaviour of such a process --
i.e. whether it ``settles down'' to some state or other. One way this
can happen is if t\textsubscript{i} is equal to an eigenvector with
eigenvalue equal to 1: in that case, after time i the stochastic matrix
won't change the vector and we will have reached a \textbf{steady
	state}.

Luckily for us, the Perron-Frobenius Theorem says the following are all
true for any \emph{positive} stochastic matrix A:

\begin{itemize}
	\item
	One of the eigenvalues of A is equal to 1
	\item
	A has a unique unit-length eigenvector, \ul{e}, corresponding to that
	eigenvalue with all positive entries.
	\item
	For any stochastic vector \ul{v}, A\textsuperscript{n}\ul{v} gets
	closer and closer to \ul{e} as n gets larger and larger.
\end{itemize}

The proof of this theorem requires some ideas from the calculus; a proof
of a slightly more general version of it can be found here:
\url{https://pi.math.cornell.edu/~web6720/Perron-Frobenius_Hannah\%20Cairns.pdf}

In the case of our running example

\[A = \ \begin{pmatrix}
	0.9 & 0.6 \\
	0.1 & 0.4
\end{pmatrix}\]

Wolfram alpha gives the following eigenvalues and eigenvectors:

\[{\lambda_{1} = 1\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\underline{e}}_{1} = \begin{pmatrix}
		6 \\
		1
	\end{pmatrix}
}{\lambda_{2} = 0.09\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\underline{e}}_{2} = \begin{pmatrix}
		- 1 \\
		1
\end{pmatrix}}\]

Therefore the steady state vector of A is obtained by dividing each
entry in $e_1$ by the sum of the entries, ensuring
they all add up to 1:

\[\underline{v} = \begin{pmatrix}
	6/7 \\
	1/7
\end{pmatrix} \approx \begin{pmatrix}
	0.86 \\
	0.14
\end{pmatrix}\]

\[
\]We can check this by starting with any stochastic vector and applying
a high power of A:

\[\begin{pmatrix}
	0.9 & 0.6 \\
	0.1 & 0.4
\end{pmatrix}^{100}\begin{pmatrix}
	0.2 \\
	0.8
\end{pmatrix} \approx \begin{pmatrix}
	0.86 \\
	0.14
\end{pmatrix}\]

If the matrix is diagonalizable, it's easy to calculate a high power and
use this to approximate the steady state, although in the process of
diagonalizing it we will have found eigenvalues and eigenvectors anyway.
If not, simply choose any stochastic vector and apply the matrix
repeatedly until the difference between steps is smaller than whatever
error you're willing to accept.

A famous application of Perron-Frobenius is Google's PageRank algorithm.
There are many explanations of this on the web but the one on the page
below should be very accessible after reading this section -- search in
the page for the word ``Google'' as the relevant part begins about
halfway down the page:

\url{https://math.libretexts.org/Bookshelves/Linear_Algebra/Interactive_Linear_Algebra_(Margalit_and_Rabinoff)/05\%3A_Eigenvalues_and_Eigenvectors/5.05\%3A_Stochastic_Matrices}


\end{document}